{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "celtic-greek",
   "metadata": {},
   "source": [
    "# 제9장 Tacotron 2: 일관 학습을 목표로 한 음성 합성\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/r9y9/ttslearn/blob/master/notebooks/ch09_Tacotron.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-biography",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-cause",
   "metadata": {},
   "source": [
    "### Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -VV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-senator",
   "metadata": {},
   "source": [
    "### ttslearn 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import ttslearn\n",
    "except ImportError:\n",
    "    !pip install ttslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttslearn\n",
    "ttslearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-bargain",
   "metadata": {},
   "source": [
    "### 패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "import tensorboard as tb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치 연산\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "# 음성 파형 불러오기\n",
    "from scipy.io import wavfile\n",
    "# 풀 컨텍스트 라벨, 질문 파일 로드\n",
    "from nnmnkwii.io import hts\n",
    "# 음성 분석\n",
    "import pyworld\n",
    "# 음성 분석, 시각화\n",
    "import librosa\n",
    "import librosa.display\n",
    "# 파이썬에서 배우는 음성 합성\n",
    "import ttslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정\n",
    "from ttslearn.util import init_seed\n",
    "init_seed(773)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-reader",
   "metadata": {},
   "source": [
    "### 그래프 그리기 설정 (描画周りの設定) // 번역 수정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.notebook import get_cmap, init_plot_style, savefig\n",
    "cmap = get_cmap()\n",
    "init_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-advantage",
   "metadata": {},
   "source": [
    "## 9.3 엔코더"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-effort",
   "metadata": {},
   "source": [
    "### 문자열을 숫자 열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어휘 정의\n",
    "characters = \"abcdefghijklmnopqrstuvwxyz!'(),-.:;? \"\n",
    "# 기타 특수 기호\n",
    "extra_symbols = [\n",
    "    \"^\",  # 문장의 시작을 나타내는 특수 기호 <SOS>\n",
    "    \"$\",  # 문장의 끝을 나타내는 특수 기호 <EOS>\n",
    "]\n",
    "_pad = \"~\"\n",
    "\n",
    "# NOTE: 패딩을 0 번째로 배치\n",
    "symbols = [_pad] + extra_symbols + list(characters)\n",
    "\n",
    "# 문자열 ⇔ 숫자의 상호 변환을위한 사전\n",
    "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "_id_to_symbol = {i: s for i, s in enumerate(symbols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text):\n",
    "    # 단순화를 위해 대문자와 소문자를 구별하지 않고 모든 대문자를 소문자로 변환\n",
    "    text = text.lower()\n",
    "\n",
    "    # <SOS>\n",
    "    seq = [_symbol_to_id[\"^\"]]\n",
    "\n",
    "    # 본문\n",
    "    seq += [_symbol_to_id[s] for s in text]\n",
    "\n",
    "    # <EOS>\n",
    "    seq.append(_symbol_to_id[\"$\"])\n",
    "\n",
    "    return seq\n",
    "\n",
    "\n",
    "def sequence_to_text(seq):\n",
    "    return [_id_to_symbol[s] for s in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = text_to_sequence(\"Hello!\")\n",
    "print(f\"문자열을 숫자 열로 변환: {seq}\")\n",
    "print(f\"숫자 열에서 문자열로 역변환: {sequence_to_text(seq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-sister",
   "metadata": {},
   "source": [
    "### 문자 끼워 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestEncoder(nn.Module):\n",
    "    def __init__(self, num_vocab=40, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_vocab, embed_dim, padding_idx=0)\n",
    "    \n",
    "    def forward(self, seqs):\n",
    "        return self.embed(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "SimplestEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-wiring",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.util import pad_1d\n",
    "\n",
    "def get_dummy_input():\n",
    "    # 배치 사이즈에 2를 상정하여 적당한 문자열을 생성\n",
    "    seqs = [\n",
    "        text_to_sequence(\"What is your favorite language?\"),\n",
    "        text_to_sequence(\"Hello world.\"),\n",
    "    ]\n",
    "    in_lens = torch.tensor([len(x) for x in seqs], dtype=torch.long)\n",
    "    max_len = max(len(x) for x in seqs)\n",
    "    seqs = torch.stack([torch.from_numpy(pad_1d(seq, max_len)) for seq in seqs])\n",
    "    \n",
    "    return seqs, in_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs, in_lens = get_dummy_input()\n",
    "print(\"입력\", seqs)\n",
    "print(\"계열 길이:\", in_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SimplestEncoder(num_vocab=40, embed_dim=256)\n",
    "seqs, in_lens = get_dummy_input()\n",
    "encoder_outs = encoder(seqs)\n",
    "print(f\"입력 사이즈: {tuple(seqs.shape)}\")\n",
    "print(f\"출력 사이즈: {tuple(encoder_outs.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 부분은 0을 취하고 그 이외는 연속 값으로 표현됩니다.\n",
    "encoder_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-thanks",
   "metadata": {},
   "source": [
    "### 1차원 컨벌루션 도입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_vocab=40,\n",
    "        embed_dim=256,\n",
    "        conv_layers=3,\n",
    "        conv_channels=256,\n",
    "        conv_kernel_size=5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # 문자 끼워넣기\n",
    "        self.embed = nn.Embedding(num_vocab, embed_dim, padding_idx=0)\n",
    "\n",
    "        # 1차원 컨벌루션의 중첩: 국소 의존성 모델링\n",
    "        self.convs = nn.ModuleList()\n",
    "        for layer in range(conv_layers):\n",
    "            in_channels = embed_dim if layer == 0 else conv_channels\n",
    "            self.convs += [\n",
    "                nn.Conv1d(\n",
    "                    in_channels,\n",
    "                    conv_channels,\n",
    "                    conv_kernel_size,\n",
    "                    padding=(conv_kernel_size - 1) // 2,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm1d(conv_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "            ]\n",
    "        self.convs = nn.Sequential(*self.convs)\n",
    "\n",
    "    def forward(self, seqs):\n",
    "        emb = self.embed(seqs)\n",
    "        # 1차원 컨벌루션과 embedding 에서는 입력의 크기가 다르므로 주의\n",
    "        out = self.convs(emb.transpose(1, 2)).transpose(1, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ConvEncoder(num_vocab=40, embed_dim=256)\n",
    "seqs, in_lens = get_dummy_input()\n",
    "encoder_outs = encoder(seqs)\n",
    "print(f\"입력 사이즈: {tuple(seqs.shape)}\")\n",
    "print(f\"출력 사이즈: {tuple(encoder_outs.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-track",
   "metadata": {},
   "source": [
    "### 양방향 LSTM 도입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Encoder(ConvEncoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_vocab=40,\n",
    "        embed_dim=512,\n",
    "        hidden_dim=512,\n",
    "        conv_layers=3,\n",
    "        conv_channels=512,\n",
    "        conv_kernel_size=5,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            num_vocab, embed_dim, conv_layers, conv_channels, conv_kernel_size\n",
    "        )\n",
    "        # 양방향 LSTM을 통한 장기 종속성 모델링\n",
    "        self.blstm = nn.LSTM(\n",
    "            conv_channels, hidden_dim // 2, 1, batch_first=True, bidirectional=True\n",
    "        )\n",
    "\n",
    "    def forward(self, seqs, in_lens):\n",
    "        emb = self.embed(seqs)\n",
    "        # 1차원 컨벌루션과 embedding에서는 입력의 크기가 다르므로 주의\n",
    "        out = self.convs(emb.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        # 양방향 LSTM 계산\n",
    "        out = pack_padded_sequence(out, in_lens, batch_first=True)\n",
    "        out, _ = self.blstm(out)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(num_vocab=40, embed_dim=256)\n",
    "seqs, in_lens = get_dummy_input()\n",
    "in_lens, indices = torch.sort(in_lens, dim=0, descending=True)\n",
    "seqs = seqs[indices]\n",
    "\n",
    "encoder_outs = encoder(seqs, in_lens)\n",
    "print(f\"입력 사이즈: {tuple(seqs.shape)}\")\n",
    "print(f\"출력 사이즈: {tuple(encoder_outs.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-female",
   "metadata": {},
   "source": [
    "## 9.4 주의 기구"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-kenya",
   "metadata": {},
   "source": [
    "### 내용 의존주의 메카니즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "# 책의 수식에 따라 이해하기 쉽도록 강조한 구현\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, encoder_dim=512, decoder_dim=1024, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.V = nn.Linear(encoder_dim, hidden_dim)\n",
    "        self.W = nn.Linear(decoder_dim, hidden_dim, bias=False)\n",
    "        # NOTE: 이 문서의 수식대로 구현하면 bias = False이지만 실용상 bias = True로 문제가 없습니다.\n",
    "        self.w = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_state, mask=None):\n",
    "        # 식 (9.11) 계산\n",
    "        erg = self.w(\n",
    "            torch.tanh(self.W(decoder_state).unsqueeze(1) + self.V(encoder_outs))\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        if mask is not None:\n",
    "            erg.masked_fill_(mask, -float(\"inf\"))\n",
    "\n",
    "        attention_weights = F.softmax(erg, dim=1)\n",
    "\n",
    "        # 엔코더 출력의 길이 방향에 대해 가중치 합을 취합니다.\n",
    "        attention_context = torch.sum(\n",
    "            encoder_outs * attention_weights.unsqueeze(-1), dim=1\n",
    "        )\n",
    "\n",
    "        return attention_context, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "BahdanauAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.util import make_pad_mask\n",
    "\n",
    "mask =  make_pad_mask(in_lens).to(encoder_outs.device)\n",
    "attention = BahdanauAttention()\n",
    "\n",
    "decoder_input = torch.ones(len(seqs), 1024)\n",
    "\n",
    "attention_context, attention_weights = attention(encoder_outs, decoder_input, mask)\n",
    "\n",
    "print(f\"엔코더 출력 크기: {tuple(encoder_outs.shape)}\")\n",
    "print(f\"디코더의 숨겨진 상태의 크기: {tuple(decoder_input.shape)}\")\n",
    "print(f\"컨텍스트 벡터의 크기: {tuple(attention_context.shape)}\")\n",
    "print(f\"어텐션 가중치: {tuple(attention_weights.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-bacteria",
   "metadata": {},
   "source": [
    "### 하이브리드 주의 기구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationSensitiveAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_dim=512,\n",
    "        decoder_dim=1024,\n",
    "        hidden_dim=128,\n",
    "        conv_channels=32,\n",
    "        conv_kernel_size=31,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.V = nn.Linear(encoder_dim, hidden_dim)\n",
    "        self.W = nn.Linear(decoder_dim, hidden_dim, bias=False)\n",
    "        self.U = nn.Linear(conv_channels, hidden_dim, bias=False)\n",
    "        self.F = nn.Conv1d(\n",
    "            1,\n",
    "            conv_channels,\n",
    "            conv_kernel_size,\n",
    "            padding=(conv_kernel_size - 1) // 2,\n",
    "            bias=False,\n",
    "        )\n",
    "        # NOTE: 이 문서의 수식대로 구현하면 bias = False이지만 실용상 bias = True로 문제가 없습니다.\n",
    "        self.w = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_outs, src_lens, decoder_state, att_prev, mask=None):\n",
    "        # 어텐션 가중치를 균일 분포로 초기화\n",
    "        if att_prev is None:\n",
    "            att_prev = 1.0 - make_pad_mask(src_lens).to(\n",
    "                device=decoder_state.device, dtype=decoder_state.dtype\n",
    "            )\n",
    "            att_prev = att_prev / src_lens.unsqueeze(-1).to(encoder_outs.device)\n",
    "\n",
    "        # (B x T_enc) -> (B x 1 x T_enc) -> (B x conv_channels x T_enc) ->\n",
    "        # (B x T_enc x conv_channels)\n",
    "        f = self.F(att_prev.unsqueeze(1)).transpose(1, 2)\n",
    "\n",
    "        # 식 (9.13) 계산\n",
    "        erg = self.w(\n",
    "            torch.tanh(\n",
    "                self.W(decoder_state).unsqueeze(1) + self.V(encoder_outs) + self.U(f)\n",
    "            )\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        if mask is not None:\n",
    "            erg.masked_fill_(mask, -float(\"inf\"))\n",
    "\n",
    "        attention_weights = F.softmax(erg, dim=1)\n",
    "\n",
    "        # 엔코더 출력의 길이 방향에 대해 가중치 합을 취합니다.\n",
    "        attention_context = torch.sum(\n",
    "            encoder_outs * attention_weights.unsqueeze(-1), dim=1\n",
    "        )\n",
    "\n",
    "        return attention_context, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "LocationSensitiveAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.util import make_pad_mask\n",
    "\n",
    "mask =  make_pad_mask(in_lens).to(encoder_outs.device)\n",
    "attention = LocationSensitiveAttention()\n",
    "\n",
    "decoder_input = torch.ones(len(seqs), 1024)\n",
    "\n",
    "attention_context, attention_weights = attention(encoder_outs, in_lens, decoder_input, None, mask)\n",
    "\n",
    "print(f\"엔코더 출력 크기: {tuple(encoder_outs.shape)}\")\n",
    "print(f\"디코더의 숨겨진 상태의 크기: {tuple(decoder_input.shape)}\")\n",
    "print(f\"컨텍스트 벡터의 크기: {tuple(attention_context.shape)}\")\n",
    "print(f\"어텐션 가중치: {tuple(attention_weights.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-tackle",
   "metadata": {},
   "source": [
    "## 9.5 디코더"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-facility",
   "metadata": {},
   "source": [
    "### Pre-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prenet(nn.Module):\n",
    "    def __init__(self, in_dim, layers=2, hidden_dim=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        prenet = nn.ModuleList()\n",
    "        for layer in range(layers):\n",
    "            prenet += [\n",
    "                nn.Linear(in_dim if layer == 0 else hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "        self.prenet = nn.Sequential(*prenet)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.prenet:\n",
    "            # 학습 및 추론 모두에 Dropout을 적용합니다.\n",
    "            x = F.dropout(layer(x), self.dropout, training=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prenet(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = torch.ones(len(seqs), 80)\n",
    "\n",
    "prenet = Prenet(80)\n",
    "out = prenet(decoder_input)\n",
    "print(f\"디코더 입력 크기: {tuple(decoder_input.shape)}\")\n",
    "print(f\"Pre-Net 출력 크기: {tuple(out.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-junction",
   "metadata": {},
   "source": [
    "### 주의 메커니즘이 있는 디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.tacotron.decoder import ZoneOutCell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_hidden_dim=512,\n",
    "        out_dim=80,\n",
    "        layers=2,\n",
    "        hidden_dim=1024,\n",
    "        prenet_layers=2,\n",
    "        prenet_hidden_dim=256,\n",
    "        prenet_dropout=0.5,\n",
    "        zoneout=0.1,\n",
    "        reduction_factor=1,\n",
    "        attention_hidden_dim=128,\n",
    "        attention_conv_channels=32,\n",
    "        attention_conv_kernel_size=31,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        # 주의 기구\n",
    "        self.attention = LocationSensitiveAttention(\n",
    "            encoder_hidden_dim,\n",
    "            hidden_dim,\n",
    "            attention_hidden_dim,\n",
    "            attention_conv_channels,\n",
    "            attention_conv_kernel_size,\n",
    "        )\n",
    "        self.reduction_factor = reduction_factor\n",
    "\n",
    "        # Prenet\n",
    "        self.prenet = Prenet(out_dim, prenet_layers, prenet_hidden_dim, prenet_dropout)\n",
    "\n",
    "        # 단방향 LSTM\n",
    "        self.lstm = nn.ModuleList()\n",
    "        for layer in range(layers):\n",
    "            lstm = nn.LSTMCell(\n",
    "                encoder_hidden_dim + prenet_hidden_dim if layer == 0 else hidden_dim,\n",
    "                hidden_dim,\n",
    "            )\n",
    "            lstm = ZoneOutCell(lstm, zoneout)\n",
    "            self.lstm += [lstm]\n",
    "\n",
    "        # 출력에 projection 레이어\n",
    "        proj_in_dim = encoder_hidden_dim + hidden_dim\n",
    "        self.feat_out = nn.Linear(proj_in_dim, out_dim * reduction_factor, bias=False)\n",
    "        self.prob_out = nn.Linear(proj_in_dim, reduction_factor)\n",
    "\n",
    "    def _zero_state(self, hs):\n",
    "        init_hs = hs.new_zeros(hs.size(0), self.lstm[0].hidden_size)\n",
    "        return init_hs\n",
    "\n",
    "    def forward(self, encoder_outs, in_lens, decoder_targets=None):\n",
    "        is_inference = decoder_targets is None\n",
    "\n",
    "        # Reduction factor를 기반으로 프레임 수 조정\n",
    "        # (B, Lmax, out_dim) ->  (B, Lmax/r, out_dim)\n",
    "        if self.reduction_factor > 1 and not is_inference:\n",
    "            decoder_targets = decoder_targets[\n",
    "                :, self.reduction_factor - 1 :: self.reduction_factor\n",
    "            ]\n",
    "\n",
    "        # 디코더의 계열 길이를 유지\n",
    "        # 추론시는 엔코더의 계열 길이로부터 경험적으로 상한을 정한다\n",
    "        if is_inference:\n",
    "            max_decoder_time_steps = int(encoder_outs.shape[1] * 10.0)\n",
    "        else:\n",
    "            max_decoder_time_steps = decoder_targets.shape[1]\n",
    "\n",
    "        # 제로 패딩된 부분에 대한 마스크\n",
    "        mask = make_pad_mask(in_lens).to(encoder_outs.device)\n",
    "\n",
    "        # LSTM 상태를 0으로 초기화\n",
    "        h_list, c_list = [], []\n",
    "        for _ in range(len(self.lstm)):\n",
    "            h_list.append(self._zero_state(encoder_outs))\n",
    "            c_list.append(self._zero_state(encoder_outs))\n",
    "\n",
    "        # 디코더의 첫 번째 입력\n",
    "        go_frame = encoder_outs.new_zeros(encoder_outs.size(0), self.out_dim)\n",
    "        prev_out = go_frame\n",
    "\n",
    "        # 이전 시간의 어텐션 가중치\n",
    "        prev_att_w = None\n",
    "\n",
    "        # 메인 루프\n",
    "        outs, logits, att_ws = [], [], []\n",
    "        t = 0\n",
    "        while True:\n",
    "            # 컨텍스트 벡터, 주의 가중치 계산\n",
    "            att_c, att_w = self.attention(\n",
    "                encoder_outs, in_lens, h_list[0], prev_att_w, mask\n",
    "            )\n",
    "\n",
    "            # Pre-Net\n",
    "            prenet_out = self.prenet(prev_out)\n",
    "\n",
    "            # LSTM\n",
    "            xs = torch.cat([att_c, prenet_out], dim=1)\n",
    "            h_list[0], c_list[0] = self.lstm[0](xs, (h_list[0], c_list[0]))\n",
    "            for i in range(1, len(self.lstm)):\n",
    "                h_list[i], c_list[i] = self.lstm[i](\n",
    "                    h_list[i - 1], (h_list[i], c_list[i])\n",
    "                )\n",
    "            # 출력 계산\n",
    "            hcs = torch.cat([h_list[-1], att_c], dim=1)\n",
    "            outs.append(self.feat_out(hcs).view(encoder_outs.size(0), self.out_dim, -1))\n",
    "            logits.append(self.prob_out(hcs))\n",
    "            att_ws.append(att_w)\n",
    "\n",
    "            # 다음 시간 디코더 입력 업데이트\n",
    "            if is_inference:\n",
    "                prev_out = outs[-1][:, :, -1]  # (1, out_dim)\n",
    "            else:\n",
    "                # Teacher forcing\n",
    "                prev_out = decoder_targets[:, t, :]\n",
    "\n",
    "            # 누적 어텐션 가중치\n",
    "            prev_att_w = att_w if prev_att_w is None else prev_att_w + att_w\n",
    "\n",
    "            t += 1\n",
    "            # 정지 조건 확인\n",
    "            if t >= max_decoder_time_steps:\n",
    "                break\n",
    "            if is_inference and (torch.sigmoid(logits[-1]) >= 0.5).any():\n",
    "                break\n",
    "                \n",
    "        # 각 시간의 출력 결합\n",
    "        logits = torch.cat(logits, dim=1)  # (B, Lmax)\n",
    "        outs = torch.cat(outs, dim=2)  # (B, out_dim, Lmax)\n",
    "        att_ws = torch.stack(att_ws, dim=1)  # (B, Lmax, Tmax)\n",
    "\n",
    "        if self.reduction_factor > 1:\n",
    "            outs = outs.view(outs.size(0), self.out_dim, -1)  # (B, out_dim, Lmax)\n",
    "\n",
    "        return outs, logits, att_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_targets = torch.ones(encoder_outs.shape[0], 120, 80)\n",
    "decoder = Decoder(encoder_outs.shape[-1], 80)\n",
    "\n",
    "# Teaccher forcing: decoder_targets(교사 데이터) 제공\n",
    "with torch.no_grad():\n",
    "    outs, logits, att_ws = decoder(encoder_outs, in_lens, decoder_targets);\n",
    "\n",
    "print(f\"디코더 입력 크기: {tuple(decoder_input.shape)}\")\n",
    "print(f\"디코더 출력 크기: {tuple(outs.shape)}\")\n",
    "print(f\"stop token (logits) 크기: {tuple(logits.shape)}\")\n",
    "print(f\"어텐션 가중치: {tuple(att_ws.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자기 회귀에 기초한 추론\n",
    "with torch.no_grad():\n",
    "    decoder(encoder_outs[0], torch.tensor([in_lens[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-pixel",
   "metadata": {},
   "source": [
    "## 9.6 Post-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-apparel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Postnet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim=80,\n",
    "        layers=5,\n",
    "        channels=512,\n",
    "        kernel_size=5,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        postnet = nn.ModuleList()\n",
    "        for layer in range(layers):\n",
    "            in_channels = in_dim if layer == 0 else channels\n",
    "            out_channels = in_dim if layer == layers - 1 else channels\n",
    "            postnet += [\n",
    "                nn.Conv1d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=(kernel_size - 1) // 2,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            ]\n",
    "            if layer != layers - 1:\n",
    "                postnet += [nn.Tanh()]\n",
    "            postnet += [nn.Dropout(dropout)]\n",
    "        self.postnet = nn.Sequential(*postnet)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        return self.postnet(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Postnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "postnet = Postnet(80)\n",
    "residual = postnet(outs)\n",
    "\n",
    "print(f\"입력 크기: {tuple(outs.shape)}\")\n",
    "print(f\"출력 크기: {tuple(residual.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-promotion",
   "metadata": {},
   "source": [
    "## 9.7 Tacotron 2 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-lightning",
   "metadata": {},
   "source": [
    "### Tacotron 2의 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tacotron2(nn.Module):\n",
    "    def __init__(self\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.postnet = Postnet()\n",
    "\n",
    "    def forward(self, seq, in_lens, decoder_targets):\n",
    "        # 인코더로 텍스트에 잠재적인 표현 획득\n",
    "        encoder_outs = self.encoder(seq, in_lens)\n",
    "\n",
    "        # 디코더에 의한 멜 스펙트로그램, 정지 토큰 예측\n",
    "        outs, logits, att_ws = self.decoder(encoder_outs, in_lens, decoder_targets)\n",
    "\n",
    "        # Post-Net에 의한 Mel spectrogram의 잔차 예측\n",
    "        outs_fine = outs + self.postnet(outs)\n",
    "\n",
    "        # (B, C, T) -> (B, T, C)\n",
    "        outs = outs.transpose(2, 1)\n",
    "        outs_fine = outs_fine.transpose(2, 1)\n",
    "\n",
    "        return outs, outs_fine, logits, att_ws\n",
    "    \n",
    "    def inference(self, seq):\n",
    "        seq = seq.unsqueeze(0) if len(seq.shape) == 1 else seq\n",
    "        in_lens = torch.tensor([seq.shape[-1]], dtype=torch.long, device=seq.device)\n",
    "\n",
    "        return self.forward(seq, in_lens, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs, in_lens = get_dummy_input()\n",
    "model = Tacotron2()\n",
    "\n",
    "# Tacotron 2 계산\n",
    "outs, outs_fine, logits, att_ws = model(seqs, in_lens, decoder_targets)\n",
    "\n",
    "print(f\"입력 크기: {tuple(seqs.shape)}\")\n",
    "print(f\"디코더 출력 크기: {tuple(outs.shape)}\")\n",
    "print(f\"Post-Net 출력 크기: {tuple(outs_fine.shape)}\")\n",
    "print(f\"stop token (logits) 크기: {tuple(logits.shape)}\")\n",
    "print(f\"어텐션 가중치: {tuple(att_ws.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-extraction",
   "metadata": {},
   "source": [
    "### 장난감 모델을 이용한 Tacotron 2의 동작 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.tacotron import Tacotron2\n",
    "model = Tacotron2(encoder_conv_layers=1, decoder_prenet_layers=1, decoder_layers=1, postnet_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_inout():\n",
    "    seqs, in_lens = get_dummy_input()\n",
    "   \n",
    "    # 디코더 출력(멜 스펙트로그램)의 교사 데이터\n",
    "    decoder_targets = torch.ones(2, 120, 80)\n",
    "    \n",
    "    # stop token 교사 데이터\n",
    "    # stop token의 예상 값은 확률이지만 교사 데이터는 이진 레이블입니다.\n",
    "    # 1은 디코더 출력이 완료되었음을 나타냅니다.\n",
    "    stop_tokens = torch.zeros(2, 120)\n",
    "    stop_tokens[:, -1:] = 1.0\n",
    "    \n",
    "    return seqs, in_lens, decoder_targets, stop_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적절한 입출력 생성\n",
    "seqs, in_lens, decoder_targets, stop_tokens = get_dummy_inout()\n",
    "\n",
    "# Tacotron 2의 출력 계산\n",
    "# NOTE: teacher-forcing 를 위해, decoder targets 를 명시적으로 준다\n",
    "outs, outs_fine, logits, att_ws = model(seqs, in_lens, decoder_targets)\n",
    "\n",
    "print(\"입력 크기:\", tuple(seqs.shape))\n",
    "print(\"디코더 출력 크기:\", tuple(outs.shape))\n",
    "print(\"Stop token 크기:\", tuple(logits.shape))\n",
    "print(\"어텐션 가중치:\", tuple(att_ws.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-commercial",
   "metadata": {},
   "source": [
    "### Tacotron 2の損失関数の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 디코더 출력에 대한 손실\n",
    "out_loss = nn.MSELoss()(outs, decoder_targets)\n",
    "# 2. Post-Net 이후의 출력에 대한 손실\n",
    "out_fine_loss = nn.MSELoss()(outs_fine, decoder_targets)\n",
    "# 3. Stop token에 대한 손실\n",
    "stop_token_loss = nn.BCEWithLogitsLoss()(logits, stop_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"out_loss: \", out_loss.item())\n",
    "print(\"out_fine_loss: \", out_fine_loss.item())\n",
    "print(\"stop_token_loss: \", stop_token_loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}