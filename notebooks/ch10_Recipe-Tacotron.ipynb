{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wired-flash",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 제10장 한국어 Tacotron에 기반한 음성 합성 시스템의 구현\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/r9y9/ttslearn/blob/master/notebooks/ch10_Recipe-Tacotron.ipynb)\n",
    "\n",
    "Google colab에서 실행하는 예상 소요 시간: 5시간\n",
    "\n",
    "이 노트북의 레시피 설정은 Google Colab에서 실행할 때 시간 초과를 피하기 위해 학습 조건을 책에 나열된 설정에서 일부 수정한 것입니다. (배치 크기 줄이기 등).\n",
    "참고로 책에 기재된 조건으로 저자(야마모토)가 레시피를 실행한 결과를 아래에서 공개하고 있습니다.\n",
    "\n",
    "- Tensorboard logs: https://tensorboard.dev/experiment/gHKogn7wRxa4B3NIVw27xw/\n",
    "- exp 디렉토리 (학습 모델, 합성 음성 포함) : https://drive.google.com/file/d/1LoIGkwTLUZmkJkxbTR1S7yyaWexn-Wfp/view?usp=sharing (226.9 MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-memorabilia",
   "metadata": {},
   "source": [
    "## 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-documentation",
   "metadata": {},
   "source": [
    "### Google Colab을 이용하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-reach",
   "metadata": {},
   "source": [
    "Google Colab에서 이 노트북을 실행하려면 메뉴의 '런타임 -> 런타임 시간 변경'에서 '하드웨어 가속기'를 **GPU**로 변경하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-terminology",
   "metadata": {},
   "source": [
    "### Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -VV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-short",
   "metadata": {},
   "source": [
    "### ttslearn 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-prague",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import ttslearn\n",
    "except ImportError:\n",
    "    !pip install ttslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttslearn\n",
    "ttslearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-prize",
   "metadata": {},
   "source": [
    "## 10.1 이 장의 일본어 음성 합성 시스템 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-senator",
   "metadata": {},
   "source": [
    "### 학습된 모델을 이용한 음성 합성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.tacotron import Tacotron2TTS\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import Audio\n",
    "\n",
    "engine = Tacotron2TTS()\n",
    "wav, sr = engine.tts(\"一貫学習にチャレンジしましょう！\", tqdm=tqdm)\n",
    "Audio(wav, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,2))\n",
    "librosa.display.waveshow(wav.astype(np.float32), sr, ax=ax)\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-surname",
   "metadata": {},
   "source": [
    "### 레시피 실행 전 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from ttslearn.env import is_colab\n",
    "from os.path import exists\n",
    "\n",
    "# pip install ttslearn은 레시피를 설치하지 않으므로 수동으로 다운로드\n",
    "if is_colab() and not exists(\"recipes.zip\"):\n",
    "    !curl -LO https://github.com/r9y9/ttslearn/releases/download/v{ttslearn.__version__}/recipes.zip\n",
    "    !unzip -o recipes.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# recipe 디렉토리로 이동\n",
    "cwd = os.getcwd()\n",
    "if cwd.endswith(\"notebooks\"):\n",
    "    os.chdir(\"../recipes/tacotron/\")\n",
    "elif is_colab():\n",
    "    os.chdir(\"recipes/tacotron/\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-honduras",
   "metadata": {},
   "source": [
    "### 패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "import tensorboard as tb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치 연산\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "# 음성 파형 불러오기\n",
    "from scipy.io import wavfile\n",
    "# 풀 컨텍스트 라벨, 질문 파일 로드\n",
    "from nnmnkwii.io import hts\n",
    "# 음성 분석\n",
    "import pyworld\n",
    "# 음성 분석, 시각화\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "# 파이썬에서 배우는 음성 합성\n",
    "import ttslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정\n",
    "from ttslearn.util import init_seed\n",
    "init_seed(773)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-falls",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-fellow",
   "metadata": {},
   "source": [
    "### 그래프 그리기 설정 (描画周りの設定) // 번역 수정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.notebook import get_cmap, init_plot_style, savefig\n",
    "cmap = get_cmap()\n",
    "init_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-wrestling",
   "metadata": {},
   "source": [
    "### 레시피 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.sh를 사용하여 학습 스크립트를 노트북에서 실행하려면 True\n",
    "# google colab의 경우 True라고 가정합니다.\n",
    "# 로컬 환경의 경우 run.sh를 터미널에서 실행하는 것이 좋습니다.\n",
    "# 이 경우이 노트북은 시각화 및 학습 된 모델을 테스트하는 데 사용됩니다.\n",
    "run_sh = is_colab()\n",
    "\n",
    "# 참고 : WaveNet을 사용한 평가 데이터에 대한 음성 생성은 시간이 오래 걸립니다.\n",
    "run_stage6 = True\n",
    "\n",
    "# run.sh를 통해 실행되는 스크립트의 tqdm\n",
    "run_sh_tqdm = \"none\"\n",
    "\n",
    "# CUDA\n",
    "# NOTE: run.sh의 인수로 전달하므로 bool이 아닌 문자열로 정의됩니다.\n",
    "cudnn_benchmark = \"true\"\n",
    "cudnn_deterministic = \"false\"\n",
    "\n",
    "# 특징량 추출시 병렬 처리 작업 수\n",
    "n_jobs = os.cpu_count()//2\n",
    "\n",
    "# 음향 모델 (Tacotron)의 설정 파일 이름\n",
    "acoustic_config_name=\"tacotron2_rf2\"\n",
    "# WaveNet 보코더 설정 파일 이름\n",
    "wavenet_config_name=\"wavenet_sr16k_mulaw256_30layers\"\n",
    "\n",
    "# Tacotron 학습의 배치 크기\n",
    "tacotron_batch_size = 16\n",
    "# Tacotron 학습의 반복 수\n",
    "# 주의: 충분한 품질을 얻기 위해 필요한 값: 50k ~ 100k steps\n",
    "tacotron_max_train_steps = 5000\n",
    "\n",
    "# WaveNet 보코더 학습의 배치 크기\n",
    "# 권장 배치 크기: 8 이상\n",
    "# 동작 확인을 위해 작은 값으로 설정합니다.\n",
    "wavenet_batch_size = 4\n",
    "# WavaNet의 학습 반복 수\n",
    "# 주의: 충분한 품질을 얻기 위해 필요한 값: 300k ~ 500k steps\n",
    "wavenet_max_train_steps = 20000\n",
    "\n",
    "# 음성을 생성하는 발화 수\n",
    "# WaveNet의 추론은 시간이 걸리므로 노트북에서 표시하는 5개만 생성\n",
    "num_eval_utts = 5\n",
    "\n",
    "# 노트북에서 사용하는 테스트용 발화(학습 데이터, 평가 데이터)\n",
    "train_utt = \"BASIC5000_0001\"\n",
    "test_utt = \"BASIC5000_5000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-yesterday",
   "metadata": {},
   "source": [
    "### Tensorboard로 로그 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노트북에서 tensorboard 로깅을 확인하려면 다음 줄을 사용하도록 설정하십시오.\n",
    "if is_colab():\n",
    "    %tensorboard --logdir tensorboard/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-release",
   "metadata": {},
   "source": [
    "## 10.2 Tacotron 2를 일본어에 적용하기위한 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-liability",
   "metadata": {},
   "source": [
    "### 음소열과 운율 기호가 있는 음소열 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopenjtalk\n",
    "# 이 구현은 나중에 설명합니다.\n",
    "from ttslearn.tacotron.frontend.openjtalk import pp_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"음소열:\", pyopenjtalk.g2p(\"端が\"))\n",
    "print(\"음소열:\", pyopenjtalk.g2p(\"箸が\"))\n",
    "print(\"음소열:\", pyopenjtalk.g2p(\"橋が\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"운율 기호가있는 음소열:\", \" \".join(pp_symbols(pyopenjtalk.extract_fullcontext(\"端が\"))))\n",
    "print(\"운율 기호가있는 음소열:\", \" \".join(pp_symbols(pyopenjtalk.extract_fullcontext(\"箸が\"))))\n",
    "print(\"운율 기호가있는 음소열:\", \" \".join(pp_symbols(pyopenjtalk.extract_fullcontext(\"橋が\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-simon",
   "metadata": {},
   "source": [
    "### 풀 컨텍스트 라벨에서 음소 열과 운율 기호 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def numeric_feature_by_regex(regex, s):\n",
    "    match = re.search(regex, s)\n",
    "    # 정의되지 않은 (xx)의 경우, 컨텍스트의 가능한 값 이외의 적절한 값\n",
    "    if match is None:\n",
    "        return -50\n",
    "    return int(match.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = hts.load(ttslearn.util.example_label_file())\n",
    "labels.contexts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feature_by_regex(r\"/A:([0-9\\-]+)\\+\", labels.contexts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_symbols(labels, drop_unvoiced_vowels=True):\n",
    "    PP = []\n",
    "    N = len(labels)\n",
    "\n",
    "    # 각 음소마다 순서대로 처리\n",
    "    for n in range(N):\n",
    "        lab_curr = labels[n]\n",
    "\n",
    "        # 해당 음소\n",
    "        p3 = re.search(r\"\\-(.*?)\\+\", lab_curr).group(1)\n",
    "\n",
    "        # 무성화 모음을 일반 모음으로 취급\n",
    "        if drop_unvoiced_vowels and p3 in \"AEIOU\":\n",
    "            p3 = p3.lower()\n",
    "\n",
    "        # 선두와 후행의 sil만 예외 대응\n",
    "        if p3 == \"sil\":\n",
    "            assert n == 0 or n == N - 1\n",
    "            if n == 0:\n",
    "                PP.append(\"^\")\n",
    "            elif n == N - 1:\n",
    "                # 질문 시스템인지 여부\n",
    "                e3 = numeric_feature_by_regex(r\"!(\\d+)_\", lab_curr)\n",
    "                if e3 == 0:\n",
    "                    PP.append(\"$\")\n",
    "                elif e3 == 1:\n",
    "                    PP.append(\"?\")\n",
    "            continue\n",
    "        elif p3 == \"pau\":\n",
    "            PP.append(\"_\")\n",
    "            continue\n",
    "        else:\n",
    "            PP.append(p3)\n",
    "\n",
    "        # 악센트 유형 및 위치 정보(전방 또는 후방)\n",
    "        a1 = numeric_feature_by_regex(r\"/A:([0-9\\-]+)\\+\", lab_curr)\n",
    "        a2 = numeric_feature_by_regex(r\"\\+(\\d+)\\+\", lab_curr)\n",
    "        a3 = numeric_feature_by_regex(r\"\\+(\\d+)/\", lab_curr)\n",
    "        # 악센트 절의 모라 수\n",
    "        f1 = numeric_feature_by_regex(r\"/F:(\\d+)_\", lab_curr)\n",
    "\n",
    "        a2_next = numeric_feature_by_regex(r\"\\+(\\d+)\\+\", labels[n + 1])\n",
    "\n",
    "        # 악센트 구 경계\n",
    "        if a3 == 1 and a2_next == 1:\n",
    "            PP.append(\"#\")\n",
    "        # 피치 하강(악센트 핵)\n",
    "        elif a1 == 0 and a2_next == a2 + 1 and a2 != f1:\n",
    "            PP.append(\"]\")\n",
    "        # 피치의 상승\n",
    "        elif a2 == 1 and a2_next == 2:\n",
    "            PP.append(\"[\")\n",
    "\n",
    "    return PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopenjtalk\n",
    "\n",
    "text = \"今日の天気は？\"\n",
    "\n",
    "# 텍스트에서 풀 컨텍스트 추출\n",
    "labels = pyopenjtalk.extract_fullcontext(text)\n",
    "# 풀 컨텍스트에서 운율 기호가있는 음소 열로 변환\n",
    "PP = pp_symbols(labels)\n",
    "\n",
    "print(\"입력 문자열:\", text)\n",
    "print(\"음소열:\", pyopenjtalk.g2p(text))\n",
    "print(\"운율 기호가있는 음소열:\", \" \".join(PP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-hours",
   "metadata": {},
   "source": [
    "## 프로그램 구현 전 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-sheffield",
   "metadata": {},
   "source": [
    "### stage -1: 코퍼스 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_colab():\n",
    "    ! ./run.sh --stage -1 --stop-stage -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-listening",
   "metadata": {},
   "source": [
    "### Stage 0: 학습/검증/평가 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 0 --stop-stage 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head data/dev.list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-station",
   "metadata": {},
   "source": [
    "## 10.3 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-dietary",
   "metadata": {},
   "source": [
    "### Tacotron 2를 위한 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-ceramic",
   "metadata": {},
   "source": [
    "#### 1 발화에 대한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.tacotron.frontend.openjtalk import text_to_sequence, pp_symbols\n",
    "from ttslearn.dsp import mulaw_quantize, logmelspectrogram\n",
    "\n",
    "# 운율 기호가있는 음소 열 추출\n",
    "labels = hts.load(ttslearn.util.example_label_file())\n",
    "PP = pp_symbols(labels.contexts)\n",
    "in_feats = np.array(text_to_sequence(PP), dtype=np.int64)\n",
    "\n",
    "# 멜 스펙트로그램 계산\n",
    "sr = 16000\n",
    "_sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "x = (x / 32768).astype(np.float64)\n",
    "x = librosa.resample(x, _sr, sr)\n",
    "\n",
    "out_feats = logmelspectrogram(x, sr)\n",
    "\n",
    "# 시작과 끝의 비음성 구간 길이 조정\n",
    "assert \"sil\" in labels.contexts[0] and \"sil\" in labels.contexts[-1]\n",
    "start_frame = int(labels.start_times[1] / 125000)\n",
    "end_frame = int(labels.end_times[-2] / 125000)\n",
    "\n",
    "# 처음: 50ms, 마지막: 100ms\n",
    "start_frame = max(0, start_frame - int(0.050 / 0.0125))\n",
    "end_frame = min(len(out_feats), end_frame + int(0.100 / 0.0125))\n",
    "\n",
    "out_feats = out_feats[start_frame:end_frame]\n",
    "\n",
    "# 시간 영역에서 오디오의 길이 조정\n",
    "x = x[int(start_frame * 0.0125 * sr) :]\n",
    "length = int(sr * 0.0125) * out_feats.shape[0]\n",
    "x = pad_1d(x, length) if len(x) < length else x[:length]\n",
    "\n",
    "# 특징량의 업샘플링을 하기 때문에, 음성 파형의 길이는 프레임 시프트로 나눌 필요가 있습니다\n",
    "assert len(x) % int(sr * 0.0125) == 0\n",
    "\n",
    "# mu-law 양자화\n",
    "x = mulaw_quantize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-steam",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tacotron의 입력 특징의 크기:\", in_feats.shape)\n",
    "print(\"Tacotron 출력 특징의 크기:\", out_feats.shape)\n",
    "print(\"WaveNet 보코더 출력의 오디오 파형 크기:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.tacotron.frontend.openjtalk import num_vocab\n",
    "from ttslearn.dsp import inv_mulaw_quantize\n",
    "from torch.nn import functional as F\n",
    "\n",
    "inp = F.one_hot(torch.from_numpy(in_feats), num_vocab()).numpy()\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(8,8))\n",
    "ax[0].set_title(\"Phoneme sequence + prosody symbols (one-hot)\")\n",
    "ax[1].set_title(\"Mel-spectrogram\")\n",
    "ax[2].set_title(\"Mu-law quantized waveform\")\n",
    "\n",
    "ax[0].imshow(inp.T, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\", cmap=cmap)\n",
    "ax[1].imshow(out_feats.T, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\", cmap=cmap)\n",
    "librosa.display.waveshow(x.astype(np.float32), ax=ax[2], sr=sr)\n",
    "\n",
    "ax[0].set_xlabel(\"Phoneme\")\n",
    "ax[0].set_ylabel(\"Binary value\")\n",
    "ax[1].set_xlabel(\"Time [frame]\")\n",
    "ax[1].set_ylabel(\"Mel filter channel\")\n",
    "ax[2].set_xlabel(\"Time [sec]\")\n",
    "ax[2].set_ylabel(\"Amplitude\")\n",
    "\n",
    "plt.tight_layout()\n",
    "savefig(\"fig/e2etts_impl_taco2_inout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-computer",
   "metadata": {},
   "source": [
    "#### 레시피의 stage 1 실행\n",
    "\n",
    "배치 처리를 실시하는 커멘드 라인 프로그램은, `preprocess.py`를 참조해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 1 --stop-stage 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-invention",
   "metadata": {},
   "source": [
    "### 특징량 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-aurora",
   "metadata": {},
   "source": [
    "정규화를 위한 통계량을 계산하는 명령행 프로그램은 `recipes/common/fit_scaler.py`를 참조하십시오. 또, 정규화를 실시하는 커멘드 라인 프로그램은, `recipes/common/preprocess_normalize.py` 를 참조해 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-peoples",
   "metadata": {},
   "source": [
    "#### 레시피의 stage 2 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 2 --stop-stage 2 --n-jobs $n_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-senegal",
   "metadata": {},
   "source": [
    "#### 정규화 처리 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_feats = np.load(f\"dump/jsut_sr16000/org/train/out_tacotron/{train_utt}-feats.npy\")\n",
    "in_feats_norm = np.load(f\"dump/jsut_sr16000/norm/train/out_tacotron/{train_utt}-feats.npy\")\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8,6), sharex=True)\n",
    "ax[0].set_title(\"Mel-spectrogram (before normalization)\")\n",
    "ax[1].set_title(\"Mel-spectrogram (after normalization)\")\n",
    "\n",
    "hop_length = int(sr * 0.0125)\n",
    "mesh = librosa.display.specshow(\n",
    "    in_feats.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\", ax=ax[0], cmap=cmap)\n",
    "fig.colorbar(mesh, ax=ax[0])\n",
    "mesh = librosa.display.specshow(\n",
    "    in_feats_norm.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\",ax=ax[1], cmap=cmap)\n",
    "mesh.set_clim(-4, 4)\n",
    "fig.colorbar(mesh, ax=ax[1])\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Mel filter channel\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-silicon",
   "metadata": {},
   "source": [
    "## 10.4 Tacotron 학습 스크립트 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-armstrong",
   "metadata": {},
   "source": [
    "### DataLoader 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-africa",
   "metadata": {},
   "source": [
    "#### collate_fn 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_divisible_by(feats, N):\n",
    "    if N == 1:\n",
    "        return feats\n",
    "    mod = len(feats) % N\n",
    "    if mod != 0:\n",
    "        feats = feats[: len(feats) - mod]\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.util import pad_1d, pad_2d\n",
    "\n",
    "def collate_fn_tacotron(batch, reduction_factor=1):\n",
    "    xs = [x[0] for x in batch]\n",
    "    ys = [ensure_divisible_by(x[1], reduction_factor) for x in batch]\n",
    "    in_lens = [len(x) for x in xs]\n",
    "    out_lens = [len(y) for y in ys]\n",
    "    in_max_len = max(in_lens)\n",
    "    out_max_len = max(out_lens)\n",
    "    x_batch = torch.stack([torch.from_numpy(pad_1d(x, in_max_len)) for x in xs])\n",
    "    y_batch = torch.stack([torch.from_numpy(pad_2d(y, out_max_len)) for y in ys])\n",
    "    in_lens = torch.tensor(in_lens, dtype=torch.long)\n",
    "    out_lens = torch.tensor(out_lens, dtype=torch.long)\n",
    "    stop_flags = torch.zeros(y_batch.shape[0], y_batch.shape[1])\n",
    "    for idx, out_len in enumerate(out_lens):\n",
    "        stop_flags[idx, out_len - 1 :] = 1.0\n",
    "    return x_batch, in_lens, y_batch, out_lens, stop_flags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-acceptance",
   "metadata": {},
   "source": [
    "#### DataLoader 사용 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from ttslearn.train_util import Dataset, collate_fn_tacotron\n",
    "from functools import partial\n",
    "\n",
    "in_paths = sorted(Path(\"./dump/jsut_sr16000/norm/dev/in_tacotron/\").glob(\"*.npy\"))\n",
    "out_paths = sorted(Path(\"./dump/jsut_sr16000/norm/dev/out_tacotron/\").glob(\"*.npy\"))\n",
    "\n",
    "dataset = Dataset(in_paths, out_paths)\n",
    "collate_fn = partial(collate_fn_tacotron, reduction_factor=1)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=8, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "in_feats, in_lens, out_feats, out_lens, stop_flags = next(iter(data_loader))\n",
    "print(\"입력 특징량의 크기:\", tuple(in_feats.shape))\n",
    "print(\"출력 특징량의 크기:\", tuple(out_feats.shape))\n",
    "print(\"stop flags 크기:\", tuple(stop_flags.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-islam",
   "metadata": {},
   "source": [
    "#### ミニバッチの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(out_feats), 1, figsize=(8,10), sharex=True, sharey=True)\n",
    "for n in range(len(in_feats)):\n",
    "    x = out_feats[n].data.numpy()\n",
    "    hop_length = int(sr * 0.0125)\n",
    "    mesh = librosa.display.specshow(x.T, sr=sr, x_axis=\"time\", y_axis=\"frames\", hop_length=hop_length, cmap=cmap, ax=ax[n])\n",
    "    fig.colorbar(mesh, ax=ax[n])\n",
    "    mesh.set_clim(-4, 4)\n",
    "    # 나중에 다시 붙이기 때문에 여기에서 라벨을 삭제합니다.\n",
    "    ax[n].set_xlabel(\"\")\n",
    "    \n",
    "ax[-1].set_xlabel(\"Time [sec]\")\n",
    "for a in ax:\n",
    "    a.set_ylabel(\"Mel channel\")\n",
    "\n",
    "plt.tight_layout()\n",
    "savefig(\"fig/e2etts_impl_minibatch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-delivery",
   "metadata": {},
   "source": [
    "### 간단한 학습 스크립트 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-southeast",
   "metadata": {},
   "source": [
    "#### 학습 전 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.tacotron import Tacotron2 as Tacotron\n",
    "from torch import optim\n",
    "\n",
    "# 동작 확인용: 레이어 수를 줄인 작은 Tacotron\n",
    "model = Tacotron(\n",
    "    embed_dim=32, encoder_conv_layers=1, encoder_conv_channels=32, encoder_hidden_dim=32,\n",
    "    decoder_hidden_dim=32, postnet_channels=32, postnet_layers=1)\n",
    "\n",
    "# lr은 학습률을 나타냅니다.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# gamma는 학습률의 감쇠 계수를 나타냅니다.\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, gamma=0.5, step_size=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-controversy",
   "metadata": {},
   "source": [
    "#### 학습 루프 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.util import make_non_pad_mask\n",
    "\n",
    "# DataLoader를 사용하여 미니 배치 생성 : 미니 배치 당 처리\n",
    "for in_feats, in_lens, out_feats, out_lens, stop_flags in tqdm(data_loader):\n",
    "    in_lens, indices = torch.sort(in_lens, dim=0, descending=True)\n",
    "    in_feats, out_feats, out_lens = in_feats[indices], out_feats[indices], out_lens[indices]\n",
    "    \n",
    "    # 순전파 계산\n",
    "    outs, outs_fine, logits, _ = model(in_feats, in_lens, out_feats)\n",
    "    \n",
    "    # 제로 패디그 부분을 손실 함수의 계산에서 제외하기 위해 마스크를 적용합니다.\n",
    "    # Mask (B x T x 1)\n",
    "    mask = make_non_pad_mask(out_lens).unsqueeze(-1)\n",
    "    out_feats = out_feats.masked_select(mask)\n",
    "    outs = outs.masked_select(mask)\n",
    "    outs_fine = outs_fine.masked_select(mask)\n",
    "    stop_flags = stop_flags.masked_select(mask.squeeze(-1))\n",
    "    logits = logits.masked_select(mask.squeeze(-1))\n",
    "\n",
    "    # 손실 계산\n",
    "    decoder_out_loss = nn.MSELoss()(outs, out_feats)\n",
    "    postnet_out_loss = nn.MSELoss()(outs_fine, out_feats) \n",
    "    stop_token_loss = nn.BCEWithLogitsLoss()(logits, stop_flags)\n",
    "    \n",
    "    # 손실의 합계\n",
    "    loss = decoder_out_loss + postnet_out_loss + stop_token_loss\n",
    "\n",
    "    # 손실 값 출력\n",
    "    print(f\"decoder_out_loss: {decoder_out_loss:.2f}, postnet_out_loss: {postnet_out_loss:.2f}, stop_token_loss: {stop_token_loss:.2f}\")\n",
    "    # optimizer에 축적 된 그라디언트 재설정\n",
    "    optimizer.zero_grad()\n",
    "    # 오차의 역전파\n",
    "    loss.backward()\n",
    "    # 매개변수 업데이트\n",
    "    optimizer.step()\n",
    "    # 학습률 스케줄러 업데이트\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-multiple",
   "metadata": {},
   "source": [
    "### 어텐션 가중치 시각화\n",
    "\n",
    "여기에서는 학습이 성공적으로 진행되지 않는 경우의 예로 의도적으로 학습된 모델의 일부 매개 변수를 난수로 초기화합니다. 자세한 내용은 `randomize_tts_engine_`을 참조하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.tacotron import Tacotron2TTS\n",
    "from ttslearn.tacotron.tts import randomize_tts_engine_\n",
    "\n",
    "tacotron_engine = Tacotron2TTS()\n",
    "\n",
    "tacotron_engine_bad = Tacotron2TTS()\n",
    "randomize_tts_engine_(tacotron_engine_bad)\n",
    "print(\"randomized some of network weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"水をマレーシアから買わなくてはならないのです。\"\n",
    "\n",
    "import pyopenjtalk\n",
    "from ttslearn.tacotron.frontend.openjtalk import text_to_sequence, pp_symbols\n",
    "\n",
    "labels = pyopenjtalk.extract_fullcontext(text)\n",
    "# 운율 기호가있는 음소열\n",
    "in_feats = text_to_sequence(pp_symbols(labels))\n",
    "in_feats = torch.tensor(in_feats, dtype=torch.long)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outs, outs_fine, logits, att_ws = tacotron_engine.acoustic_model.inference(in_feats)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    outs2, outs_fine2, logits2, att_ws2 = tacotron_engine_bad.acoustic_model.inference(in_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax[0].set_title(\"Failure\")\n",
    "ax[1].set_title(\"Normal\")\n",
    "\n",
    "mesh = ax[0].imshow(att_ws2.cpu().data.numpy().T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\")\n",
    "fig.colorbar(mesh, ax=ax[0])\n",
    "ax[0].set_xlabel(\"Decoder time step [frame]\")\n",
    "ax[0].set_ylabel(\"Encoder time step [phoneme]\")\n",
    "\n",
    "mesh = ax[1].imshow(att_ws.cpu().data.numpy().T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\")\n",
    "fig.colorbar(mesh, ax=ax[1])\n",
    "ax[1].set_xlabel(\"Decoder time step [frame]\")\n",
    "ax[1].set_ylabel(\"Encoder time step [phoneme]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 10-5\n",
    "savefig(\"./fig/e2etts_impl_attention_failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-sleeping",
   "metadata": {},
   "source": [
    "### 실용적인 학습 스크립트 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-wayne",
   "metadata": {},
   "source": [
    "`train_tacotron.py`를 참조하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-techno",
   "metadata": {},
   "source": [
    "## 10.5 Tacotron 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-information",
   "metadata": {},
   "source": [
    "### Tacotron 구성 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat conf/train_tacotron/model/{acoustic_config_name}.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-aspect",
   "metadata": {},
   "source": [
    "### Tacotron 인스턴스화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "hydra.utils.instantiate(OmegaConf.load(f\"./conf/train_tacotron/model/{acoustic_config_name}.yaml\")[\"netG\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-administrator",
   "metadata": {},
   "source": [
    "### 레시피의 stage 3 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 3 --stop-stage 3 --acoustic-model $acoustic_config_name \\\n",
    "        --tqdm $run_sh_tqdm --tacotron-train-max-train-steps $tacotron_max_train_steps \\\n",
    "        --tacotron-data-batch-size $tacotron_batch_size \\\n",
    "        --cudnn-benchmark $cudnn_benchmark --cudnn-deterministic $cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-consultancy",
   "metadata": {},
   "source": [
    "### 손실 함수의 값 추이\n",
    "\n",
    "著者による実験結果です。Tensorboardのログは https://tensorboard.dev/ にアップロードされています。\n",
    "ログデータを`tensorboard` パッケージを利用してダウンロードします。\n",
    "\n",
    "https://tensorboard.dev/experiment/yXyg9qgfQRSGxvil5FA4xw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists(\"tensorboard/all_log.csv\"):\n",
    "    df = pd.read_csv(\"tensorboard/all_log.csv\")\n",
    "else:\n",
    "    experiment_id = \"gHKogn7wRxa4B3NIVw27xw\"\n",
    "    experiment = tb.data.experimental.ExperimentFromDev(experiment_id)\n",
    "    df = experiment.get_scalars() \n",
    "    df.to_csv(\"tensorboard/all_log.csv\", index=False)\n",
    "df[\"run\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "tacotron_loss = df[df.run.str.contains(\"tacotron2_rf2\")]\n",
    "\n",
    "tacotron_train_loss = tacotron_loss[tacotron_loss.tag.str.startswith(\"Loss/train\")]\n",
    "tacotron_dev_loss = tacotron_loss[tacotron_loss.tag.str.startswith(\"Loss/dev\")]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(tacotron_train_loss[\"step\"], tacotron_train_loss[\"value\"], label=\"Train\")\n",
    "ax.plot(tacotron_dev_loss[\"step\"], tacotron_dev_loss[\"value\"], \"--\", label=\"Dev\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Epoch loss\")\n",
    "plt.legend()\n",
    "\n",
    "# 그림 10-6\n",
    "savefig(\"fig/tacotron_impl_tacotron_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-entry",
   "metadata": {},
   "source": [
    "## 10.6 WaveNet 보코더 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-director",
   "metadata": {},
   "source": [
    "### WaveNet 보코더 설정 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat conf/train_wavenet/model/{wavenet_config_name}.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-plaza",
   "metadata": {},
   "source": [
    "### WaveNet 보코더 인스턴스화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "# WaveNet의 30 층 모두를 표시하면 길어지므로 여기서는 생략합니다.\n",
    "# hydra.utils.instantiate(OmegaConf.load(f\"./conf/train_wavenet/model/{wavenet_config_name}.yaml\")[\"netG\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-notebook",
   "metadata": {},
   "source": [
    "### 레시피 stage 4 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 4 --stop-stage 4 --wavenet-model $wavenet_config_name \\\n",
    "        --tqdm $run_sh_tqdm --wavenet-train-max-train-steps $wavenet_max_train_steps \\\n",
    "        --wavenet-data-batch-size $wavenet_batch_size \\\n",
    "        --cudnn-benchmark $cudnn_benchmark --cudnn-deterministic $cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-brighton",
   "metadata": {},
   "source": [
    "### 손실 함수의 값 추이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet_loss = df[df.run.str.contains(\"wavenet\")]\n",
    "\n",
    "wavenet_train_loss = wavenet_loss[wavenet_loss.tag.str.contains(\"Loss/train\")]\n",
    "wavenet_dev_loss = wavenet_loss[wavenet_loss.tag.str.contains(\"Loss/dev\")]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(wavenet_train_loss[\"step\"], wavenet_train_loss[\"value\"], label=\"Train\")\n",
    "ax.plot(wavenet_dev_loss[\"step\"], wavenet_dev_loss[\"value\"], \"--\", label=\"Dev\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Epoch loss\")\n",
    "ax.set_ylim(1.6, 2.3)\n",
    "plt.legend()\n",
    "\n",
    "# 그림 10-7\n",
    "savefig(\"fig/tacotron_impl_wavenet_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-coordinator",
   "metadata": {},
   "source": [
    "## 10.7 학습된 모델을 사용하여 텍스트에서 음성 합성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-virus",
   "metadata": {},
   "source": [
    "### 학습된 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-visibility",
   "metadata": {},
   "source": [
    "#### Tacotron 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_config = OmegaConf.load(f\"exp/jsut_sr16000/{acoustic_config_name}/model.yaml\")\n",
    "acoustic_model = hydra.utils.instantiate(acoustic_config.netG)\n",
    "checkpoint = torch.load(f\"exp/jsut_sr16000/{acoustic_config_name}/latest.pth\", map_location=device)\n",
    "acoustic_model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "acoustic_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-wages",
   "metadata": {},
   "source": [
    "#### WaveNet 보코더 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet_config = OmegaConf.load(f\"exp/jsut_sr16000/{wavenet_config_name}/model.yaml\")\n",
    "wavenet_model = hydra.utils.instantiate(wavenet_config.netG)\n",
    "checkpoint = torch.load(f\"exp/jsut_sr16000/{wavenet_config_name}/latest_ema.pth\", map_location=device)\n",
    "wavenet_model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "# weight normalization 은 추론시에는 불필요하므로 제외\n",
    "wavenet_model.remove_weight_norm_()\n",
    "wavenet_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-module",
   "metadata": {},
   "source": [
    "#### 통계량 로드\n",
    "\n",
    "통계는 Griffin-Lim 알고리즘을 사용하는 경우에만 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_out_scaler = joblib.load(\"./dump/jsut_sr16000/norm/out_tacotron_scaler.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-brisbane",
   "metadata": {},
   "source": [
    "### 멜 스펙트로그램 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.util import find_lab, find_feats\n",
    "\n",
    "labels = hts.load(find_lab(\"downloads/jsut-label/\", test_utt))\n",
    "\n",
    "in_feats = text_to_sequence(pp_symbols(labels.contexts))\n",
    "in_feats = torch.tensor(in_feats, dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_feats, out_feats_fine, stop_flags, alignment = acoustic_model.inference(in_feats)\n",
    "    \n",
    "# 비교를 위해 자연음성에서 추출한 음향 특징량을 읽기\n",
    "feats = np.load(find_feats(\"dump/jsut_sr16000/norm/\", test_utt, typ=\"out_tacotron\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-peoples",
   "metadata": {},
   "source": [
    "#### 멜 스펙트로그램 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
    "ax[0].set_title(\"Mel-spectrogram of natural speech\")\n",
    "ax[1].set_title(\"Mel-spectrogram of Tacotron output\")\n",
    "\n",
    "mindb = min(feats.min(), out_feats_fine.min())\n",
    "maxdb = max(feats.max(), out_feats_fine.max())\n",
    "\n",
    "hop_length = int(sr * 0.0125)\n",
    "mesh = librosa.display.specshow(\n",
    "    feats.T, sr=sr, x_axis=\"time\", y_axis=\"frames\", hop_length=hop_length, cmap=cmap, ax=ax[0])\n",
    "mesh.set_clim(mindb, maxdb)\n",
    "fig.colorbar(mesh, ax=ax[0])\n",
    "mesh = librosa.display.specshow(\n",
    "    out_feats_fine.data.numpy().T, sr=sr, x_axis=\"time\", y_axis=\"frames\", hop_length=hop_length, cmap=cmap, ax=ax[1])\n",
    "mesh.set_clim(mindb, maxdb)\n",
    "fig.colorbar(mesh, ax=ax[1])\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Mel filter channel\")\n",
    "fig.tight_layout()\n",
    "\n",
    "# 그림 10-8\n",
    "savefig(\"./fig/e2etts_impl_logmel_comp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-shadow",
   "metadata": {},
   "source": [
    "#### 어텐션 가중치 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "im = ax.imshow(alignment.cpu().data.numpy().T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\")\n",
    "fig.colorbar(im, ax=ax)\n",
    "ax.set_xlabel(\"Decoder time step [frame]\")\n",
    "ax.set_ylabel(\"Encoder time step [phoneme]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-conversion",
   "metadata": {},
   "source": [
    "#### Stop token 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(torch.sigmoid(stop_flags).cpu().numpy())\n",
    "ax.set_xlabel(\"Time [frame]\")\n",
    "ax.set_ylabel(\"Stop probability\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-break",
   "metadata": {},
   "source": [
    "### 음성 파형 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dsp import inv_mulaw_quantize\n",
    "\n",
    "@torch.no_grad()\n",
    "def gen_waveform(wavenet_model, out_feats):\n",
    "    # (B, T, C) -> (B, C, T)\n",
    "    c = out_feats.view(1, -1, out_feats.size(-1)).transpose(1, 2)\n",
    "\n",
    "    # 오디오 샘플 수 계산\n",
    "    upsample_scale = np.prod(wavenet_model.upsample_scales)\n",
    "    T = (\n",
    "        c.shape[-1] - wavenet_model.aux_context_window * 2\n",
    "    ) * upsample_scale\n",
    "\n",
    "    # WaveNet으로 음성 파형 생성\n",
    "    # 참고 : 계산에 시간이 오래 걸리므로 tqdm의 진행률 표시 줄을 수락합니다.\n",
    "    gen_wav = wavenet_model.inference(c, T, tqdm)\n",
    "\n",
    "    # One-hot 벡터를 1차원 신호로 변환\n",
    "    gen_wav = gen_wav.max(1)[1].float().cpu().numpy().reshape(-1)\n",
    "\n",
    "    # Mu-law 양자화의 역변환\n",
    "    gen_wav = inv_mulaw_quantize(\n",
    "        gen_wav, wavenet_model.out_channels - 1\n",
    "    )\n",
    "    \n",
    "    return gen_wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-wildlife",
   "metadata": {},
   "source": [
    "### 모든 모델을 결합하여 음성 파형 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.util import find_lab, find_feats\n",
    "from ttslearn.dsp import logmelspectrogram_to_audio\n",
    "\n",
    "# WaveNet 보코더 대신 Griffin-Lim 알고리즘을 사용하는 경우 다음을 True로 설정하십시오.\n",
    "griffin_lim = False\n",
    "\n",
    "labels = hts.load(find_lab(\"downloads/jsut-label/\", test_utt))\n",
    "in_feats = text_to_sequence(pp_symbols(labels.contexts))\n",
    "in_feats = torch.tensor(in_feats, dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, out_feats, _, _ = acoustic_model.inference(in_feats)\n",
    "    \n",
    "if griffin_lim:\n",
    "    # Griffin-Lim 알고리즘을 기반으로 음성 파형 생성\n",
    "    out_feats = out_feats.cpu().data.numpy()\n",
    "    # 정규화의 역변환\n",
    "    logmel = acoustic_out_scaler.inverse_transform(out_feats)\n",
    "    gen_wav = logmelspectrogram_to_audio(logmel, sr)\n",
    "else:\n",
    "    # WaveNet 보코더로 음성 파형 생성\n",
    "    gen_wav = gen_waveform(wavenet_model, out_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비교를 위해 원래 음성 로드\n",
    "from scipy.io import wavfile\n",
    "_sr, ref_wav = wavfile.read(f\"./downloads/jsut_ver1.1/basic5000/wav/{test_utt}.wav\")\n",
    "ref_wav = (ref_wav / 32768.0).astype(np.float64)\n",
    "ref_wav = librosa.resample(ref_wav, _sr, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
    "\n",
    "hop_length = int(sr * 0.005)\n",
    "fft_size = pyworld.get_cheaptrick_fft_size(sr)\n",
    "\n",
    "# Tacotron의 출력과 거칠게 정렬하기 위해 자연 음성의 시작과 끝의 무음 구간 삭제\n",
    "ref_wav_trim = librosa.effects.trim(ref_wav, top_db=20)[0]\n",
    "\n",
    "spec_ref = librosa.stft(ref_wav_trim, n_fft=fft_size, hop_length=hop_length, window=\"hann\")\n",
    "logspec_ref = np.log(np.abs(spec_ref))\n",
    "spec_gen = librosa.stft(gen_wav, n_fft=fft_size, hop_length=hop_length, window=\"hann\")\n",
    "logspec_gen = np.log(np.abs(spec_gen))\n",
    "\n",
    "mindb = min(logspec_ref.min(), logspec_gen.min())\n",
    "maxdb = max(logspec_ref.max(), logspec_gen.max())\n",
    "\n",
    "mesh = librosa.display.specshow(logspec_ref, hop_length=hop_length, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax[0])\n",
    "mesh.set_clim(mindb, maxdb)\n",
    "fig.colorbar(mesh, ax=ax[0], format=\"%+2.fdB\")\n",
    "\n",
    "mesh = librosa.display.specshow(logspec_gen, hop_length=hop_length, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax[1])\n",
    "mesh.set_clim(mindb, maxdb)\n",
    "fig.colorbar(mesh, ax=ax[1], format=\"%+2.fdB\")\n",
    "\n",
    "ax[0].set_title(\"Spectrogram of natural speech\")\n",
    "ax[1].set_title(\"Spectrogram of generated speech\")\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Frequency [Hz]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"자연 음성\")\n",
    "IPython.display.display(Audio(ref_wav_trim, rate=sr))\n",
    "print(\"Tacotron 2에 의한 합성 음성\")\n",
    "IPython.display.display(Audio(gen_wav, rate=sr))\n",
    "\n",
    "# 그림 10-9\n",
    "savefig(\"./fig/e2etts_impl_tts_spec_comp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-parliament",
   "metadata": {},
   "source": [
    "### 합성 음성의 보다 상세한 비교 (bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비교를 위해 자연 음성에서 추출한 멜 스펙트로 그램에서 음성 파형을 생성합니다.\n",
    "feats = np.load(find_feats(\"dump/jsut_sr16000/norm/\", test_utt, typ=\"out_tacotron\"))\n",
    "feats = torch.from_numpy(feats)\n",
    "gen_wav_wn_gt = gen_waveform(wavenet_model, feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_wav_inv = np.load(find_feats(\"./dump/jsut_sr16000/org/\", test_utt, typ=\"out_wavenet\"))\n",
    "ref_wav_inv = inv_mulaw_quantize(ref_wav_inv, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"자연 음성\")\n",
    "IPython.display.display(Audio(ref_wav, rate=sr))\n",
    "print(\"자연 음성 (8-bit mu-law)\")\n",
    "IPython.display.display(Audio(ref_wav_inv, rate=sr))\n",
    "print(\"WaveNet 보코더 출력\")\n",
    "IPython.display.display(Audio(gen_wav_wn_gt, rate=sr))\n",
    "print(\"Tacotron + WaveNet 보코더 출력\")\n",
    "IPython.display.display(Audio(gen_wav, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-medication",
   "metadata": {},
   "source": [
    "### 평가 데이터에 대한 음성 파형 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-greenhouse",
   "metadata": {},
   "source": [
    "#### 레시피 stage 5 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 5 --stop-stage 5 --acoustic-model $acoustic_config_name \\\n",
    "        --tqdm $run_sh_tqdm --wavenet-model $wavenet_config_name \\\n",
    "        --reverse true --num-eval-utts $num_eval_utts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-writer",
   "metadata": {},
   "source": [
    "#### 레시피 stage 6 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-harvest",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh and run_stage6:\n",
    "    ! ./run.sh --stage 6 --stop-stage 6 --acoustic-model $acoustic_config_name \\\n",
    "        --tqdm $run_sh_tqdm --wavenet-model $wavenet_config_name \\\n",
    "        --reverse true --num-eval-utts $num_eval_utts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-poverty",
   "metadata": {},
   "source": [
    "## 자연 음성과 합성 음성의 비교 (bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from ttslearn.util import load_utt_list\n",
    "\n",
    "with open(\"./downloads/jsut_ver1.1/basic5000/transcript_utf8.txt\") as f:\n",
    "    transcripts = {}\n",
    "    for l in f:\n",
    "        utt_id, script = l.split(\":\")\n",
    "        transcripts[utt_id] = script\n",
    "        \n",
    "eval_list = load_utt_list(\"data/eval.list\")[::-1][:5]\n",
    "\n",
    "for utt_id in eval_list:\n",
    "    # ref file \n",
    "    ref_file = f\"./downloads/jsut_ver1.1/basic5000/wav/{utt_id}.wav\"\n",
    "    _sr, ref_wav = wavfile.read(ref_file)\n",
    "    ref_wav = (ref_wav / 32768.0).astype(np.float64)\n",
    "    ref_wav = librosa.resample(ref_wav, _sr, sr)\n",
    "  \n",
    "    print(f\"{utt_id}: {transcripts[utt_id]}\")\n",
    "    print(\"자연 음성\")\n",
    "    IPython.display.display(Audio(ref_wav, rate=sr))\n",
    "\n",
    "    gen_file = f\"exp/jsut_sr16000/synthesis_{acoustic_config_name}_griffin_lim/eval/{utt_id}.wav\"\n",
    "    if exists(gen_file):\n",
    "        _sr, gen_wav = wavfile.read(gen_file)\n",
    "        print(\"Tacotron + Griffin-Lim\")\n",
    "        IPython.display.display(Audio(gen_wav, rate=sr))\n",
    "    else:\n",
    "        print(\"Tacotron + Griffin-Lim: not found\")\n",
    "\n",
    "    gen_file_wn = f\"exp/jsut_sr16000/synthesis_{acoustic_config_name}_{wavenet_config_name}/eval/{utt_id}.wav\"\n",
    "    if exists(gen_file_wn):\n",
    "        _sr, gen_wav_wn = wavfile.read(gen_file_wn)\n",
    "        print(\"Tacotron + WaveNet 보코더\")\n",
    "        IPython.display.display(Audio(gen_wav_wn, rate=sr))\n",
    "    else:\n",
    "        print(\"Tacotron + WaveNet 보코더: not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-labor",
   "metadata": {},
   "source": [
    "## 학습된 모델 패키징 (bonus)\n",
    "\n",
    "학습된 모델을 사용하는 TTS에 필요한 모든 파일을 단일 디렉토리로 그룹화합니다.\n",
    "`ttslearn.tacotron.Tacotron2TTS` 클래스에는, 정리한 디렉토리를 지정해, TTS를 실시하는 기능이 구현되고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-contribution",
   "metadata": {},
   "source": [
    "### 레시피 stage 99 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 99 --stop-stage 99 --acoustic-model $acoustic_config_name \\\n",
    "        --wavenet-model $wavenet_config_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls tts_models/jsut_sr16000_{acoustic_config_name}_{wavenet_config_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-provider",
   "metadata": {},
   "source": [
    "### 패키징된 모델을 이용한 TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.tacotron import Tacotron2TTS\n",
    "\n",
    "# 패키징된 모델의 경로를 지정합니다.\n",
    "engine = Tacotron2TTS(\n",
    "    model_dir=f\"./tts_models/jsut_sr16000_{acoustic_config_name}_{wavenet_config_name}\"\n",
    ")\n",
    "wav, sr = engine.tts(\"ここまでお読みいただき、ありがとうございました。\", tqdm=tqdm)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,2))\n",
    "librosa.display.waveshow(wav.astype(np.float32), sr, ax=ax)\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()\n",
    "\n",
    "Audio(wav, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_colab():\n",
    "    from datetime import timedelta\n",
    "    elapsed = (time.time() - start_time)\n",
    "    print(\"소요시간:\", str(timedelta(seconds=elapsed)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}