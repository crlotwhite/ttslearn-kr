{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "civic-merchandise",
   "metadata": {},
   "source": [
    "# 제6장: 일본어 DNN 음성 합성 시스템 구현\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/r9y9/ttslearn/blob/master/notebooks/ch06_Recipe-DNNTTS.ipynb)\n",
    "\n",
    "Google colab에서 실행하는 데 예상되는 소요 시간: 1시간\n",
    "\n",
    "이 노트북의 레시피 설정은 Google Colab에서 실행할 때 시간 초과를 피하기 위해 학습 조건을 책에 나열된 설정에서 일부 수정한 것입니다. (배치 크기 줄이기 등).\n",
    "참고로 책에 기재된 조건으로 저자(야마모토)가 레시피를 실행한 결과를 아래에서 공개하고 있습니다.\n",
    "\n",
    "- Tensorboard logs: https://tensorboard.dev/experiment/ajmqiymoTx6rADKLF8d6sA/\n",
    "- exp 디렉토리 (학습 모델, 합성 음성 포함) : https://drive.google.com/file/d/171gGoH3H4PJ-9cMQES-l6KpTu9n0udGD/view?usp=sharing (12.8 MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-religion",
   "metadata": {},
   "source": [
    "## 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-murray",
   "metadata": {},
   "source": [
    "### Google Colab을 이용하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-instrumentation",
   "metadata": {},
   "source": [
    "Google Colab에서 이 노트북을 실행하려면 메뉴의 '런타임 -> 런타임 시간 변경'에서 '하드웨어 가속기'를 **GPU**로 변경하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-premium",
   "metadata": {},
   "source": [
    "### Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -VV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-ownership",
   "metadata": {},
   "source": [
    "### ttslearn 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import ttslearn\n",
    "except ImportError:\n",
    "    !pip install ttslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttslearn\n",
    "ttslearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-coaching",
   "metadata": {},
   "source": [
    "## 6.1 이 장의 일본어 음성 합성 시스템 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-champagne",
   "metadata": {},
   "source": [
    "### 학습된 모델을 이용한 음성 합성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dnntts import DNNTTS\n",
    "from IPython.display import Audio\n",
    "\n",
    "engine = DNNTTS()\n",
    "wav, sr = engine.tts(\"深層学習に基づく音声合成システムです。\")\n",
    "Audio(wav, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,2))\n",
    "librosa.display.waveshow(wav.astype(np.float32), sr, ax=ax)\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-relations",
   "metadata": {},
   "source": [
    "### 레시피 실행 전 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from ttslearn.env import is_colab\n",
    "from os.path import exists\n",
    "\n",
    "# pip install ttslearn은 레시피를 설치하지 않으므로 수동으로 다운로드\n",
    "if is_colab() and not exists(\"recipes.zip\"):\n",
    "    !curl -LO https://github.com/r9y9/ttslearn/releases/download/v{ttslearn.__version__}/recipes.zip\n",
    "    !unzip -o recipes.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-mount",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# recipe 디렉토리로 이동\n",
    "cwd = os.getcwd()\n",
    "if cwd.endswith(\"notebooks\"):\n",
    "    os.chdir(\"../recipes/dnntts/\")\n",
    "elif is_colab():\n",
    "    os.chdir(\"recipes/dnntts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-interaction",
   "metadata": {},
   "source": [
    "### 패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "import tensorboard as tb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치 연산\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "# 음성 파형 불러오기\n",
    "from scipy.io import wavfile\n",
    "#  풀 컨텍스트 라벨, 질의 파일 로드\n",
    "from nnmnkwii.io import hts\n",
    "# 음성 분석\n",
    "import pysptk\n",
    "import pyworld\n",
    "# 음성 분석, 시각화\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "# 파이썬에서 배우는 음성 합성\n",
    "import ttslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정\n",
    "from ttslearn.util import init_seed\n",
    "init_seed(773)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-spirituality",
   "metadata": {},
   "source": [
    "### 그래프 그리기 설정 (描画周りの設定) // 번역 수정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.notebook import get_cmap, init_plot_style, savefig\n",
    "cmap = get_cmap()\n",
    "init_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-relevance",
   "metadata": {},
   "source": [
    "### 레시피 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.sh를 사용하여 학습 스크립트를 노트북에서 실행하려면 True\n",
    "# google colab의 경우 True라고 가정합니다.\n",
    "# 로컬 환경의 경우 run.sh를 터미널에서 실행하는 것이 좋습니다.\n",
    "# 이 경우이 노트북은 시각화 및 학습 된 모델을 테스트하는 데 사용됩니다.\n",
    "run_sh = is_colab()\n",
    "\n",
    "# CUDA\n",
    "# NOTE: run.sh의 인수로 전달하기 때문에 bool이 아닌 문자열로 정의합니다.\n",
    "cudnn_benchmark = \"true\"\n",
    "cudnn_deterministic = \"false\"\n",
    "\n",
    "# 특징(feature) 추출시 병렬 처리 작업 수\n",
    "n_jobs = os.cpu_count()//2\n",
    "\n",
    "# 연속 길이 모델의 설정 파일 이름\n",
    "duration_config_name=\"duration_dnn\"\n",
    "# 음향 모델 설정 파일 이름\n",
    "acoustic_config_name=\"acoustic_dnn_sr16k\"\n",
    "\n",
    "# 연속 길이 모델 및 음향 모델 학습의 배치 크기\n",
    "batch_size = 32\n",
    "# 지속 길이 모델 및 음향 모델 학습의 에포크 수\n",
    "# 참고: 계산 시간을 줄이기 위해 다소 적게 설정했습니다. 품질을 높이려면 30-50 에포크 수를 사용해보십시오.\n",
    "nepochs = 10\n",
    "\n",
    "# run.sh를 통해 실행되는 스크립트의 tqdm\n",
    "run_sh_tqdm = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노트북에서 사용하는 테스트용 발화(학습 데이터, 평가 데이터)\n",
    "train_utt = \"BASIC5000_0001\"\n",
    "test_utt = \"BASIC5000_5000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-maximum",
   "metadata": {},
   "source": [
    "### Tensorboard로 로그 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노트북에서 tensorboard 로깅을 확인하려면 다음 행을 활성화하십시오.\n",
    "if is_colab():\n",
    "    %tensorboard --logdir tensorboard/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-notion",
   "metadata": {},
   "source": [
    "## 6.2 프로그램 구현 전 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-matter",
   "metadata": {},
   "source": [
    "### stage -1: 코퍼스 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_colab():\n",
    "    ! ./run.sh --stage -1 --stop-stage -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-synthesis",
   "metadata": {},
   "source": [
    "### Stage 0: 학습/검증/평가 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 0 --stop-stage 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head data/dev.list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-trail",
   "metadata": {},
   "source": [
    "## 6.3 연속 길이 모델에 대한 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-avatar",
   "metadata": {},
   "source": [
    "### 연속 길이 모델을 위한 1 발화에 대한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttslearn\n",
    "from nnmnkwii.io import hts\n",
    "from nnmnkwii.frontend import merlin as fe\n",
    "\n",
    "# 언어 특징량 추출에 사용하기 위한 질문 파일\n",
    "binary_dict, numeric_dict = hts.load_question_set(ttslearn.util.example_qst_file())\n",
    "\n",
    "# 음성의 풀 컨텍스트 라벨 로드\n",
    "labels = hts.load(ttslearn.util.example_label_file())\n",
    "\n",
    "# 연속 길이 모델 입력: 언어 특징량\n",
    "in_feats = fe.linguistic_features(labels, binary_dict, numeric_dict)\n",
    "\n",
    "# 연속 길이 모델 출력: 음소 연속 길이\n",
    "out_feats = fe.duration_features(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"入力特徴量のサイズ:\", in_feats.shape)\n",
    "print(\"出力特徴量のサイズ:\", out_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화를 위해 정규화\n",
    "in_feats_norm = in_feats / np.maximum(1, np.abs(in_feats).max(0))\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
    "ax[0].set_title(\"Duration model's input: linguistic features\")\n",
    "ax[1].set_title(\"Duration model's output: phoneme durations\")\n",
    "ax[0].imshow(in_feats_norm.T, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\", cmap=cmap)\n",
    "ax[0].set_ylabel(\"Context\")\n",
    "\n",
    "ax[1].bar(np.arange(len(out_feats)), out_feats.reshape(-1))\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlim(-0.5, len(in_feats)-0.5)\n",
    "    a.set_xlabel(\"Phoneme\")\n",
    "ax[1].set_ylabel(\"Duration (the number of frames)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 6-3\n",
    "savefig(\"fig/dnntts_impl_duration_inout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-rehabilitation",
   "metadata": {},
   "source": [
    "### 레시피의 stage 1 실행\n",
    "\n",
    "배치 처리를 실시하는 커멘드 라인 프로그램은, `preprocess_duration.py`를 참조해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 1 --stop-stage 1 --n-jobs $n_jobs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-representation",
   "metadata": {},
   "source": [
    "## 6.4: 음향 모델을 위한 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-interstate",
   "metadata": {},
   "source": [
    "### 음향 모델을 위한 1 발화에 대한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dsp import world_spss_params\n",
    "\n",
    "# 언어 특징량 추출에 사용하기 위한 질의 파일\n",
    "binary_dict, numeric_dict = hts.load_question_set(ttslearn.util.example_qst_file())\n",
    "\n",
    "# 음성의 풀 컨텍스트 라벨 로드\n",
    "labels = hts.load(ttslearn.util.example_label_file())\n",
    "\n",
    "# 음향 모델 입력: 언어 특징량\n",
    "in_feats = fe.linguistic_features(labels, binary_dict, numeric_dict, add_frame_features=True, subphone_features=\"coarse_coding\")\n",
    "\n",
    "# 음성 파일 읽기\n",
    "_sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "sr = 16000\n",
    "x = (x / 32768).astype(np.float64)\n",
    "x = librosa.resample(x, _sr, sr)\n",
    "\n",
    "# 음향 모델 출력: 음향 특징량\n",
    "out_feats = world_spss_params(x, sr)\n",
    "\n",
    "# 프레임 수 조정\n",
    "minL = min(in_feats.shape[0], out_feats.shape[0])\n",
    "in_feats, out_feats = in_feats[:minL], out_feats[:minL]\n",
    "\n",
    "# 시작과 끝의 음성이 아닌 구간의 길이 조정\n",
    "assert \"sil\" in labels.contexts[0] and \"sil\" in labels.contexts[-1]\n",
    "start_frame = int(labels.start_times[1] / 50000)\n",
    "end_frame = int(labels.end_times[-2] / 50000)\n",
    "\n",
    "# 시작: 50ms, 후미: 100ms\n",
    "start_frame = max(0, start_frame - int(0.050 / 0.005))\n",
    "end_frame = min(minL, end_frame + int(0.100 / 0.005))\n",
    "\n",
    "in_feats = in_feats[start_frame:end_frame]\n",
    "out_feats = out_feats[start_frame:end_frame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"입력 특징량의 크기:\", in_feats.shape)\n",
    "print(\"출력 특징량의 크기:\", out_feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-assault",
   "metadata": {},
   "source": [
    "#### 음향 특징량을 분리하여 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dnntts.multistream import get_static_features\n",
    "\n",
    "sr = 16000\n",
    "hop_length = int(sr * 0.005)\n",
    "alpha = pysptk.util.mcepalpha(sr)\n",
    "fft_size = pyworld.get_cheaptrick_fft_size(sr)\n",
    "\n",
    "# 동적 특징량을 제외하고 각 음향 특징량을 꺼내\n",
    "mgc, lf0, vuv, bap = get_static_features(\n",
    "    out_feats, num_windows=3, stream_sizes=[120, 3, 1, 3],\n",
    "    has_dynamic_features=[True, True, False, True]) \n",
    "print(\"멜 캡스트럼의 크기:\", mgc.shape)\n",
    "print(\"연속 로그 기본 주파수의 크기:\", lf0.shape)\n",
    "print(\"유성/무성 플래그의 크기:\", vuv.shape)\n",
    "print(\"대역 비주기성 지표의 크기:\", bap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_out_feats(mgc, lf0, vuv, bap):\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(8,8))\n",
    "    ax[0].set_title(\"Spectral envelope\")\n",
    "    ax[1].set_title(\"Fundamental frequency\")\n",
    "    ax[2].set_title(\"Aperiodicity\")\n",
    "    \n",
    "    logsp = np.log(pysptk.mc2sp(mgc, alpha, fft_size))\n",
    "    librosa.display.specshow(logsp.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"hz\", cmap=cmap, ax=ax[0])\n",
    "    \n",
    "    timeaxis = np.arange(len(lf0)) * 0.005\n",
    "    f0 = np.exp(lf0)\n",
    "    f0[vuv < 0.5] = 0\n",
    "    ax[1].plot(timeaxis, f0, linewidth=2)\n",
    "    ax[1].set_xlim(0, len(f0)*0.005)\n",
    "\n",
    "    aperiodicity = pyworld.decode_aperiodicity(bap.astype(np.float64), sr, fft_size)\n",
    "    librosa.display.specshow(aperiodicity.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"hz\", cmap=cmap, ax=ax[2])\n",
    "    \n",
    "    for a in ax:\n",
    "        a.set_xlabel(\"Time [sec]\")\n",
    "        a.set_ylabel(\"Frequency [Hz]\")\n",
    "        # 말미의 비음성 구간 제외\n",
    "        a.set_xlim(0, 2.55)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "# 음향 특징량의 시각화\n",
    "vis_out_feats(mgc, lf0, vuv, bap)\n",
    "# 그림 6-4\n",
    "savefig(\"./fig/dnntts_impl_acoustic_out_feats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-coalition",
   "metadata": {},
   "source": [
    "#### 음향 모델 입력 및 출력 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화를 위해 정규화\n",
    "from scipy.stats import zscore \n",
    "\n",
    "in_feats_norm = in_feats / np.maximum(1, np.abs(in_feats).max(0))\n",
    "out_feats_norm = zscore(out_feats)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
    "ax[0].set_title(\"Acoustic model's input: linguistic features\")\n",
    "ax[1].set_title(\"Acoustic model's output: acoustic features\")\n",
    "mesh = librosa.display.specshow(\n",
    "    in_feats_norm.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\", ax=ax[0], cmap=cmap)\n",
    "fig.colorbar(mesh, ax=ax[0])\n",
    "mesh = librosa.display.specshow(\n",
    "    out_feats_norm.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\",ax=ax[1], cmap=cmap)\n",
    "# NOTE: 실제로는 [-4, 4] 범위를 벗어난 값이 있지만 시인성을 위해 [-4, 4]로 설정합니다.\n",
    "mesh.set_clim(-4, 4)\n",
    "fig.colorbar(mesh, ax=ax[1])\n",
    "\n",
    "ax[0].set_ylabel(\"Context\")\n",
    "ax[1].set_ylabel(\"Feature\")\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    # 말미의 비음성 구간 제외\n",
    "    a.set_xlim(0, 2.55)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-region",
   "metadata": {},
   "source": [
    "### 레시피의 stage 2 실행\n",
    "\n",
    "배치 처리를 실시하는 커멘드 라인 프로그램은, `preprocess_acoustic.py`를 참조해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 2 --stop-stage 2 --n-jobs $n_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-surveillance",
   "metadata": {},
   "source": [
    "## 6.5 특징량 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-albuquerque",
   "metadata": {},
   "source": [
    "정규화를 위한 통계량을 계산하는 명령행 프로그램은 `recipes/common/fit_scaler.py`를 참조하십시오. 또, 정규화를 실시하는 커멘드 라인 프로그램은, `recipes/common/preprocess_normalize.py` 를 참조해 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-error",
   "metadata": {},
   "source": [
    "### 레시피의 stage 3 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 3 --stop-stage 3 --n-jobs $n_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-grade",
   "metadata": {},
   "source": [
    "### 정규화 처리 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언어 특징량 정규화 전후\n",
    "in_feats = np.load(f\"dump/jsut_sr16000/org/train/in_acoustic/{train_utt}-feats.npy\")\n",
    "in_feats_norm = np.load(f\"dump/jsut_sr16000/norm/train/in_acoustic/{train_utt}-feats.npy\")\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
    "ax[0].set_title(\"Linguistic features (before normalization)\")\n",
    "ax[1].set_title(\"Linguistic features (after normalization)\")\n",
    "mesh = librosa.display.specshow(\n",
    "    in_feats.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\", ax=ax[0], cmap=cmap)\n",
    "fig.colorbar(mesh, ax=ax[0])\n",
    "mesh = librosa.display.specshow(\n",
    "    in_feats_norm.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\",ax=ax[1], cmap=cmap)\n",
    "# 참고 : 실제로는 [-4, 4] 범위를 벗어난 값이 있지만 가시성을 위해 [-4, 4]로 설정합니다.\n",
    "mesh.set_clim(-4, 4)\n",
    "fig.colorbar(mesh, ax=ax[1])\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Context\")\n",
    "    # 말미의 비음성 구간 제외\n",
    "    a.set_xlim(0, 2.55) \n",
    "plt.tight_layout()\n",
    "# 그림 6-5\n",
    "savefig(\"./fig/dnntts_impl_in_feats_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음향 특징량의 정규화 전후\n",
    "out_feats = np.load(f\"dump/jsut_sr16000/org/train/out_acoustic/{train_utt}-feats.npy\")\n",
    "out_feats_norm = np.load(f\"dump/jsut_sr16000/norm/train/out_acoustic/{train_utt}-feats.npy\")\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
    "ax[0].set_title(\"Acoustic features (before normalization)\")\n",
    "ax[1].set_title(\"Acoustic features (after normalization)\")\n",
    "mesh = librosa.display.specshow(\n",
    "    out_feats.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\", ax=ax[0], cmap=cmap)\n",
    "fig.colorbar(mesh, ax=ax[0])\n",
    "mesh = librosa.display.specshow(\n",
    "    out_feats_norm.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\",ax=ax[1], cmap=cmap)\n",
    "# NOTE : 실제로는 [-4, 4] 범위를 벗어난 값이 있지만 가시성을 위해 [-4, 4]로 설정합니다.\n",
    "mesh.set_clim(-4, 4)\n",
    "fig.colorbar(mesh, ax=ax[1])\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Feature\")\n",
    "    # 말미의 비음성 구간 제외\n",
    "    a.set_xlim(0, 2.55)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-federal",
   "metadata": {},
   "source": [
    "## 6.6 신경망 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-marine",
   "metadata": {},
   "source": [
    "### 완전 결합형 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=2):\n",
    "        super(DNN, self).__init__()\n",
    "        model = [nn.Linear(in_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers):\n",
    "            model.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            model.append(nn.ReLU())\n",
    "        model.append(nn.Linear(hidden_dim, out_dim))\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x, lens=None):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN(in_dim=325, hidden_dim=64, out_dim=1, num_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-boutique",
   "metadata": {},
   "source": [
    "### LSTM-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class LSTMRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_dim, hidden_dim, out_dim, num_layers=1, bidirectional=True, dropout=0.0\n",
    "    ):\n",
    "        super(LSTMRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        num_direction = 2 if bidirectional else 1\n",
    "        self.lstm = nn.LSTM(\n",
    "            in_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.hidden2out = nn.Linear(num_direction * hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, seqs, lens):\n",
    "        seqs = pack_padded_sequence(seqs, lens, batch_first=True)\n",
    "        out, _ = self.lstm(seqs)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True)\n",
    "        out = self.hidden2out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMRNN(in_dim=325, hidden_dim=64, out_dim=1, num_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-elder",
   "metadata": {},
   "source": [
    "## 6.7 학습 스크립트 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-nashville",
   "metadata": {},
   "source": [
    "### DataLoader 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-sword",
   "metadata": {},
   "source": [
    "#### Dataset 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data as data_utils\n",
    "\n",
    "class Dataset(data_utils.Dataset):\n",
    "    def __init__(self, in_paths, out_paths):\n",
    "        self.in_paths = in_paths\n",
    "        self.out_paths = out_paths\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return np.load(self.in_paths[idx]), np.load(self.out_paths[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.in_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-texas",
   "metadata": {},
   "source": [
    "#### DataLoader 사용 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from ttslearn.util import pad_2d\n",
    "\n",
    "def collate_fn_dnntts(batch):\n",
    "    lengths = [len(x[0]) for x in batch]\n",
    "    max_len = max(lengths)\n",
    "    x_batch = torch.stack([torch.from_numpy(pad_2d(x[0], max_len)) for x in batch])\n",
    "    y_batch = torch.stack([torch.from_numpy(pad_2d(x[1], max_len)) for x in batch])\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    return x_batch, y_batch, lengths\n",
    "\n",
    "\n",
    "in_paths = sorted(Path(\"./dump/jsut_sr16000/norm/dev/in_duration/\").glob(\"*.npy\"))\n",
    "out_paths = sorted(Path(\"./dump/jsut_sr16000/norm/dev/out_duration/\").glob(\"*.npy\"))\n",
    "\n",
    "dataset = Dataset(in_paths, out_paths)\n",
    "data_loader = data_utils.DataLoader(dataset, batch_size=8, collate_fn=collate_fn_dnntts, num_workers=0)\n",
    "\n",
    "in_feats, out_feats, lengths = next(iter(data_loader))\n",
    "\n",
    "print(\"입력 특징량의 크기:\", tuple(in_feats.shape))\n",
    "print(\"출력 특징량의 크기:\", tuple(out_feats.shape))\n",
    "print(\"계열 길이의 크기:\", tuple(lengths.shape))\n",
    "# 계열: seq2seq에서 sequence에 해당하는 단어로 보입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-belfast",
   "metadata": {},
   "source": [
    "#### ミニバッチの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(in_feats), 1, figsize=(8,10), sharex=True, sharey=True)\n",
    "for n in range(len(in_feats)):\n",
    "    x = in_feats[n].data.numpy()\n",
    "    mesh = ax[n].imshow(x.T, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\", cmap=cmap)\n",
    "    fig.colorbar(mesh, ax=ax[n])\n",
    "    # NOTE : 실제로는 [-4, 4] 범위를 벗어난 값이 있지만 가시성을 위해 [-4, 4]로 설정합니다.\n",
    "    mesh.set_clim(-4, 4)\n",
    "    \n",
    "ax[-1].set_xlabel(\"Phoneme\")\n",
    "for a in ax:\n",
    "    a.set_ylabel(\"Context\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 6-6\n",
    "savefig(\"fig/dnntts_impl_minibatch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-abortion",
   "metadata": {},
   "source": [
    "### 학습 전 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dnntts import DNN\n",
    "from torch import optim\n",
    "\n",
    "model = DNN(in_dim=325, hidden_dim=64, out_dim=1, num_layers=2)\n",
    "\n",
    "# lr은 학습률을 나타냅니다.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# gamma는 학습률의 감쇠 계수를 나타냅니다.\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, gamma=0.5, step_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-humidity",
   "metadata": {},
   "source": [
    "### 학습 루프 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader를 사용하여 미니 배치 생성 : 미니 일괄 처리\n",
    "for in_feats, out_feats, lengths in data_loader:\n",
    "    # 순전파 계산\n",
    "    pred_out_feats = model(in_feats, lengths)\n",
    "    # 손실 계산\n",
    "    loss = nn.MSELoss()(pred_out_feats, out_feats)\n",
    "    # 손실 값을 출력\n",
    "    print(loss.item())\n",
    "    # optimizer에 축적 된 기울기를 재설정\n",
    "    optimizer.zero_grad()\n",
    "    # 오차의 역전파 계산\n",
    "    loss.backward()\n",
    "    # 매개변수 업데이트\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-damage",
   "metadata": {},
   "source": [
    "### hydra를 이용한 커맨드 라인 프로그램 구현\n",
    "\n",
    "`hydra/hydra_quick_start`와 `hydra/hydra_composision`을 참조하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-failing",
   "metadata": {},
   "source": [
    "### hydra를 사용한 실용적인 학습 스크립트 구성 파일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-shell",
   "metadata": {},
   "source": [
    "`conf/train_dnntts` 디렉토리를 참조하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat conf/train_dnntts/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-performance",
   "metadata": {},
   "source": [
    "### hydra를 사용하여 실용적인 학습 스크립트 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-newman",
   "metadata": {},
   "source": [
    "`train_dnntts.py`를 참조하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-screening",
   "metadata": {},
   "source": [
    "## 6.8 연속 길이 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-damage",
   "metadata": {},
   "source": [
    "### 연속 길이 모델의 설정 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat conf/train_dnntts/model/{duration_config_name}.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-batch",
   "metadata": {},
   "source": [
    "### 연속 길이 모델 인스턴스화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "hydra.utils.instantiate(OmegaConf.load(f\"conf/train_dnntts/model/{duration_config_name}.yaml\").netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-toddler",
   "metadata": {},
   "source": [
    "### 레시피 stage 4 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 4 --stop-stage 4 --duration-model $duration_config_name \\\n",
    "    --tqdm $run_sh_tqdm --dnntts-data-batch-size $batch_size --dnntts-train-nepochs $nepochs \\\n",
    "    --cudnn-benchmark $cudnn_benchmark --cudnn-deterministic $cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-angle",
   "metadata": {},
   "source": [
    "### 손실 함수의 값 추이\n",
    "\n",
    "저자에 의한 실험 결과입니다. Tensorboard 로그는 https://tensorboard.dev/에 업로드되었습니다.\n",
    "로그 데이터를 `tensorboard` 패키지를 이용해 다운로드합니다.\n",
    "\n",
    "https://tensorboard.dev/experiment/ajmqiymoTx6rADKLF8d6sA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists(\"tensorboard/all_log.csv\"):\n",
    "    df = pd.read_csv(\"tensorboard/all_log.csv\")\n",
    "else:\n",
    "    experiment_id = \"ajmqiymoTx6rADKLF8d6sA\"\n",
    "    experiment = tb.data.experimental.ExperimentFromDev(experiment_id)\n",
    "    df = experiment.get_scalars(pivot=True) \n",
    "    df.to_csv(\"tensorboard/all_log.csv\", index=False)\n",
    "df[\"run\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_loss = df[df.run.str.contains(\"duration\")]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(duration_loss[\"step\"], duration_loss[\"Loss/train\"], label=\"Train\")\n",
    "ax.plot(duration_loss[\"step\"], duration_loss[\"Loss/dev\"], \"--\", label=\"Dev\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Epoch loss\")\n",
    "plt.legend()\n",
    "\n",
    "# 그림 6-8\n",
    "savefig(\"fig/dnntts_impl_duration_dnn_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-david",
   "metadata": {},
   "source": [
    "## 6.9 음향 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-jordan",
   "metadata": {},
   "source": [
    "### 음향 모델 설정 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat conf/train_dnntts/model/{acoustic_config_name}.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-clinic",
   "metadata": {},
   "source": [
    "### 음향 모델 인스턴스화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "hydra.utils.instantiate(OmegaConf.load(f\"conf/train_dnntts/model/{acoustic_config_name}.yaml\").netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-findings",
   "metadata": {},
   "source": [
    "### 레시피의 stage 5 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 5 --stop-stage 5 --acoustic-model $acoustic_config_name \\\n",
    "    --tqdm $run_sh_tqdm --dnntts-data-batch-size $batch_size --dnntts-train-nepochs $nepochs \\\n",
    "    --cudnn-benchmark $cudnn_benchmark --cudnn-deterministic $cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-mexico",
   "metadata": {},
   "source": [
    "### 손실 함수의 값 추이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_loss = df[df.run.str.contains(\"acoustic\")]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(acoustic_loss[\"step\"], acoustic_loss[\"Loss/train\"], label=\"Train\")\n",
    "ax.plot(acoustic_loss[\"step\"], acoustic_loss[\"Loss/dev\"], \"--\", label=\"Dev\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Epoch loss\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# 그림 6-9\n",
    "savefig(\"fig/dnntts_impl_acoustic_dnn_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-horror",
   "metadata": {},
   "source": [
    "## 6.10 학습된 모델을 사용하여 텍스트에서 음성 합성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-rating",
   "metadata": {},
   "source": [
    "### 학습된 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-gasoline",
   "metadata": {},
   "source": [
    "#### 연속 길이 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_config = OmegaConf.load(f\"exp/jsut_sr16000/{duration_config_name}/model.yaml\")\n",
    "duration_model = hydra.utils.instantiate(duration_config.netG)\n",
    "checkpoint = torch.load(f\"exp/jsut_sr16000/{duration_config_name}/latest.pth\", map_location=device)\n",
    "duration_model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "duration_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-sucking",
   "metadata": {},
   "source": [
    "#### 음향 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_config = OmegaConf.load(f\"exp/jsut_sr16000/{acoustic_config_name}/model.yaml\")\n",
    "acoustic_model = hydra.utils.instantiate(acoustic_config.netG)\n",
    "checkpoint = torch.load(f\"exp/jsut_sr16000/{acoustic_config_name}/latest.pth\", map_location=device)\n",
    "acoustic_model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "acoustic_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-angel",
   "metadata": {},
   "source": [
    "#### 통계량 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_in_scaler = joblib.load(\"./dump/jsut_sr16000/norm/in_duration_scaler.joblib\")\n",
    "duration_out_scaler = joblib.load(\"./dump/jsut_sr16000/norm/out_duration_scaler.joblib\")\n",
    "acoustic_in_scaler = joblib.load(\"./dump/jsut_sr16000/norm/in_acoustic_scaler.joblib\")\n",
    "acoustic_out_scaler = joblib.load(\"./dump/jsut_sr16000/norm/out_acoustic_scaler.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-prediction",
   "metadata": {},
   "source": [
    "### 음소 연속 길이 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_duration(\n",
    "    device,  # cpu or cuda\n",
    "    labels,  # 풀 컨텍스트 라벨\n",
    "    duration_model,  # 학습된 연속 길이 모델\n",
    "    duration_config,  # 연속 길이 모델 설정\n",
    "    duration_in_scaler,  # 언어 특징량 정규화용 StandardScaler\n",
    "    duration_out_scaler,  # 음소 연속 길이 정규화용 StandardScaler\n",
    "    binary_dict,  # 이진 특징량을 추출하는 정규식\n",
    "    numeric_dict,  # 수치 특징량을 추출하는 정규 표현\n",
    "):\n",
    "    # 언어 특징량 추출\n",
    "    in_feats = fe.linguistic_features(labels, binary_dict, numeric_dict).astype(np.float32)\n",
    "\n",
    "    # 언어 특징량 정규화\n",
    "    in_feats = duration_in_scaler.transform(in_feats)\n",
    "\n",
    "    # 지속 길이 예측\n",
    "    x = torch.from_numpy(in_feats).float().to(device).view(1, -1, in_feats.shape[-1])\n",
    "    pred_durations = duration_model(x, [x.shape[1]]).squeeze(0).cpu().data.numpy()\n",
    "\n",
    "    # 예측된 연속 길이에 대해 정규화를 역변환합니다.\n",
    "    pred_durations = duration_out_scaler.inverse_transform(pred_durations)\n",
    "\n",
    "    # 임계값 처리\n",
    "    pred_durations[pred_durations <= 0] = 1\n",
    "    pred_durations = np.round(pred_durations)\n",
    "\n",
    "    return pred_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-melissa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.util import lab2phonemes, find_lab, find_feats\n",
    "\n",
    "labels = hts.load(find_lab(\"downloads/jsut_ver1.1/\", test_utt))\n",
    "\n",
    "# 풀 컨텍스트 라벨에서 음소만 추출\n",
    "test_phonemes = lab2phonemes(labels)\n",
    "\n",
    "# 언어 특징량 추출에 사용하기 위한 질의 파일\n",
    "binary_dict, numeric_dict = hts.load_question_set(ttslearn.util.example_qst_file())\n",
    "\n",
    "# 음소 연속 길이 예측\n",
    "durations_test = predict_duration(\n",
    "    device, labels, duration_model, duration_config, duration_in_scaler, duration_out_scaler,\n",
    "    binary_dict, numeric_dict)\n",
    "durations_test_target = np.load(find_feats(\"dump/jsut_sr16000/org\", test_utt, typ=\"out_duration\"))\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "ax.plot(durations_test_target, \"-+\", label=\"Target\")\n",
    "ax.plot(durations_test, \"--*\", label=\"Predicted\")\n",
    "ax.set_xticks(np.arange(len(test_phonemes)))\n",
    "ax.set_xticklabels(test_phonemes)\n",
    "ax.set_xlabel(\"Phoneme\")\n",
    "ax.set_ylabel(\"Duration (the number of frames)\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 6-10\n",
    "savefig(\"fig/dnntts_impl_duration_comp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-profile",
   "metadata": {},
   "source": [
    "### 음향 특징량 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dnntts.multistream import get_windows, multi_stream_mlpg\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_acoustic(\n",
    "    device,  # CPU or GPU\n",
    "    labels,  # 풀 컨텍스트 라벨\n",
    "    acoustic_model,  # 학습된 음향 모델\n",
    "    acoustic_config,  # 음향 모델 설정\n",
    "    acoustic_in_scaler,  # 언어 특징량 정규화용 StandardScaler\n",
    "    acoustic_out_scaler,  # 음향 특징량의 정규화용 StandardScaler\n",
    "    binary_dict,  # 이진 특징량을 추출하는 정규식\n",
    "    numeric_dict,  # 수치 특징량을 추출하는 정규 표현\n",
    "    mlpg=True,  # MLPG 사용 여부\n",
    "):\n",
    "    # 프레임별 언어 특징량 추출\n",
    "    in_feats = fe.linguistic_features(\n",
    "        labels,\n",
    "        binary_dict,\n",
    "        numeric_dict,\n",
    "        add_frame_features=True,\n",
    "        subphone_features=\"coarse_coding\",\n",
    "    )\n",
    "    # 정규화\n",
    "    in_feats = acoustic_in_scaler.transform(in_feats)\n",
    "\n",
    "    # 음향 특징량 예측\n",
    "    x = torch.from_numpy(in_feats).float().to(device).view(1, -1, in_feats.shape[-1])\n",
    "    pred_acoustic = acoustic_model(x, [x.shape[1]]).squeeze(0).cpu().data.numpy()\n",
    "\n",
    "    # 예측된 음향 특징량에 대해 정규화의 역변환을 실시합니다.\n",
    "    pred_acoustic = acoustic_out_scaler.inverse_transform(pred_acoustic)\n",
    "\n",
    "    # 파라미터 생성 알고리즘(MLPG) 실행\n",
    "    if mlpg and np.any(acoustic_config.has_dynamic_features):\n",
    "        # (T, D_out) -> (T, static_dim)\n",
    "        pred_acoustic = multi_stream_mlpg(\n",
    "            pred_acoustic,\n",
    "            acoustic_out_scaler.var_,\n",
    "            get_windows(acoustic_config.num_windows),\n",
    "            acoustic_config.stream_sizes,\n",
    "            acoustic_config.has_dynamic_features,\n",
    "        )\n",
    "\n",
    "    return pred_acoustic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = hts.load(f\"./downloads/jsut_ver1.1/basic5000/lab/{test_utt}.lab\")\n",
    "\n",
    "# 음향 특징량 예측\n",
    "out_feats = predict_acoustic(\n",
    "    device, labels, acoustic_model, acoustic_config, acoustic_in_scaler,\n",
    "    acoustic_out_scaler, binary_dict, numeric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.util import trim_silence\n",
    "from ttslearn.dnntts.multistream import split_streams\n",
    "\n",
    "# 특징량은, 전처리로 서두와 말미에 비음성 구간이 잘려져 있기 때문에, 비교를 위해 여기에서도 같은 처리를 실시합니다\n",
    "out_feats = trim_silence(out_feats, labels)\n",
    "# 결합된 특징량 분리\n",
    "mgc_gen, lf0_gen, vuv_gen, bap_gen = split_streams(out_feats, [40, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비교를 위해 자연음성에서 추출한 음향 특징량을 읽기\n",
    "feats = np.load(f\"./dump/jsut_sr16000/org/eval/out_acoustic/{test_utt}-feats.npy\")\n",
    "# 특징량 분리\n",
    "mgc_ref, lf0_ref, vuv_ref, bap_ref = get_static_features(\n",
    "    feats, acoustic_config.num_windows, acoustic_config.stream_sizes, acoustic_config.has_dynamic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-dubai",
   "metadata": {},
   "source": [
    "#### 스펙트럼 포락(envelope)의 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음향 특징을 WORLD의 음성 파라미터로 변환\n",
    "\n",
    "# 멜 켑스트럼(MFCC)에서 스펙트럼 포락(envelope)으로의 변환\n",
    "sp_gen= pysptk.mc2sp(mgc_gen, alpha, fft_size)\n",
    "sp_ref= pysptk.mc2sp(mgc_ref, alpha, fft_size)\n",
    "\n",
    "mindb = min(np.log(sp_ref).min(), np.log(sp_gen).min())\n",
    "maxdb = max(np.log(sp_ref).max(), np.log(sp_gen).max())\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
    "mesh = librosa.display.specshow(np.log(sp_ref).T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"linear\", cmap=cmap, ax=ax[0])\n",
    "mesh.set_clim(mindb, maxdb)\n",
    "fig.colorbar(mesh, ax=ax[0], format=\"%+2.fdB\")\n",
    "mesh = librosa.display.specshow(np.log(sp_gen).T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"linear\", cmap=cmap, ax=ax[1])\n",
    "mesh.set_clim(mindb, maxdb)\n",
    "fig.colorbar(mesh, ax=ax[1], format=\"%+2.fdB\")\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Frequency [Hz]\")\n",
    "    \n",
    "ax[0].set_title(\"Spectral envelope of natural speech\")\n",
    "ax[1].set_title(\"Spectral envelope of generated speech\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 6-11\n",
    "savefig(\"./fig/dnntts_impl_spec_comp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-advisory",
   "metadata": {},
   "source": [
    "#### F0 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로그 기본 주파수에서 기본 주파수로 변환\n",
    "f0_ref = np.exp(lf0_ref)\n",
    "f0_ref[vuv_ref < 0.5] = 0\n",
    "f0_gen = np.exp(lf0_gen)\n",
    "f0_gen[vuv_gen < 0.5] = 0\n",
    "\n",
    "timeaxis = librosa.frames_to_time(np.arange(len(f0_ref)), sr=sr, hop_length=int(0.005 * sr))\n",
    "\n",
    "fix, ax = plt.subplots(1,1, figsize=(8,3))\n",
    "ax.plot(timeaxis, f0_ref, linewidth=2, label=\"F0 of natural speech\")\n",
    "ax.plot(timeaxis, f0_gen, \"--\", linewidth=2, label=\"F0 of generated speech\")\n",
    "\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Frequency [Hz]\")\n",
    "ax.set_xlim(timeaxis[0], timeaxis[-1])\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 6-12\n",
    "savefig(\"./fig/dnntts_impl_f0_comp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-component",
   "metadata": {},
   "source": [
    "#### 대역 비주기성 지표 시각화 (bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대역 비주기성 지표\n",
    "ap_ref = pyworld.decode_aperiodicity(bap_ref.astype(np.float64), sr, fft_size)\n",
    "ap_gen = pyworld.decode_aperiodicity(bap_gen.astype(np.float64), sr, fft_size)\n",
    "\n",
    "mindb = min(np.log(ap_ref).min(), np.log(ap_gen).min())\n",
    "maxdb = max(np.log(ap_ref).max(), np.log(ap_gen).max())\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
    "mesh = librosa.display.specshow(np.log(ap_ref).T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"linear\", cmap=cmap, ax=ax[0])\n",
    "mesh.set_clim(mindb, maxdb)\n",
    "fig.colorbar(mesh, ax=ax[0])\n",
    "mesh = librosa.display.specshow(np.log(ap_gen).T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"linear\", cmap=cmap, ax=ax[1])\n",
    "mesh.set_clim(mindb, maxdb)\n",
    "fig.colorbar(mesh, ax=ax[1])\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Frequency [Hz]\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-panel",
   "metadata": {},
   "source": [
    "### 음성 파형 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnmnkwii.postfilters import merlin_post_filter\n",
    "from ttslearn.dnntts.multistream import get_static_stream_sizes\n",
    "\n",
    "def gen_waveform(\n",
    "    sample_rate,  # 샘플링 주파수\n",
    "    acoustic_features,  # 음향 특징량\n",
    "    stream_sizes,  # 스트림 크기\n",
    "    has_dynamic_features,  # 음향 특징량이 동적 특징량을 포함하는지 여부\n",
    "    num_windows=3,  # 동적 특징량을 계산하는 데 사용되는 창 개수\n",
    "    post_filter=False,  # 포먼트 강조 포스트 필터를 사용할지 여부\n",
    "):\n",
    "    # 정적 특징량의 차원수를 취득\n",
    "    if np.any(has_dynamic_features):\n",
    "        static_stream_sizes = get_static_stream_sizes(\n",
    "            stream_sizes, has_dynamic_features, num_windows\n",
    "        )\n",
    "    else:\n",
    "        static_stream_sizes = stream_sizes\n",
    "\n",
    "    # 결합된 음향 특징량을 스트림별로 분리\n",
    "    mgc, lf0, vuv, bap = split_streams(acoustic_features, static_stream_sizes)\n",
    "\n",
    "    fftlen = pyworld.get_cheaptrick_fft_size(sample_rate)\n",
    "    alpha = pysptk.util.mcepalpha(sample_rate)\n",
    "\n",
    "    # 포먼트 강조 포스트 필터\n",
    "    if post_filter:\n",
    "        mgc = merlin_post_filter(mgc, alpha)\n",
    "\n",
    "    # 음향 특징량을 음성 파라미터로 변환\n",
    "    spectrogram = pysptk.mc2sp(mgc, fftlen=fftlen, alpha=alpha)\n",
    "    aperiodicity = pyworld.decode_aperiodicity(\n",
    "        bap.astype(np.float64), sample_rate, fftlen\n",
    "    )\n",
    "    f0 = lf0.copy()\n",
    "    f0[vuv < 0.5] = 0\n",
    "    f0[np.nonzero(f0)] = np.exp(f0[np.nonzero(f0)])\n",
    "\n",
    "    # WORLD 보코더를 이용한 음성 생성\n",
    "    gen_wav = pyworld.synthesize(\n",
    "        f0.flatten().astype(np.float64),\n",
    "        spectrogram.astype(np.float64),\n",
    "        aperiodicity.astype(np.float64),\n",
    "        sample_rate,\n",
    "    )\n",
    "\n",
    "    return gen_wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-pressing",
   "metadata": {},
   "source": [
    "### 모든 모델을 결합하여 음성 파형 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = hts.load(f\"./downloads/jsut_ver1.1/basic5000/lab/{test_utt}.lab\")\n",
    "\n",
    "binary_dict, numeric_dict = hts.load_question_set(ttslearn.util.example_qst_file())\n",
    "\n",
    "# 음소 연속 길이 예측\n",
    "durations = predict_duration(\n",
    "    device, labels, duration_model, duration_config, duration_in_scaler, duration_out_scaler,\n",
    "    binary_dict, numeric_dict)\n",
    "\n",
    "# 예측된 연속장을 풀 컨텍스트 레이블로 설정\n",
    "labels.set_durations(durations)\n",
    "\n",
    "# 음향 특징량 예측\n",
    "out_feats = predict_acoustic(\n",
    "    device, labels, acoustic_model, acoustic_config, acoustic_in_scaler,\n",
    "    acoustic_out_scaler, binary_dict, numeric_dict)\n",
    "\n",
    "# 음성 파형 생성\n",
    "gen_wav = gen_waveform(\n",
    "    sr, out_feats,\n",
    "    acoustic_config.stream_sizes,\n",
    "    acoustic_config.has_dynamic_features,\n",
    "    acoustic_config.num_windows,\n",
    "    post_filter=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비교를 위해 원래 음성 로드\n",
    "_sr, ref_wav = wavfile.read(f\"./downloads/jsut_ver1.1/basic5000/wav/{test_utt}.wav\")\n",
    "ref_wav = (ref_wav / 32768.0).astype(np.float64)\n",
    "ref_wav = librosa.resample(ref_wav, _sr, sr)\n",
    "\n",
    "# 스펙트로그램 계산\n",
    "spec_ref = librosa.stft(ref_wav, n_fft=fft_size, hop_length=hop_length, window=\"hann\")\n",
    "logspec_ref = np.log(np.abs(spec_ref))\n",
    "spec_gen = librosa.stft(gen_wav, n_fft=fft_size, hop_length=hop_length, window=\"hann\")\n",
    "logspec_gen = np.log(np.abs(spec_gen))\n",
    "\n",
    "mindb = min(logspec_ref.min(), logspec_gen.min())\n",
    "maxdb = max(logspec_ref.max(), logspec_gen.max())\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
    "mesh = librosa.display.specshow(logspec_ref, hop_length=hop_length, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax[0])\n",
    "mesh.set_clim(mindb, maxdb)\n",
    "fig.colorbar(mesh, ax=ax[0], format=\"%+2.fdB\")\n",
    "\n",
    "mesh = librosa.display.specshow(logspec_gen, hop_length=hop_length, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax[1])\n",
    "mesh.set_clim(mindb, maxdb)\n",
    "fig.colorbar(mesh, ax=ax[1], format=\"%+2.fdB\")\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Frequency [Hz]\")\n",
    "    \n",
    "ax[0].set_title(\"Spectrogram of natural speech\")\n",
    "ax[1].set_title(\"Spectrogram of generated speech\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"자연 음성\")\n",
    "IPython.display.display(Audio(ref_wav, rate=sr))\n",
    "print(\"DNN 음성 합성\")\n",
    "IPython.display.display(Audio(gen_wav, rate=sr))\n",
    "\n",
    "# 그림 6-13\n",
    "savefig(\"./fig/dnntts_impl_tts_spec_comp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-minute",
   "metadata": {},
   "source": [
    "### 평가 데이터에 대한 음성 파형 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-lotus",
   "metadata": {},
   "source": [
    "#### 레시피의 stage 5 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 6 --stop-stage 6 --duration-model $duration_config_name --acoustic-model $acoustic_config_name \\\n",
    "    --tqdm $run_sh_tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-basin",
   "metadata": {},
   "source": [
    "## 자연 음성과 합성 음성의 비교 (bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from ttslearn.util import load_utt_list\n",
    "\n",
    "with open(\"./downloads/jsut_ver1.1/basic5000/transcript_utf8.txt\") as f:\n",
    "    transcripts = {}\n",
    "    for l in f:\n",
    "        utt_id, script = l.split(\":\")\n",
    "        transcripts[utt_id] = script\n",
    "        \n",
    "eval_list = load_utt_list(\"data/eval.list\")[::-1][:5]\n",
    "\n",
    "for utt_id in eval_list:\n",
    "    # ref file \n",
    "    ref_file = f\"./downloads/jsut_ver1.1/basic5000/wav/{utt_id}.wav\"\n",
    "    _sr, ref_wav = wavfile.read(ref_file)\n",
    "    ref_wav = (ref_wav / 32768.0).astype(np.float64)\n",
    "    ref_wav = librosa.resample(ref_wav, _sr, sr)\n",
    "    \n",
    "    gen_file = f\"exp/jsut_sr16000/synthesis_{duration_config_name}_{acoustic_config_name}/eval/{utt_id}.wav\"\n",
    "    _sr, gen_wav = wavfile.read(gen_file)\n",
    "    print(f\"{utt_id}: {transcripts[utt_id]}\")\n",
    "    print(\"자연 음성\")\n",
    "    IPython.display.display(Audio(ref_wav, rate=sr))\n",
    "    print(\"DNN 음성 합성\")\n",
    "    IPython.display.display(Audio(gen_wav, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-senator",
   "metadata": {},
   "source": [
    "풀 컨텍스트 라벨이 아니고, 한자가 섞인 문장을 입력으로 한 TTS 의 구현은, `ttslearn.dnntts.tts` 모듈을 참조해 주세요. 이 장의 시작 부분에 제시된 학습된 모델을 사용하는 TTS는 해당 모듈을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-shark",
   "metadata": {},
   "source": [
    "## 학습 완료 모델의 패키징 (bonus)\n",
    "\n",
    "학습된 모델을 이용한 TTS에 필요한 파일을 모두 단일 디렉토리로 정리합니다.\n",
    "'tslearn.dntts.DNTTS` 클래스에는, 정리한 디렉토리를 지정해, TTS를 실시하는 기능이 구현되고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-examination",
   "metadata": {},
   "source": [
    "### 레시피 stage 99 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 99 --stop-stage 99 --duration-model $duration_config_name --acoustic-model $acoustic_config_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls tts_models/jsut_sr16000_{duration_config_name}_{acoustic_config_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-subject",
   "metadata": {},
   "source": [
    "### 패키징된 모델을 이용한 TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dnntts import DNNTTS\n",
    "\n",
    "# 패키징된 모델의 경로를 지정합니다.\n",
    "model_dir = f\"./tts_models/jsut_sr16000_{duration_config_name}_{acoustic_config_name}\"\n",
    "engine = DNNTTS(model_dir)\n",
    "wav, sr = engine.tts(\"ここまでお読みいただき、ありがとうございました。\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,2))\n",
    "librosa.display.waveshow(wav.astype(np.float32), sr, ax=ax)\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()\n",
    "\n",
    "Audio(wav, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_colab():\n",
    "    from datetime import timedelta\n",
    "    elapsed = (time.time() - start_time)\n",
    "    print(\"소요시간:\", str(timedelta(seconds=elapsed)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}