{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "recovered-superintendent",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 제8장 한국어 WaveNet 음성 합성 시스템의 구현\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/r9y9/ttslearn/blob/master/notebooks/ch08_Recipe-WaveNet.ipynb)\n",
    "\n",
    "Google colab에서 실행하는 예상 소요 시간: 5시간\n",
    "\n",
    "이 노트북의 레시피 설정은 Google Colab에서 실행할 때 시간 초과를 피하기 위해 학습 조건을 책에 나열된 설정에서 일부 수정했음을 기억하십시오 (배치 크기 줄이기 등).\n",
    "참고로 책에 기재된 조건으로 저자(야마모토)가 레시피를 실행한 결과를 아래에서 공개하고 있습니다.\n",
    "\n",
    "- Tensorboard logs: https://tensorboard.dev/experiment/yXyg9qgfQRSGxvil5FA4xw/\n",
    "- exp 디렉토리 (학습 모델, 합성 음성 포함) : https://drive.google.com/file/d/1Z09yCCAKyKOUU3Zsdxs0mZ1Q-TCPqFHA/view?usp=sharing (135.2 MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-charter",
   "metadata": {},
   "source": [
    "## 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-terminology",
   "metadata": {},
   "source": [
    "### Google Colab을 사용하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-daisy",
   "metadata": {},
   "source": [
    "Google Colab에서 이 노트북을 실행하려면 메뉴의 '런타임 -> 런타임 시간 변경'에서 '하드웨어 가속기'를 **GPU**로 변경하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-teach",
   "metadata": {},
   "source": [
    "### Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -VV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-friday",
   "metadata": {},
   "source": [
    "### ttslearn 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-shannon",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import ttslearn\n",
    "except ImportError:\n",
    "    !pip install ttslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttslearn\n",
    "ttslearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-marker",
   "metadata": {},
   "source": [
    "## 8.1 이 장의 일본어 음성 합성 시스템 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-marker",
   "metadata": {},
   "source": [
    "### 학습된 모델을 이용한 음성 합성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.wavenet import WaveNetTTS\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import Audio\n",
    "\n",
    "engine = WaveNetTTS()\n",
    "wav, sr = engine.tts(\"ウェーブネットにチャレンジしましょう！\", tqdm=tqdm)\n",
    "Audio(wav, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,2))\n",
    "librosa.display.waveshow(wav.astype(np.float32), sr, ax=ax)\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-northern",
   "metadata": {},
   "source": [
    "### レシピ実行の前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from ttslearn.env import is_colab\n",
    "from os.path import exists\n",
    "\n",
    "# pip install ttslearn은 레시피를 설치하지 않으므로 수동으로 다운로드\n",
    "if is_colab() and not exists(\"recipes.zip\"):\n",
    "    !curl -LO https://github.com/r9y9/ttslearn/releases/download/v{ttslearn.__version__}/recipes.zip\n",
    "    !unzip -o recipes.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# recipe 디렉토리로 이동\n",
    "cwd = os.getcwd()\n",
    "if cwd.endswith(\"notebooks\"):\n",
    "    os.chdir(\"../recipes/wavenet/\")\n",
    "elif is_colab():\n",
    "    os.chdir(\"recipes/wavenet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-tongue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-radical",
   "metadata": {},
   "source": [
    "### 패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "import tensorboard as tb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치 연산\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "# 음성 파형 불러오기\n",
    "from scipy.io import wavfile\n",
    "# 풀 컨텍스트 라벨, 질문 파일 로드\n",
    "from nnmnkwii.io import hts\n",
    "# 음성 분석\n",
    "import pyworld\n",
    "# 음성 분석, 시각화\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "# 파이썬에서 배우는 음성 합성\n",
    "import ttslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정\n",
    "from ttslearn.util import init_seed\n",
    "init_seed(773)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-merchandise",
   "metadata": {},
   "source": [
    "### 그래프 그리기 설정 (描画周りの設定) // 번역 수정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.notebook import get_cmap, init_plot_style, savefig\n",
    "cmap = get_cmap()\n",
    "init_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-split",
   "metadata": {},
   "source": [
    "### 레시피 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.sh를 사용하여 학습 스크립트를 노트북에서 실행하려면 True\n",
    "# google colab의 경우 True라고 가정합니다.\n",
    "# 로컬 환경의 경우 run.sh를 터미널에서 실행하는 것이 좋습니다.\n",
    "# 이 경우이 노트북은 시각화 및 학습 된 모델을 테스트하는 데 사용됩니다.\n",
    "run_sh = is_colab()\n",
    "\n",
    "# 참고 : WaveNet을 사용한 평가 데이터에 대한 음성 생성은 시간이 오래 걸립니다.\n",
    "run_stage8 = True\n",
    "\n",
    "# run.sh를 통해 실행되는 스크립트의 tqdm\n",
    "run_sh_tqdm = \"none\"\n",
    "\n",
    "# CUDA\n",
    "# 참고 : run.sh의 인수로 전달하므로 bool이 아닌 문자열로 정의됩니다.\n",
    "cudnn_benchmark = \"true\"\n",
    "cudnn_deterministic = \"false\"\n",
    "\n",
    "# 특징량 추출시 병렬 처리 작업 수\n",
    "n_jobs = os.cpu_count()//2\n",
    "\n",
    "# 연속 길이 모델의 설정 파일 이름\n",
    "duration_config_name=\"duration_rnn\"\n",
    "# 음향 모델 설정 파일 이름\n",
    "logf0_config_name=\"logf0_rnn\"\n",
    "# WaveNet 설정 파일 이름\n",
    "wavenet_config_name=\"wavenet_sr16k_mulaw256\"\n",
    "\n",
    "# 연속 길이 모델 및 로그 F0 예측 모델 학습의 배치 크기\n",
    "dnntts_batch_size = 32\n",
    "# 연속 길이 모델 및 로그 F0 예측 모델 학습의 에포크 수\n",
    "# 주의: 계산 시간을 줄이기 위해 적게 설정합니다. 품질을 높이려면 30-50 에포크 수를 사용해보십시오.\n",
    "dnntts_nepochs = 5\n",
    "\n",
    "# WaveNet 학습의 배치 크기\n",
    "# 권장 배치 크기 : 8 이상\n",
    "# 동작 확인을 위해 작은 값으로 설정합니다.\n",
    "wavenet_batch_size = 4\n",
    "# WavaNet 학습 반복 수\n",
    "# 주의: 충분한 품질을 얻기 위해 필요한 값: 300k ~ 500k steps\n",
    "wavenet_max_train_steps = 50000\n",
    "\n",
    "# 음성을 생성하는 발화 수\n",
    "# WaveNet의 추론은 시간이 오래 걸리므로 노트북에서 볼 수 있는 5개만 생성\n",
    "num_eval_utts = 5\n",
    "\n",
    "# 노트북에서 사용하는 테스트 발화 (학습 데이터, 평가 데이터)\n",
    "train_utt = \"BASIC5000_0001\"\n",
    "test_utt = \"BASIC5000_5000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-release",
   "metadata": {},
   "source": [
    "### Tensorboard로 로그 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노트북에서 tensorboard 로그를 확인하려면 다음 행을 활성화하십시오.\n",
    "if is_colab():\n",
    "    %tensorboard --logdir tensorboard/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-hartford",
   "metadata": {},
   "source": [
    "## 프로그램 구현 전 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-rehabilitation",
   "metadata": {},
   "source": [
    "### stage -1: 코퍼스 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_colab():\n",
    "    ! ./run.sh --stage -1 --stop-stage -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-particular",
   "metadata": {},
   "source": [
    "### Stage 0: 학습/검증/평가 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 0 --stop-stage 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head data/dev.list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-ideal",
   "metadata": {},
   "source": [
    "## 8.2 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-express",
   "metadata": {},
   "source": [
    "### 연속 길이 모델에 대한 전처리\n",
    "\n",
    "배치 처리를 실시하는 커멘드 라인 프로그램은,`recipes/dnntts/preprocess_duration.py`를 참조해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 1 --stop-stage 1 --n-jobs $n_jobs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-transmission",
   "metadata": {},
   "source": [
    "### 로그 F0 예측 모델에 대한 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-distributor",
   "metadata": {},
   "source": [
    "#### 로그 F0 + 유성/무성 플래그 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnmnkwii.preprocessing import delta_features\n",
    "from nnmnkwii.preprocessing.f0 import interp1d\n",
    "from ttslearn.dsp import f0_to_lf0\n",
    "\n",
    "def world_log_f0_vuv(x, sr):\n",
    "    f0, timeaxis = pyworld.dio(x, sr)\n",
    "    f0 = pyworld.stonemask(x, f0, timeaxis, sr)\n",
    "    vuv = (f0 > 0).astype(np.float32)\n",
    "\n",
    "    # 연속 로그 기본 주파수\n",
    "    lf0 = f0_to_lf0(f0)\n",
    "    lf0 = interp1d(lf0)\n",
    "\n",
    "    # 연속 기본 주파수와 유성/무성 플래그를 2차원 행렬의 형태로 한다.\n",
    "    lf0 = lf0[:, np.newaxis] if len(lf0.shape) == 1 else lf0\n",
    "    vuv = vuv[:, np.newaxis] if len(vuv.shape) == 1 else vuv\n",
    "\n",
    "    # 동적 특징량 계산\n",
    "    windows = [\n",
    "        [1.0],  # 정적 특징량에 대한 창\n",
    "        [-0.5, 0.0, 0.5],  # 1차 동적 특징량에 대한 창\n",
    "        [1.0, -2.0, 1.0],  # 2차 동적 특징량에 대한 창\n",
    "    ]\n",
    "    lf0 = delta_features(lf0, windows)\n",
    "\n",
    "    # 모든 특징을 결합\n",
    "    feats = np.hstack([lf0, vuv]).astype(np.float32)\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-affect",
   "metadata": {},
   "source": [
    "#### 로그 F0 + 유성/무성 플래그의 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dsp import lf0_to_f0\n",
    "\n",
    "sr = 16000\n",
    "_sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "x = (x / 32768).astype(np.float64)\n",
    "x = librosa.resample(x, _sr, sr)\n",
    "\n",
    "out_feats = world_log_f0_vuv(x, sr)\n",
    "lf0 = out_feats[:, 0]\n",
    "vuv = out_feats[:, -1]\n",
    "\n",
    "timeaxis = librosa.frames_to_time(np.arange(len(lf0)), sr, int(0.005 * sr))\n",
    "\n",
    "fig, ax = plt.subplots(3,1, figsize=(8,6))\n",
    "ax[0].set_title(\"Input waveform\")\n",
    "ax[1].set_title(\"Continuous log F0\")\n",
    "ax[2].set_title(\"V/UV\")\n",
    "\n",
    "librosa.display.waveshow(x, sr, x_axis=\"time\", ax=ax[0])\n",
    "ax[1].plot(timeaxis, lf0, linewidth=2)\n",
    "ax[2].plot(timeaxis, vuv, linewidth=2)\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_xlim(0, len(x)/sr)\n",
    "    a.set_xticks(np.arange(0, 3.5, 0.5))\n",
    "    a.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "\n",
    "ax[0].set_ylabel(\"Amplitude\")\n",
    "ax[1].set_ylabel(\"Logarithmic frequency\")\n",
    "ax[2].set_ylabel(\"Binary value\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-worse",
   "metadata": {},
   "source": [
    "#### 1 발화에 대한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnmnkwii.frontend import merlin as fe\n",
    "\n",
    "# HTS 형식의 질문 파일 로드\n",
    "binary_dict, numeric_dict = hts.load_question_set(ttslearn.util.example_qst_file())\n",
    "\n",
    "# 전체 컨텍스트 라벨 로드\n",
    "labels = hts.load(ttslearn.util.example_label_file())\n",
    "\n",
    "# 프레임별 언어 특징량 추출\n",
    "in_feats = fe.linguistic_features(\n",
    "    labels,\n",
    "    binary_dict,\n",
    "    numeric_dict,\n",
    "    add_frame_features=True,\n",
    "    subphone_features=\"coarse_coding\",\n",
    ")\n",
    "\n",
    "# 오디오 파일 로드\n",
    "sr = 16000\n",
    "_sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "x = (x / 32768).astype(np.float64)\n",
    "x = librosa.resample(x, _sr, sr)\n",
    "\n",
    "# 연속 로그 기본 주파수와 유성/무성 플래그를 결합한 특징량 계산\n",
    "out_feats = world_log_f0_vuv(x.astype(np.float64), sr)\n",
    "\n",
    "# 프레임 수 조정\n",
    "minL = min(in_feats.shape[0], out_feats.shape[0])\n",
    "in_feats, out_feats = in_feats[:minL], out_feats[:minL]\n",
    "\n",
    "# 시작과 끝의 음성이 아닌 구간의 길이 조정\n",
    "assert \"sil\" in labels.contexts[0] and \"sil\" in labels.contexts[-1]\n",
    "start_frame = int(labels.start_times[1] / 50000)\n",
    "end_frame = int(labels.end_times[-2] / 50000)\n",
    "\n",
    "# 시작과 끝의 음성이 아닌 구간의 길이 조정\n",
    "start_frame = max(0, start_frame - int(0.050 / 0.005))\n",
    "end_frame = min(minL, end_frame + int(0.100 / 0.005))\n",
    "\n",
    "in_feats = in_feats[start_frame:end_frame]\n",
    "out_feats = out_feats[start_frame:end_frame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"입력 특징량의 크기:\", in_feats.shape)\n",
    "print (\"출력 특징량의 크기:\", out_feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-midnight",
   "metadata": {},
   "source": [
    "#### 레시피의 stage 2 실행\n",
    "\n",
    "위의 처리를 수행하는 일괄 처리 프로그램은 `preprocess_logf0.py`에 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 2 --stop-stage 2 --n-jobs $n_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-attack",
   "metadata": {},
   "source": [
    "### WaveNet을 위한 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-spectacular",
   "metadata": {},
   "source": [
    "#### 1 발화에 대한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dsp import mulaw_quantize\n",
    "from ttslearn.dsp import world_log_f0_vuv\n",
    "\n",
    "# HTS 형식의 질문 파일 로드\n",
    "binary_dict, numeric_dict = hts.load_question_set(ttslearn.util.example_qst_file())\n",
    "\n",
    "# 풀 컨텍스트 라벨 로드\n",
    "labels = hts.load(ttslearn.util.example_label_file())\n",
    "\n",
    "# 프레임별 언어 특징량 추출\n",
    "in_feats = fe.linguistic_features(\n",
    "    labels,\n",
    "    binary_dict,\n",
    "    numeric_dict,\n",
    "    add_frame_features=True,\n",
    "    subphone_features=\"coarse_coding\",\n",
    ")\n",
    "\n",
    "# 오디오 파일 로드\n",
    "sr = 16000\n",
    "_sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "x = (x / 32768).astype(np.float64)\n",
    "x = librosa.resample(x, _sr, sr)\n",
    "\n",
    "# 연속 로그 기본 주파수와 유성/무성 플래그를 결합한 특징량 계산\n",
    "log_f0_vuv = world_log_f0_vuv(x.astype(np.float64), sr)\n",
    "\n",
    "# 프레임 수 조정\n",
    "minL = min(in_feats.shape[0], log_f0_vuv.shape[0])\n",
    "in_feats, log_f0_vuv = in_feats[:minL], log_f0_vuv[:minL]\n",
    "\n",
    "# 시작과 끝의 음성이 아닌 구간의 길이 조정\n",
    "assert \"sil\" in labels.contexts[0] and \"sil\" in labels.contexts[-1]\n",
    "start_frame = int(labels.start_times[1] / 50000)\n",
    "end_frame = int(labels.end_times[-2] / 50000)\n",
    "\n",
    "# 시작: 50ms, 후미: 100ms\n",
    "start_frame = max(0, start_frame - int(0.050 / 0.005))\n",
    "end_frame = min(minL, end_frame + int(0.100 / 0.005))\n",
    "\n",
    "in_feats = in_feats[start_frame:end_frame]\n",
    "log_f0_vuv = log_f0_vuv[start_frame:end_frame]\n",
    "\n",
    "# 언어 특징량과 연속 로그 기본 주파수를 결합\n",
    "in_feats = np.hstack([in_feats, log_f0_vuv])\n",
    "\n",
    "# 시간 영역에서 오디오의 길이 조정\n",
    "x = x[int(start_frame * 0.005 * sr) :]\n",
    "length = int(sr * 0.005) * in_feats.shape[0]\n",
    "x = pad_1d(x, length) if len(x) < length else x[:length]\n",
    "\n",
    "# mu-law 양자화\n",
    "quantized_x = mulaw_quantize(x)\n",
    "\n",
    "# 조건부 특징 량의 업 샘플링을 생각하기 위해,\n",
    "# 음성 파형의 길이가 프레임 시프트로 나누어지는 것을 확인\n",
    "assert len(quantized_x) % int(sr * 0.005) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"조건부 특징량의 크기:\", in_feats.shape)\n",
    "print (\"양자화된 음성 파형의 크기:\", quantized_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeaxis = np.arange(len(x)) / sr\n",
    "\n",
    "fig, ax = subplots(2,1, figsize=(8,4))\n",
    "ax[0].set_title(\"Input waveform\")\n",
    "ax[1].set_title(\"Output waveform after mu-law\")\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Amplitude\")\n",
    "ax[0].plot(timeaxis, x)\n",
    "ax[1].plot(timeaxis, quantized_x)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-fifteen",
   "metadata": {},
   "source": [
    "#### 레시피의 stage 3 실행\n",
    "\n",
    "위의 처리를 수행하는 일괄 처리 프로그램은 `preprocess_wavenet.py`에 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 3 --stop-stage 3 --n-jobs $n_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-uniform",
   "metadata": {},
   "source": [
    "### 특징량 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-johnson",
   "metadata": {},
   "source": [
    "정규화를 위한 통계량을 계산하는 명령행 프로그램은 `recipes/common/fit_scaler.py`를 참조하십시오. 또, 정규화를 실시하는 커멘드 라인 프로그램은, `recipes/common/preprocess_normalize.py` 를 참조해 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-discretion",
   "metadata": {},
   "source": [
    "#### 레시피 stage 4 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 4 --stop-stage 4 --n-jobs $n_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-college",
   "metadata": {},
   "source": [
    "#### 정규화 처리 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언어 특징량 정규화 전후\n",
    "in_feats = np.load(f\"dump/jsut_sr16000/org/train/in_logf0/{train_utt}-feats.npy\")\n",
    "in_feats_norm = np.load(f\"dump/jsut_sr16000/norm/train/in_logf0/{train_utt}-feats.npy\")\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
    "ax[0].set_title(\"Linguistic features (before normalization)\")\n",
    "ax[1].set_title(\"Linguistic features (after normalization)\")\n",
    "hop_length = int(sr * 0.005)\n",
    "mesh = librosa.display.specshow(\n",
    "    in_feats.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\", ax=ax[0], cmap=cmap)\n",
    "fig.colorbar(mesh, ax=ax[0])\n",
    "mesh = librosa.display.specshow(\n",
    "    in_feats_norm.T, sr=sr, hop_length=hop_length, x_axis=\"time\", y_axis=\"frames\",ax=ax[1], cmap=cmap)\n",
    "# 참고 : 실제로는 [-4, 4] 범위를 벗어난 값이 있지만 가시성을 위해 [-4, 4]로 설정합니다.\n",
    "mesh.set_clim(-4, 4)\n",
    "fig.colorbar(mesh, ax=ax[1])\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Context\")\n",
    "    # 후미 비음성 구간 제외\n",
    "    a.set_xlim(0, 2.55)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-beverage",
   "metadata": {},
   "source": [
    "## 8.3 연속 길이 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-force",
   "metadata": {},
   "source": [
    "### 연속 길이 모델의 설정 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat conf/train_dnntts/model/{duration_config_name}.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-portuguese",
   "metadata": {},
   "source": [
    "### 연속 길이 모델 인스턴스화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "hydra.utils.instantiate(OmegaConf.load(f\"conf/train_dnntts/model/{duration_config_name}.yaml\").netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-boxing",
   "metadata": {},
   "source": [
    "### 레시피의 stage 5 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 5 --stop-stage 5 --duration-model $duration_config_name \\\n",
    "        --tqdm $run_sh_tqdm --dnntts-data-batch-size $dnntts_batch_size --dnntts-train-nepochs $dnntts_nepochs \\\n",
    "        --cudnn-benchmark $cudnn_benchmark --cudnn-deterministic $cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-plaintiff",
   "metadata": {},
   "source": [
    "### 손실 함수의 값 추이\n",
    "\n",
    "저자에 의한 실험 결과입니다. Tensorboard 로그는 https://tensorboard.dev/에 업로드되었습니다.\n",
    "로그 데이터를 `tensorboard` 패키지를 이용해 다운로드합니다.\n",
    "\n",
    "https://tensorboard.dev/experiment/yXyg9qgfQRSGxvil5FA4xw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists(\"tensorboard/all_log.csv\"):\n",
    "    df = pd.read_csv(\"tensorboard/all_log.csv\")\n",
    "else:\n",
    "    experiment_id = \"yXyg9qgfQRSGxvil5FA4xw\"\n",
    "    experiment = tb.data.experimental.ExperimentFromDev(experiment_id)\n",
    "    df = experiment.get_scalars() \n",
    "    df.to_csv(\"tensorboard/all_log.csv\", index=False)\n",
    "df[\"run\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_loss = df[df.run.str.contains(\"duration\")]\n",
    "\n",
    "duration_train_loss = duration_loss[duration_loss.tag.str.contains(\"Loss/train\")]\n",
    "duration_dev_loss = duration_loss[duration_loss.tag.str.contains(\"Loss/dev\")]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(duration_train_loss[\"step\"], duration_train_loss[\"value\"], label=\"Train\")\n",
    "ax.plot(duration_dev_loss[\"step\"], duration_dev_loss[\"value\"], \"--\", label=\"Dev\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Epoch loss\")\n",
    "plt.legend()\n",
    "\n",
    "# 그림 8-3\n",
    "savefig(\"fig/wavenet_impl_duration_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-consultation",
   "metadata": {},
   "source": [
    "## 8.4 로그 F0 예측 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-firmware",
   "metadata": {},
   "source": [
    "### 로그 F0 예측 모델 설정 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat conf/train_dnntts/model/{logf0_config_name}.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-electric",
   "metadata": {},
   "source": [
    "### 로그 F0 예측 모델 인스턴스화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "hydra.utils.instantiate(OmegaConf.load(f\"conf/train_dnntts/model/{logf0_config_name}.yaml\").netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-present",
   "metadata": {},
   "source": [
    "### 레시피 stage 6 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-reporter",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 6 --stop-stage 6 --logf0-model $logf0_config_name \\\n",
    "        --tqdm $run_sh_tqdm --dnntts-data-batch-size $dnntts_batch_size --dnntts-train-nepochs $dnntts_nepochs \\\n",
    "        --cudnn-benchmark $cudnn_benchmark --cudnn-deterministic $cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-biotechnology",
   "metadata": {},
   "source": [
    "### 손실 함수의 값 추이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "logf0_loss = df[df.run.str.contains(\"logf0\")]\n",
    "\n",
    "logf0_train_loss = logf0_loss[logf0_loss.tag.str.contains(\"Loss/train\")]\n",
    "logf0_dev_loss = logf0_loss[logf0_loss.tag.str.contains(\"Loss/dev\")]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(logf0_train_loss[\"step\"], logf0_train_loss[\"value\"], label=\"Train\")\n",
    "ax.plot(logf0_dev_loss[\"step\"], logf0_dev_loss[\"value\"], \"--\", label=\"Dev\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Epoch loss\")\n",
    "plt.legend()\n",
    "\n",
    "# 그림 8-4\n",
    "savefig(\"fig/wavenet_impl_logf0_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-mainstream",
   "metadata": {},
   "source": [
    "## 8.5 WaveNet 학습 스크립트 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-small",
   "metadata": {},
   "source": [
    "### DataLoader 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-reducing",
   "metadata": {},
   "source": [
    "#### collate_fn 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_wavenet(batch, max_time_frames=100, hop_size=80, aux_context_window=2):\n",
    "    max_time_steps = max_time_frames * hop_size\n",
    "\n",
    "    xs, cs = [b[1] for b in batch], [b[0] for b in batch]\n",
    "\n",
    "    # 조건부 특징 량의 시작 위치를 무작위로 추출한 후 해당하는 짧은 음성 파형을 잘라냅니다.\n",
    "    c_lengths = [len(c) for c in cs]\n",
    "    start_frames = np.array(\n",
    "        [\n",
    "            np.random.randint(\n",
    "                aux_context_window, cl - aux_context_window - max_time_frames\n",
    "            )\n",
    "            for cl in c_lengths\n",
    "        ]\n",
    "    )\n",
    "    x_starts = start_frames * hop_size\n",
    "    x_ends = x_starts + max_time_steps\n",
    "    c_starts = start_frames - aux_context_window\n",
    "    c_ends = start_frames + max_time_frames + aux_context_window\n",
    "    x_batch = [x[s:e] for x, s, e in zip(xs, x_starts, x_ends)]\n",
    "    c_batch = [c[s:e] for c, s, e in zip(cs, c_starts, c_ends)]\n",
    "\n",
    "    # numpy.ndarray 의 리스트형으로부터 torch.Tensor 형에 변환합니다\n",
    "    x_batch = torch.tensor(x_batch, dtype=torch.long)  # (B, T)\n",
    "    c_batch = torch.tensor(c_batch, dtype=torch.float).transpose(2, 1)  # (B, C, T')\n",
    "\n",
    "    return x_batch, c_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-princeton",
   "metadata": {},
   "source": [
    "#### DataLoader 사용 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-crystal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from ttslearn.train_util import Dataset\n",
    "from functools import partial\n",
    "\n",
    "in_paths = sorted(Path(\"./dump/jsut_sr16000/norm/dev/in_wavenet/\").glob(\"*.npy\"))\n",
    "out_paths = sorted(Path(\"./dump/jsut_sr16000/org/dev/out_wavenet/\").glob(\"*.npy\"))\n",
    "\n",
    "dataset = Dataset(in_paths, out_paths)\n",
    "collate_fn = partial(collate_fn_wavenet, max_time_frames=100, hop_size=80, aux_context_window=0)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=8, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "wavs, feats = next(iter(data_loader))\n",
    "\n",
    "print(\"음성 파형의 크기:\", tuple(wavs.shape))\n",
    "print(\"조건부 특징량의 크기:\", tuple(feats.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-differential",
   "metadata": {},
   "source": [
    "#### 미니 배치 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dsp import inv_mulaw_quantize\n",
    "\n",
    "fig, ax = plt.subplots(len(wavs), 1, figsize=(8,10), sharex=True, sharey=True)\n",
    "for n in range(len(wavs)):\n",
    "    x = wavs[n].data.numpy()\n",
    "    x = inv_mulaw_quantize(x, 255)\n",
    "    ax[n].plot(x)\n",
    "\n",
    "ax[-1].set_xlabel(\"Time [sample]\")\n",
    "for a in ax:\n",
    "    a.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 8-5\n",
    "savefig(\"fig/wavenet_impl_minibatch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-array",
   "metadata": {},
   "source": [
    "### 간단한 학습 스크립트 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-eagle",
   "metadata": {},
   "source": [
    "#### 모델 파라미터의 지수 이동 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average_(model, model_test, beta=0.9999):\n",
    "    for param, param_test in zip(model.parameters(), model_test.parameters()):\n",
    "        param_test.data = torch.lerp(param.data, param_test.data, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-billy",
   "metadata": {},
   "source": [
    "#### 학습 전 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.wavenet import WaveNet\n",
    "from torch import optim\n",
    "\n",
    "# 동작 확인용: 레이어 수를 줄인 작은 WaveNet\n",
    "ToyWaveNet = partial(WaveNet, out_channels=256, layers=2, stacks=1, kernel_size=2, cin_channels=333)\n",
    "\n",
    "model = ToyWaveNet()\n",
    "# 모델 파라미터의 지수 이동 평균\n",
    "model_ema = ToyWaveNet()\n",
    "model_ema.load_state_dict(model.state_dict())\n",
    "\n",
    "# lr은 학습률을 나타냅니다.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# gamma는 학습률의 감쇠 계수를 나타냅니다.\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, gamma=0.5, step_size=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-speech",
   "metadata": {},
   "source": [
    "#### 学習ループの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader를 사용하여 미니 배치 생성 : 미니 배치마다 처리\n",
    "for x, c in data_loader:\n",
    "    # 순전파 계산\n",
    "    x_hat = model(x, c)\n",
    "    # 음의 로그 우도(likelihood) 계산\n",
    "    loss = nn.CrossEntropyLoss()(x_hat[:, :, :-1], x[:, 1:]).mean()\n",
    "    # 손실 값을 출력\n",
    "    print(loss.item())\n",
    "    # optimizer에 축적 된 기울기를 재설정\n",
    "    optimizer.zero_grad()\n",
    "    # 오차의 역전파 계산\n",
    "    loss.backward()\n",
    "    # 매개변수 업데이트\n",
    "    optimizer.step()\n",
    "    # 이동 지수 평균 계산\n",
    "    moving_average_(model, model_ema)\n",
    "    # 학습률 스케줄러 업데이트\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-medium",
   "metadata": {},
   "source": [
    "### 실용적인 학습 스크립트 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-steps",
   "metadata": {},
   "source": [
    "`train_wavenet.py`를 참조하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-kenya",
   "metadata": {},
   "source": [
    "## 8.6 WaveNet 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-computer",
   "metadata": {},
   "source": [
    "### WaveNet 구성 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat conf/train_wavenet/model/{wavenet_config_name}.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-andrew",
   "metadata": {},
   "source": [
    "### WaveNet 인스턴스화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "# WaveNet의 30층 모두를 표시하면 길어지므로 여기서는 생략합니다.\n",
    "# hydra.utils.instantiate(OmegaConf.load(f\"./conf/train_wavenet/model/{wavenet_config_name}.yaml\")[\"netG\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-while",
   "metadata": {},
   "source": [
    "### 레시피의 stage 7 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 7 --stop-stage 7 --wavenet-model $wavenet_config_name \\\n",
    "        --tqdm $run_sh_tqdm --wavenet-data-batch-size $wavenet_batch_size --wavenet-train-max-train-steps $wavenet_max_train_steps \\\n",
    "        --cudnn-benchmark $cudnn_benchmark --cudnn-deterministic $cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-advertiser",
   "metadata": {},
   "source": [
    "### 손실 함수의 값 추이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet_loss = df[df.run.str.contains(\"wavenet\")]\n",
    "\n",
    "wavenet_train_loss = wavenet_loss[wavenet_loss.tag.str.contains(\"Loss/train\")]\n",
    "wavenet_dev_loss = wavenet_loss[wavenet_loss.tag.str.contains(\"Loss/dev\")]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(wavenet_train_loss[\"step\"], wavenet_train_loss[\"value\"], label=\"Train\")\n",
    "ax.plot(wavenet_dev_loss[\"step\"], wavenet_dev_loss[\"value\"], \"--\", label=\"Dev\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Epoch loss\")\n",
    "ax.set_ylim(1.7, 2.2)\n",
    "plt.legend()\n",
    "\n",
    "# 그림 8-6\n",
    "savefig(\"fig/wavenet_impl_wavenet_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-sweden",
   "metadata": {},
   "source": [
    "## 8.7 학습된 모델을 사용하여 텍스트에서 음성 합성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-blanket",
   "metadata": {},
   "source": [
    "### 학습된 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-bhutan",
   "metadata": {},
   "source": [
    "#### 연속 길이 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_config = OmegaConf.load(f\"exp/jsut_sr16000/{duration_config_name}/model.yaml\")\n",
    "duration_model = hydra.utils.instantiate(duration_config.netG)\n",
    "checkpoint = torch.load(f\"exp/jsut_sr16000/{duration_config_name}/latest.pth\", map_location=device)\n",
    "duration_model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "duration_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-hormone",
   "metadata": {},
   "source": [
    "#### 로그 F0 예측 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "logf0_config = OmegaConf.load(f\"exp/jsut_sr16000/{logf0_config_name}/model.yaml\")\n",
    "logf0_model = hydra.utils.instantiate(logf0_config.netG)\n",
    "checkpoint = torch.load(f\"exp/jsut_sr16000/{logf0_config_name}/latest.pth\", map_location=device)\n",
    "logf0_model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "logf0_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-passion",
   "metadata": {},
   "source": [
    "#### WaveNet 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet_config = OmegaConf.load(f\"exp/jsut_sr16000/{wavenet_config_name}/model.yaml\")\n",
    "wavenet_model = hydra.utils.instantiate(wavenet_config.netG)\n",
    "checkpoint = torch.load(f\"exp/jsut_sr16000/{wavenet_config_name}/latest_ema.pth\", map_location=device)\n",
    "wavenet_model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "# weight normalization 은 추론시에는 불필요하므로 제외\n",
    "wavenet_model.remove_weight_norm_()\n",
    "wavenet_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-extreme",
   "metadata": {},
   "source": [
    "#### 통계량 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_in_scaler = joblib.load(\"./dump/jsut_sr16000/norm/in_duration_scaler.joblib\")\n",
    "duration_out_scaler = joblib.load(\"./dump/jsut_sr16000/norm/out_duration_scaler.joblib\")\n",
    "logf0_in_scaler = joblib.load(\"./dump/jsut_sr16000/norm/in_logf0_scaler.joblib\")\n",
    "logf0_out_scaler = joblib.load(\"./dump/jsut_sr16000/norm/out_logf0_scaler.joblib\")\n",
    "wavenet_in_scaler = joblib.load(\"./dump/jsut_sr16000/norm/in_wavenet_scaler.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-favorite",
   "metadata": {},
   "source": [
    "### 음소 연속 길이 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.util import lab2phonemes, find_lab, find_feats\n",
    "from ttslearn.dnntts.gen import predict_duration\n",
    "\n",
    "labels = hts.load(find_lab(\"downloads/jsut_ver1.1/\", test_utt))\n",
    "\n",
    "# 풀 컨텍스트 라벨에서 음소만 추출\n",
    "test_phonemes = lab2phonemes(labels)\n",
    "\n",
    "# 언어 특징량 추출에 사용하기 위한 질문 파일\n",
    "binary_dict, numeric_dict = hts.load_question_set(ttslearn.util.example_qst_file())\n",
    "\n",
    "# 음소 연속 길이 예측\n",
    "durations_test = predict_duration(\n",
    "    device, labels, duration_model, duration_config, duration_in_scaler, duration_out_scaler,\n",
    "    binary_dict, numeric_dict)\n",
    "durations_test_target = np.load(find_feats(\"dump/jsut_sr16000/org\", test_utt, typ=\"out_duration\"))\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "ax.plot(durations_test_target, \"-+\", label=\"Target\")\n",
    "ax.plot(durations_test, \"--*\", label=\"Predicted\")\n",
    "ax.set_xticks(np.arange(len(test_phonemes)))\n",
    "ax.set_xticklabels(test_phonemes)\n",
    "ax.set_xlabel(\"Phoneme\")\n",
    "ax.set_ylabel(\"Duration (the number of frames)\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-flight",
   "metadata": {},
   "source": [
    "### 로그 기본 주파수 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dnntts.gen import predict_acoustic\n",
    "\n",
    "labels = hts.load(find_lab(\"downloads/jsut_ver1.1/\", test_utt))\n",
    "# 로그 기본 주파수 예측\n",
    "out_feats = predict_acoustic(\n",
    "    device, labels, logf0_model, logf0_config, logf0_in_scaler,\n",
    "    logf0_out_scaler, binary_dict, numeric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.util import trim_silence\n",
    "from ttslearn.dnntts.multistream import split_streams\n",
    "\n",
    "# 결합된 특징량 분리\n",
    "out_feats = trim_silence(out_feats, labels)\n",
    "lf0_gen, vuv_gen = out_feats[:, 0], out_feats[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dnntts.multistream import get_static_features\n",
    "\n",
    "# 비교를 위해 자연음성에서 추출한 음향 특징량을 읽기\n",
    "feats = np.load(find_feats(\"dump/jsut_sr16000/org/\", test_utt, typ=\"out_logf0\"))\n",
    "# 특징량 분리\n",
    "lf0_ref, vuv_ref = get_static_features(\n",
    "    feats, logf0_config.num_windows, logf0_config.stream_sizes, logf0_config.has_dynamic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-imaging",
   "metadata": {},
   "source": [
    "#### F0 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로그 기본 주파수에서 기본 주파수로 변환\n",
    "f0_ref = np.exp(lf0_ref)\n",
    "f0_ref[vuv_ref < 0.5] = 0\n",
    "f0_gen = np.exp(lf0_gen)\n",
    "f0_gen[vuv_gen < 0.5] = 0\n",
    "\n",
    "timeaxis = librosa.frames_to_time(np.arange(len(f0_ref)), sr=sr, hop_length=int(0.005 * sr))\n",
    "\n",
    "fix, ax = plt.subplots(1,1, figsize=(8,3))\n",
    "ax.plot(timeaxis, f0_ref, linewidth=2, label=\"F0 of natural speech\")\n",
    "ax.plot(timeaxis, f0_gen, \"--\", linewidth=2, label=\"F0 of generated speech\")\n",
    "\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Frequency [Hz]\")\n",
    "ax.set_xlim(timeaxis[0], timeaxis[-1])\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-cloud",
   "metadata": {},
   "source": [
    "### 음성 파형 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dsp import inv_mulaw_quantize\n",
    "\n",
    "@torch.no_grad()\n",
    "def gen_waveform(\n",
    "    device,  # cpu or cuda\n",
    "    labels,  # 풀 컨텍스트 라벨\n",
    "    logf0_vuv,  # 연속 로그 기본 주파수 및 유성/무성 플래그\n",
    "    wavenet_model,  # 학습된 WaveNet\n",
    "    wavenet_in_scaler,  # 조건부 특징의 정규화를위한 StandardScaler\n",
    "    binary_dict,  # 이진 특징량을 추출하는 정규식\n",
    "    numeric_dict,  # 수치 특징량을 추출하는 정규 표현\n",
    "    tqdm=tqdm,  # 프로그래스바\n",
    "):\n",
    "    # 프레임별 언어 특징량 추출\n",
    "    in_feats = fe.linguistic_features(\n",
    "        labels,\n",
    "        binary_dict,\n",
    "        numeric_dict,\n",
    "        add_frame_features=True,\n",
    "        subphone_features=\"coarse_coding\",\n",
    "    )\n",
    "    # 프레임 단위의 언어 특징량과 로그 연속 기본 주파수·유성/무성 플래그를 결합\n",
    "    in_feats = np.hstack([in_feats, logf0_vuv])\n",
    "\n",
    "    # 특징량 정규화\n",
    "    in_feats = wavenet_in_scaler.transform(in_feats)\n",
    "\n",
    "    # 조건부 특징을 numpy.ndarray에서 torch.Tensor로 변환\n",
    "    c = torch.from_numpy(in_feats).float().to(device)\n",
    "    # (B, T, C) -> (B, C, T)\n",
    "    c = c.view(1, -1, c.size(-1)).transpose(1, 2)\n",
    "\n",
    "    # 음성 파형의 길이 계산\n",
    "    upsample_scale = np.prod(wavenet_model.upsample_scales)\n",
    "    time_steps = (c.shape[-1] - wavenet_model.aux_context_window * 2) * upsample_scale\n",
    "\n",
    "    # WaveNet으로 음성 파형 생성\n",
    "    # 참고 : 계산에 시간이 걸리기 위해 tqdm의 진행률 막대를 사용합니다.\n",
    "    gen_wav = wavenet_model.inference(c, time_steps, tqdm)\n",
    "\n",
    "    # One-hot 벡터를 1차원 신호로 변환\n",
    "    gen_wav = gen_wav.max(1)[1].float().cpu().numpy().reshape(-1)\n",
    "\n",
    "    # Mu-law 양자화의 역변환\n",
    "    gen_wav = inv_mulaw_quantize(gen_wav, wavenet_model.out_channels - 1)\n",
    "\n",
    "    return gen_wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-directive",
   "metadata": {},
   "source": [
    "### 모든 모델을 결합하여 음성 파형 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: False의 경우 정답의 durations를 사용합니다.\n",
    "# 모든 모델을 연결하려면 True로 설정하십시오.\n",
    "use_ground_truth_durations = True\n",
    "\n",
    "labels = hts.load(find_lab(\"downloads/jsut_ver1.1/\", test_utt))\n",
    "\n",
    "# 언어 특징량 추출의 사전 준비\n",
    "binary_dict, numeric_dict = hts.load_question_set(ttslearn.util.example_qst_file())\n",
    "\n",
    "if not use_ground_truth_durations:\n",
    "    # 음소 연속 길이 예측\n",
    "    durations = predict_duration(\n",
    "        device, labels, duration_model, duration_config, duration_in_scaler, duration_out_scaler,\n",
    "        binary_dict, numeric_dict)\n",
    "\n",
    "    # 예측된 연속 길이를 풀 컨텍스트 레이블로 설정\n",
    "    labels.set_durations(durations)\n",
    "\n",
    "# 로그 기본 주파수 예측\n",
    "# 참고 : 동적 특징 량을 WaveNet 조건부 특징 량에 사용하기 때문에 매개 변수 생성 (mlpg)을 수행하지 않습니다.\n",
    "logf0_vuv = predict_acoustic(\n",
    "    device, labels, logf0_model, logf0_config, logf0_in_scaler,\n",
    "    logf0_out_scaler, binary_dict, numeric_dict, mlpg=False)\n",
    "\n",
    "# WaveNet으로 음성 파형 생성\n",
    "gen_wav = gen_waveform(\n",
    "    device, labels, logf0_vuv, wavenet_model, wavenet_in_scaler,\n",
    "    binary_dict, numeric_dict, tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비교를 위해 원래 음성 로드\n",
    "from scipy.io import wavfile\n",
    "_sr, ref_wav = wavfile.read(f\"./downloads/jsut_ver1.1/basic5000/wav/{test_utt}.wav\")\n",
    "ref_wav = (ref_wav / 32768.0).astype(np.float64)\n",
    "ref_wav = librosa.resample(ref_wav, _sr, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
    "\n",
    "hop_length = int(sr * 0.005)\n",
    "fft_size = pyworld.get_cheaptrick_fft_size(sr)\n",
    "\n",
    "spec_ref = librosa.stft(ref_wav, n_fft=fft_size, hop_length=hop_length, window=\"hann\")\n",
    "logspec_ref = np.log(np.abs(spec_ref))\n",
    "spec_gen = librosa.stft(gen_wav, n_fft=fft_size, hop_length=hop_length, window=\"hann\")\n",
    "logspec_gen = np.log(np.abs(spec_gen))\n",
    "\n",
    "mindb = min(logspec_ref.min(), logspec_gen.min())\n",
    "maxdb = max(logspec_ref.max(), logspec_gen.max())\n",
    "\n",
    "mesh = librosa.display.specshow(logspec_ref, hop_length=hop_length, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax[0])\n",
    "mesh.set_clim(mindb, maxdb)\n",
    "fig.colorbar(mesh, ax=ax[0], format=\"%+2.fdB\")\n",
    "\n",
    "mesh = librosa.display.specshow(logspec_gen, hop_length=hop_length, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax[1])\n",
    "mesh.set_clim(mindb, maxdb)\n",
    "fig.colorbar(mesh, ax=ax[1], format=\"%+2.fdB\")\n",
    "\n",
    "ax[0].set_title(\"Spectrogram of natural speech\")\n",
    "ax[1].set_title(\"Spectrogram of generated speech\")\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Frequency [Hz]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"自然音声\")\n",
    "IPython.display.display(Audio(ref_wav, rate=sr))\n",
    "print(\"WaveNet音声合成\")\n",
    "IPython.display.display(Audio(gen_wav, rate=sr))\n",
    "\n",
    "# 그림 8-7\n",
    "savefig(\"./fig/wavenet_impl_tts_spec_comp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-wholesale",
   "metadata": {},
   "source": [
    "### 평가 데이터에 대한 음성 파형 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-external",
   "metadata": {},
   "source": [
    "#### 레시피의 stage 8 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh and run_stage8:\n",
    "    ! ./run.sh --stage 8 --stop-stage 8 \\\n",
    "        --tqdm $run_sh_tqdm --duration-model $duration_config_name --logf0-model $logf0_config_name --wavenet-model $wavenet_config_name \\\n",
    "        --reverse true --num-eval-utts $num_eval_utts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-giving",
   "metadata": {},
   "source": [
    "## 자연 음성과 합성 음성의 비교 (bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from ttslearn.util import load_utt_list\n",
    "\n",
    "with open(\"./downloads/jsut_ver1.1/basic5000/transcript_utf8.txt\") as f:\n",
    "    transcripts = {}\n",
    "    for l in f:\n",
    "        utt_id, script = l.split(\":\")\n",
    "        transcripts[utt_id] = script\n",
    "        \n",
    "eval_list = load_utt_list(\"data/eval.list\")[::-1][:5]\n",
    "\n",
    "for utt_id in eval_list:\n",
    "    # ref file \n",
    "    ref_file = f\"./downloads/jsut_ver1.1/basic5000/wav/{utt_id}.wav\"\n",
    "    _sr, ref_wav = wavfile.read(ref_file)\n",
    "    ref_wav = (ref_wav / 32768.0).astype(np.float64)\n",
    "    ref_wav = librosa.resample(ref_wav, _sr, sr)\n",
    "    \n",
    "    print(f\"{utt_id}: {transcripts[utt_id]}\")\n",
    "    print(\"自然音声\")\n",
    "    IPython.display.display(Audio(ref_wav, rate=sr))\n",
    "\n",
    "    gen_file = f\"exp/jsut_sr16000/synthesis_{duration_config_name}_{logf0_config_name}_{wavenet_config_name}/eval/{utt_id}.wav\"\n",
    "    if exists(gen_file):\n",
    "        _sr, gen_wav = wavfile.read(gen_file)    \n",
    "        print(\"WaveNet音声合成\")\n",
    "        IPython.display.display(Audio(gen_wav, rate=sr))\n",
    "    else:\n",
    "        # 음성 생성이 완료되지 않은 경우\n",
    "        print(\"WaveNet音声合成: not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-worthy",
   "metadata": {},
   "source": [
    "## 학습된 모델 패키징 (bonus)\n",
    "\n",
    "학습된 모델을 사용하는 TTS에 필요한 모든 파일을 단일 디렉토리로 그룹화합니다.\n",
    "`ttslearn.wavenet.WaveNetTTS` 클래스에는, 정리한 디렉토리를 지정해, TTS를 실시하는 기능이 구현되고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-delhi",
   "metadata": {},
   "source": [
    "### 레시피의 stage 99 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sh:\n",
    "    ! ./run.sh --stage 99 --stop-stage 99 \\\n",
    "        --duration-model $duration_config_name --logf0-model $logf0_config_name --wavenet-model $wavenet_config_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls tts_models/jsut_sr16000_{duration_config_name}_{logf0_config_name}_{wavenet_config_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-float",
   "metadata": {},
   "source": [
    "### 패키징된 모델을 이용한 TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.wavenet import WaveNetTTS\n",
    "\n",
    "# 패키징된 모델의 경로를 지정합니다.\n",
    "engine = WaveNetTTS(\n",
    "    model_dir=f\"./tts_models/jsut_sr16000_{duration_config_name}_{logf0_config_name}_{wavenet_config_name}\"\n",
    ")\n",
    "wav, sr = engine.tts(\"ここまでお読みいただき、ありがとうございました。\", tqdm=tqdm)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,2))\n",
    "librosa.display.waveshow(wav.astype(np.float32), sr, ax=ax)\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()\n",
    "\n",
    "Audio(wav, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_colab():\n",
    "    from datetime import timedelta\n",
    "    elapsed = (time.time() - start_time)\n",
    "    print(\"소요시간:\", str(timedelta(seconds=elapsed)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}