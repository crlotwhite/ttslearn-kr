{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "smoking-somalia",
   "metadata": {},
   "source": [
    "# 7장 WaveNet: 심층 학습을 기반으로 음성 파형 생성 모델\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/r9y9/ttslearn/blob/master/notebooks/ch07_WaveNet.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-radical",
   "metadata": {},
   "source": [
    "## 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-joseph",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -VV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-subsection",
   "metadata": {},
   "source": [
    "### ttslearn 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import ttslearn\n",
    "except ImportError:\n",
    "    !pip install ttslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttslearn\n",
    "ttslearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-flexibility",
   "metadata": {},
   "source": [
    "### 패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "import tensorboard as tb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치 연산\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "# 음성 파형 불러오기\n",
    "from scipy.io import wavfile\n",
    "# 사운드 분석 및 시각화\n",
    "import librosa\n",
    "import librosa.display\n",
    "# 파이썬에서 배우는 음성 합성\n",
    "import ttslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정\n",
    "from ttslearn.util import init_seed\n",
    "init_seed(773)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-confusion",
   "metadata": {},
   "source": [
    "### 그래프 그리기 설정 (描画周りの設定) // 번역 수정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.notebook import get_cmap, init_plot_style, savefig\n",
    "cmap = get_cmap()\n",
    "init_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-symbol",
   "metadata": {},
   "source": [
    "## 7.3 WaveNet에서 음성 파형 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-sender",
   "metadata": {},
   "source": [
    "### $\\mu$-law 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mulaw(x, mu=255):\n",
    "    return np.sign(x) * np.log1p(mu * np.abs(x)) / np.log1p(mu)\n",
    "\n",
    "def quantize(y, mu=255, offset=1):\n",
    "    # [-1, 1] -> [0, 2] -> [0, 1] -> [0, mu]\n",
    "    return ((y + offset) / 2 * mu).astype(np.int64)    \n",
    "\n",
    "def mulaw_quantize(x, mu=255):\n",
    "    return quantize(mulaw(x, mu), mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-cinema",
   "metadata": {},
   "source": [
    "#### $\\mu$-law 알고리즘 적용 전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "x = (x / 32768.0).astype(np.float32)\n",
    "\n",
    "mu = 2**8-1 # 8-bit\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(6,4))\n",
    "ax[0].set_title(\"Waveform\")\n",
    "ax[1].set_title(\"Histrogram\")\n",
    "\n",
    "ax[0].set_ylim(-0.9, 0.9)\n",
    "librosa.display.waveshow(x, ax=ax[0], sr=16000)\n",
    "\n",
    "ax[1].set_xlim(-0.9, 0.9)\n",
    "ax[1].hist(x, bins=mu)\n",
    "\n",
    "ax[0].set_xlabel(\"Time [sec]\")\n",
    "ax[0].set_ylabel(\"Amplitude\")\n",
    "ax[1].set_xlabel(\"Amplitude\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 7-6 (a)\n",
    "savefig(\"./fig/wavenet_mulaw_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-supplier",
   "metadata": {},
   "source": [
    "#### $\\mu$-law 알고리즘 적용 후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(6,4))\n",
    "ax[0].set_title(\"Waveform\")\n",
    "ax[1].set_title(\"Histrogram\")\n",
    "\n",
    "ax[0].set_ylim(-0.9, 0.9)\n",
    "librosa.display.waveshow(mulaw(x), ax=ax[0], sr=16000)\n",
    "\n",
    "ax[1].set_xlim(-0.9, 0.9)\n",
    "ax[1].hist(mulaw(x), bins=mu)\n",
    "\n",
    "ax[0].set_xlabel(\"Time [sec]\")\n",
    "ax[0].set_ylabel(\"Amplitude\")\n",
    "ax[1].set_xlabel(\"Amplitude\")\n",
    "ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 7-6 (b)\n",
    "savefig(\"./fig/wavenet_mulaw_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-junior",
   "metadata": {},
   "source": [
    "### $\\mu$-law 알고리즘에 의한 역변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_mulaw(y, mu=255):\n",
    "    return np.sign(y) * (1.0 / mu) * ((1.0 + mu)**np.abs(y) - 1.0)\n",
    "\n",
    "def inv_quantize(y, mu):\n",
    "    # [0, mu] -> [-1, 1]\n",
    "    return 2 * y.astype(np.float32) / mu - 1\n",
    "\n",
    "def inv_mulaw_quantize(y, mu=255):\n",
    "    return inv_mulaw(inv_quantize(y, mu), mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-diesel",
   "metadata": {},
   "source": [
    "#### $\\mu$-law 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "x = (x / 32768.0).astype(np.float32)  \n",
    "x = librosa.resample(x, sr, 16000)\n",
    "sr = 16000\n",
    "\n",
    "bits = [8, 4]\n",
    "\n",
    "fig, ax = plt.subplots(len(bits)+1, 1, figsize=(6,2*(len(bits)+1)), sharey=True)\n",
    "ax[0].set_title(\"Input waveform\")\n",
    "librosa.display.waveshow(x, sr, x_axis=\"time\", ax=ax[0])\n",
    "IPython.display.display(Audio(x, rate=sr))\n",
    "\n",
    "for idx, bit in enumerate(bits):\n",
    "    mu = 2**bit - 1\n",
    "    x_hat = inv_quantize(quantize(x, mu), mu)\n",
    "    librosa.display.waveshow(x_hat, sr, x_axis=\"time\", ax=ax[idx+1])\n",
    "    ax[idx+1].set_title(f\"{bit}-bit waveform\")\n",
    "    IPython.display.display(Audio(x_hat, rate=sr))\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Amplitude\")\n",
    "    a.set_xticks(np.arange(0, 3.5, 0.5))\n",
    "    a.set_ylim(-0.5, 0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 7-7 (a)\n",
    "savefig(\"./fig/wavenet_inv_mulaw_waveform_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-maximum",
   "metadata": {},
   "source": [
    "#### $\\mu$-law 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "x = (x / 32768.0).astype(np.float32)  \n",
    "x = librosa.resample(x, sr, 16000)\n",
    "sr = 16000\n",
    "\n",
    "bits = [8, 4]\n",
    "\n",
    "fig, ax = plt.subplots(len(bits)+1, 1, figsize=(6,2*(len(bits)+1)), sharey=True)\n",
    "ax[0].set_title(\"Input waveform\")\n",
    "librosa.display.waveshow(x, sr, x_axis=\"time\", ax=ax[0])\n",
    "IPython.display.display(Audio(x, rate=sr))\n",
    "\n",
    "for idx, bit in enumerate(bits):\n",
    "    mu = 2**bit - 1\n",
    "    x_hat = inv_mulaw_quantize(mulaw_quantize(x, mu), mu)\n",
    "    librosa.display.waveshow(x_hat, sr, x_axis=\"time\", ax=ax[idx+1])\n",
    "    ax[idx+1].set_title(f\"{bit}-bit waveform\")\n",
    "    IPython.display.display(Audio(x_hat, rate=sr))\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Amplitude\")\n",
    "    a.set_xticks(np.arange(0, 3.5, 0.5))\n",
    "    a.set_ylim(-0.5, 0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 7-7 (b)\n",
    "savefig(\"./fig/wavenet_inv_mulaw_waveform_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-green",
   "metadata": {},
   "source": [
    "## 7.4 인과적인 팽창 컨벌루션 (Dilated Convolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-burst",
   "metadata": {},
   "source": [
    "### 1차원 컨벌루션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _toy_1d_input():\n",
    "    # (B, C, T) where B and C = 1\n",
    "    return torch.tensor([1,2,3,0,1,2,4],dtype=torch.float).view(1,1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-creek",
   "metadata": {},
   "source": [
    "#### 패딩을 하지 않는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv1d(1,1,3,bias=False, padding=0)\n",
    "conv.weight.data[0,0,:] = torch.tensor([1,2,4],dtype=torch.float)\n",
    "\n",
    "x = _toy_1d_input()\n",
    "with torch.no_grad():\n",
    "    y= conv(x)\n",
    "print(\"입력:\", x.long().view(-1).tolist())\n",
    "print(\"출력:\", y.long().view(-1).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-secret",
   "metadata": {},
   "source": [
    "#### 패딩을 할 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv1d(1,1,3,bias=False, padding=1)\n",
    "conv.weight.data[0,0,:] = torch.tensor([1,2,4],dtype=torch.float)\n",
    "\n",
    "x = _toy_1d_input()\n",
    "with torch.no_grad():\n",
    "    y= conv(x)\n",
    "print(\"입력:\", x.long().view(-1).tolist())\n",
    "print(\"출력:\", y.long().view(-1).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-admission",
   "metadata": {},
   "source": [
    "#### 2층의 1차원 컨벌루션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv1d(1,1,3,bias=False, padding=1)\n",
    "conv.weight.data[0,0,:] = torch.tensor([1,2,4],dtype=torch.float)\n",
    "\n",
    "x = _toy_1d_input()\n",
    "with torch.no_grad():\n",
    "    y= conv(conv(x))\n",
    "print(\"입력:\", x.long().view(-1).tolist())\n",
    "print(\"출력:\", y.long().view(-1).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-disaster",
   "metadata": {},
   "source": [
    "### 인과적인 컨벌루션 (Dilated Convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        super().__init__()\n",
    "        self.padding = (kernel_size - 1)\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=self.padding, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1차원 컨벌루션\n",
    "        y = self.conv(x)\n",
    "        # 인과성을 담보하기 위해 순방향(forward)으로 이동\n",
    "        if self.padding > 0:\n",
    "            y = y[:, :, :-self.padding]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = CausalConv1d(1,1,3,bias=False)\n",
    "# 테스트를 위해 컨벌루션 커널을 수동으로 설정\n",
    "conv.conv.weight.data[0,0,:] = torch.tensor([1,2,4],dtype=torch.float)\n",
    "\n",
    "x = _toy_1d_input()\n",
    "y= conv(x)\n",
    "print(\"입력:\", x.long().view(-1).tolist())\n",
    "print(\"출력:\", y.long().view(-1).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-agenda",
   "metadata": {},
   "source": [
    "### 1차원 팽창 컨벌루션 (Dilated Convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedCausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, **kwargs):\n",
    "        super().__init__()\n",
    "        # 패딩의 너비를 계산할 때 dilation factor를 고려해야합니다.\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=self.padding, dilation=dilation, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1차원 컨벌루션\n",
    "        y = self.conv(x)\n",
    "        # 인과성을 담보하기 위해 순방향으로 이동\n",
    "        if self.padding > 0:\n",
    "            y = y[:, :, :-self.padding]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = DilatedCausalConv1d(1,1,3,dilation=2, bias=False)\n",
    "# 테스트를 위해 컨벌루션 커널을 수동으로 설정\n",
    "conv.conv.weight.data[0,0,:] = torch.tensor([1,2,4],dtype=torch.float)\n",
    "\n",
    "x = _toy_1d_input()\n",
    "y= conv(x)\n",
    "print(\"입력:\", x.long().view(-1).tolist())\n",
    "print(\"출력:\", y.long().view(-1).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-locking",
   "metadata": {},
   "source": [
    "## 7.5 게이트화된 활성화 함수를 이용한 1 차원 컨볼 루션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedDilatedCausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):\n",
    "        super().__init__()\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels*2, kernel_size, padding=self.padding, dilation=dilation)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1차원 컨벌루션\n",
    "        y = self.conv(x)\n",
    "        \n",
    "        # 인과성을 담보하기 위해 순방향으로 이동\n",
    "        if self.padding > 0:\n",
    "            y = y[:, :, :-self.padding]\n",
    "\n",
    "        # 채널 방향으로 분할\n",
    "        a, b = y.split(y.size(1) // 2, dim=1)\n",
    "        \n",
    "        # 게이트화된 활성화 함수 적용\n",
    "        y = torch.tanh(a) * torch.sigmoid(b)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = GatedDilatedCausalConv1d(128, 16, 3, dilation=2)\n",
    "x = torch.ones(32, 128, 100)\n",
    "print(\"입력 사이즈:\", tuple(x.shape))\n",
    "print(\"출력 사이즈:\", tuple(conv(x).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-beatles",
   "metadata": {},
   "source": [
    "## 7.6 조건부 특징 량의 업 샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-cosmetic",
   "metadata": {},
   "source": [
    "### 반복 기반 업샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3],[1, 2, 3],[1,2,3]]).view(1,3,-1).float()\n",
    "y = nn.Upsample(scale_factor=3, mode=\"nearest\")(x)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatUpsampling(nn.Module):\n",
    "    def __init__(self, upsample_scales):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=np.prod(upsample_scales), mode=\"nearest\")\n",
    "\n",
    "    def forward(self, c):\n",
    "        return self.upsample(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.ones(32, 80, 10)\n",
    "# 예를 들어 100배로 업샘플링\n",
    "c_up = RepeatUpsampling([100])(c)\n",
    "\n",
    "print(\"입력 사이즈:\", tuple(c.shape))\n",
    "print(\"출력 사이즈:\", tuple(c_up.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-adoption",
   "metadata": {},
   "source": [
    "### 최근접 보간(nearest neighbor interpolation)과 컨벌루션의 병용에 기초한 업샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class UpsampleNetwork(nn.Module):\n",
    "    def __init__(self, upsample_scales):\n",
    "        super().__init__()\n",
    "        self.upsample_scales = upsample_scales\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        for scale in upsample_scales:\n",
    "            kernel_size = (1, scale * 2 + 1)\n",
    "            conv = nn.Conv2d(\n",
    "                1, 1, kernel_size=kernel_size, padding=(0, scale), bias=False\n",
    "            )\n",
    "            conv.weight.data.fill_(1.0 / np.prod(kernel_size))\n",
    "            self.conv_layers.append(conv)\n",
    "\n",
    "    def forward(self, c):\n",
    "        # (B, 1, C, T)\n",
    "        c = c.unsqueeze(1)\n",
    "        # 최근접 보간과 컨벌루션 반복\n",
    "        for idx, scale in enumerate(self.upsample_scales):\n",
    "            # 시간 방향으로만 업샘플링\n",
    "            # (B, 1, C, T) -> (B, 1, C, T*scale)\n",
    "            c = F.interpolate(c, scale_factor=(1, scale), mode=\"nearest\")\n",
    "            c = self.conv_layers[idx](c)\n",
    "        # B x C x T\n",
    "        return c.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.ones(32, 80, 10)\n",
    "c_up = UpsampleNetwork([10, 8])(c)\n",
    "\n",
    "print(\"입력 사이즈:\", tuple(c.shape))\n",
    "print(\"출력 사이즈:\", tuple(c_up.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-alarm",
   "metadata": {},
   "source": [
    "#### 실제 데이터(mel-spectrogram) 업샘플링(bonus)\n",
    "\n",
    "책에서는 해설하지 않았지만, 2차원 컨벌루션의 가중치를 적절히 초기화하는 것으로, 컨벌루션 전후에 스케일이 유지되는 것을 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기화의 영향을 확인하기 위해, 컨벌루션 파라미터를 난수로 초기화\n",
    "class RandomInitUpsampleNetwork(UpsampleNetwork):\n",
    "    def __init__(self, upsample_scales):\n",
    "        super().__init__(upsample_scales)\n",
    "        for conv in self.conv_layers:\n",
    "            nn.init.normal_(conv.weight.data, 0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dsp import logmelspectrogram\n",
    "\n",
    "_sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "x = (x / 32768.0).astype(np.float32)\n",
    "sr = 16000\n",
    "x = librosa.resample(x, _sr, sr)\n",
    "hop_length = int(0.0125 * sr)\n",
    "sp = logmelspectrogram(x, sr, hop_length=hop_length)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "mesh = librosa.display.specshow(sp.T, sr=sr, hop_length=hop_length, cmap=cmap, x_axis=\"time\", y_axis=\"frames\")\n",
    "fig.colorbar(mesh, ax=ax)\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Frequency [Hz]\")\n",
    "plt.tight_layout()\n",
    "\n",
    "Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_net = UpsampleNetwork([10, 8])\n",
    "upsample_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp = torch.from_numpy(sp.T).view(1, 80, -1)\n",
    "\n",
    "# 컨벌루션 커널을 적절히 초기화한 경우\n",
    "tsp_up = upsample_net(tsp)\n",
    "\n",
    "# 무작위로 초기화한 경우\n",
    "torch.manual_seed(0)\n",
    "upsample_net_rand_init = RandomInitUpsampleNetwork([10, 8])\n",
    "\n",
    "tsp_up_rand_init = upsample_net_rand_init(tsp)\n",
    "\n",
    "A = tsp.squeeze(0).numpy()\n",
    "B = tsp_up_rand_init.squeeze(0).detach().numpy()\n",
    "C = tsp_up.squeeze(0).detach().numpy()\n",
    "\n",
    "s, e = 100, 120\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10,5))\n",
    "ax[0].set_title(\"Mel-spectrogram\")\n",
    "ax[1].set_title(\"Upsample (random init)\")\n",
    "ax[2].set_title(\"Upsample (proper init)\")\n",
    "\n",
    "ax[0].set_xlim(s, e)\n",
    "ax[0].imshow(A, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\", cmap=cmap)\n",
    "fig.colorbar(ax[0].pcolormesh(A, cmap=cmap, rasterized=True), ax=ax[0])\n",
    "\n",
    "ax[1].set_xlim(s*80, e*80)\n",
    "ax[1].imshow(B, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\", cmap=cmap)\n",
    "fig.colorbar(ax[1].pcolormesh(B, cmap=cmap, rasterized=True), ax=ax[1])\n",
    "\n",
    "ax[2].set_xlim(s*80, e*80)\n",
    "ax[2].imshow(C, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\", cmap=cmap)\n",
    "fig.colorbar(ax[2].pcolormesh(C, cmap=cmap, rasterized=True), ax=ax[2])\n",
    "\n",
    "for a in ax:\n",
    "    # 나중에 다시 라벨을 붙이기 때문에 여기에서 지워 둡니다.\n",
    "    a.set_ylabel(\"\")\n",
    "\n",
    "ax[0].set_ylabel(\"Mel filter channel\")\n",
    "ax[0].set_xlabel(\"Time [frame]\")\n",
    "for a in ax[1:]:\n",
    "    a.set_xlabel(\"Time [sample]\")\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-business",
   "metadata": {},
   "source": [
    "### 주변의 조건부 특징량을 고려한 업 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvInUpsampleNetwork(nn.Module):\n",
    "    def __init__(self, upsample_scales, cin_channels, aux_context_window):\n",
    "        super(ConvInUpsampleNetwork, self).__init__()\n",
    "        # 조건부 특징량의 시간 방향으로 인접한 정보를 1차원 컨벌루션으로 고려\n",
    "        kernel_size = 2 * aux_context_window + 1\n",
    "        self.conv_in = nn.Conv1d(cin_channels, cin_channels, kernel_size, bias=False)\n",
    "        # 업샘플링\n",
    "        self.upsample = UpsampleNetwork(upsample_scales)\n",
    "\n",
    "    def forward(self, c):\n",
    "        c_up = self.upsample(self.conv_in(c))\n",
    "        return c_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.ones(32, 80, 10)\n",
    "\n",
    "c_up = ConvInUpsampleNetwork([10, 8], 80, 2)(c)\n",
    "print(\"입력 사이즈:\", tuple(c.shape))\n",
    "print(\"출력 사이즈:\", tuple(c_up.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-wisdom",
   "metadata": {},
   "source": [
    "## 7.7 WaveNet 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-password",
   "metadata": {},
   "source": [
    "### 1 x 1 컨벌루션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-despite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1d1x1(in_channels, out_channels, bias=True):\n",
    "    return nn.Conv1d(\n",
    "        in_channels, out_channels, kernel_size=1, padding=0, dilation=1, bias=bias\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-domain",
   "metadata": {},
   "source": [
    "\n",
    "### 컨벌루션 블록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResSkipBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        residual_channels,  # 잔차 결합 채널 (residual channel) 수\n",
    "        gate_channels,  # 게이트 채널 수\n",
    "        kernel_size,  # 커널 크기\n",
    "        skip_out_channels,  # 건너뛰기 조인 채널 수\n",
    "        dilation=1,  # dilation factor\n",
    "        cin_channels=80,  # 조건부 특징 량의 채널 수\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "\n",
    "        # 1차원 팽창 컨벌루션(dilation == 1일 때는 일반 1차원 컨벌루션)\n",
    "        self.conv = nn.Conv1d(\n",
    "            residual_channels,\n",
    "            gate_channels,\n",
    "            kernel_size,\n",
    "            padding=self.padding,\n",
    "            dilation=dilation,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # local conditioning 용 1x1 convolution\n",
    "        self.conv1x1c = Conv1d1x1(cin_channels, gate_channels, bias=False)\n",
    "\n",
    "        # 게이트 된 활성화 함수로 인해 1 차원 컨벌루션 출력은 2 분할됩니다.\n",
    "        gate_out_channels = gate_channels // 2\n",
    "        self.conv1x1_out = Conv1d1x1(gate_out_channels, residual_channels)\n",
    "        self.conv1x1_skip = Conv1d1x1(gate_out_channels, skip_out_channels)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # 잔차 연결을 위한 입력 유지\n",
    "        residual = x\n",
    "\n",
    "        # 1차원 컨벌루션\n",
    "        splitdim = 1  # (B, C, T)\n",
    "        x = self.conv(x)\n",
    "        # 인과성을 보장하기 위해 출력을 이동\n",
    "        x = x[:, :, : -self.padding]\n",
    "\n",
    "        # 채널 방향으로 출력 분할\n",
    "        a, b = x.split(x.size(1) // 2, dim=1)\n",
    "\n",
    "        # local conditioning\n",
    "        c = self.conv1x1c(c)\n",
    "        ca, cb = c.split(c.size(1) // 2, dim=1)\n",
    "        a, b = a + ca, b + cb\n",
    "\n",
    "        # 게이트화된 활성화 함수\n",
    "        x = torch.tanh(a) * torch.sigmoid(b)\n",
    "\n",
    "        # 건너뛰기 연결을 위한 출력 계산\n",
    "        s = self.conv1x1_skip(x)\n",
    "\n",
    "        # 잔차 연결의 요소 합을하기 전에 차원 수를 맞춥니다.\n",
    "        x = self.conv1x1_out(x)\n",
    "\n",
    "        x = x + residual\n",
    "\n",
    "        return x, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 3\n",
    "conv = ResSkipBlock(128,16,kernel_size, 64, dilation=4)\n",
    "x = torch.ones(32, 128, 100)\n",
    "c = torch.ones(32, 80, 100)\n",
    "out, skip = conv(x, c)\n",
    "out.shape, skip.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-pendant",
   "metadata": {},
   "source": [
    "### WaveNet 전체 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-chaos",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 수용 필드의 크기를 수식대로 단순하게 계산\n",
    "(2 - 1) * sum([1,2,4,8,16,32,64,128,256,512]) * 3 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수용 필드의 크기를 계산하는 함수\n",
    "from ttslearn.wavenet import receptive_field_size\n",
    "\n",
    "for layers, stacks, kernel_size in [\n",
    "    (30, 3, 2), # WaveNet 논문 설정\n",
    "]:\n",
    "    print(f\"[Layers: {layers}, Dilation cycles: {stacks}, kernel size: {kernel_size}]: recepive field (ミリ秒):\")\n",
    "    size = receptive_field_size(layers, stacks, kernel_size)\n",
    "    print(f\"{size} samples ({size / 16000 * 1000} ミリ秒)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-casino",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WaveNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_channels=256,  # 출력 채널 수\n",
    "        layers=30,  # 레이어 수\n",
    "        stacks=3,  # 컨벌루션 블록 수\n",
    "        residual_channels=64,  # 잔차 결합 채널 수\n",
    "        gate_channels=128,  # 게이트 채널 수\n",
    "        skip_out_channels=64,  # 건너뛰기 연결 채널 수\n",
    "        kernel_size=2,  # 1차원 컨벌루션 커널 크기\n",
    "        cin_channels=80,  # 조건부 특징 량의 채널 수\n",
    "        upsample_scales=None,  # 업샘플링 스케일\n",
    "        aux_context_window=0,  # 업샘플링 시 참조하는 이웃 프레임 수\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.cin_channels = cin_channels\n",
    "        self.aux_context_window = aux_context_window\n",
    "        if upsample_scales is None:\n",
    "            upsample_scales = [10, 8]\n",
    "        self.upsample_scales = upsample_scales\n",
    "\n",
    "        self.first_conv = Conv1d1x1(out_channels, residual_channels)\n",
    "\n",
    "        # 메인이 되는 컨벌루션층\n",
    "        self.main_conv_layers = nn.ModuleList()\n",
    "        layers_per_stack = layers // stacks\n",
    "        for layer in range(layers):\n",
    "            dilation = 2 ** (layer % layers_per_stack)\n",
    "            conv = ResSkipBlock(\n",
    "                residual_channels,\n",
    "                gate_channels,\n",
    "                kernel_size,\n",
    "                skip_out_channels,\n",
    "                dilation=dilation,\n",
    "                cin_channels=cin_channels,\n",
    "            )\n",
    "            self.main_conv_layers.append(conv)\n",
    "\n",
    "        # 건너뛰기 연결의 합을 파형으로 변환\n",
    "        self.last_conv_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.ReLU(),\n",
    "                Conv1d1x1(skip_out_channels, skip_out_channels),\n",
    "                nn.ReLU(),\n",
    "                Conv1d1x1(skip_out_channels, out_channels),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 프레임 단위의 특징량을 샘플 단위로 업샘플링\n",
    "        self.upsample_net = ConvInUpsampleNetwork(\n",
    "            upsample_scales, cin_channels, aux_context_window\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # 양자화 된 이산수열을 One-hot 벡터로 변환\n",
    "        # (B, T) -> (B, T, out_channels) -> (B, out_channels, T)\n",
    "        x = F.one_hot(x, self.out_channels).transpose(1, 2).float()\n",
    "\n",
    "        # 조건부 특징 량의 업 샘플링\n",
    "        c = self.upsample_net(c)\n",
    "\n",
    "        # One-hot 벡터의 차원을 숨겨진 레이어의 차원으로 변환\n",
    "        x = self.first_conv(x)\n",
    "\n",
    "        # 메인 컨벌루션 레이어 처리\n",
    "        # 각 레이어에서 스킵 연결의 출력을 더하여 유지\n",
    "        skips = 0\n",
    "        for f in self.main_conv_layers:\n",
    "            x, h = f(x, c)\n",
    "            skips += h\n",
    "\n",
    "        # 건너 뛰기 연결의 합계를 입력으로 사용하여 출력 계산\n",
    "        x = skips\n",
    "        for f in self.last_conv_layers:\n",
    "            x = f(x)\n",
    "\n",
    "        # NOTE: 출력을 확률 값으로 해석하는 경우 softmax가 필요하지만,\n",
    "        # 학습시에는 nn.CrossEntropyLoss의 계산에 두고 softmax의 계산이 행해지므로,\n",
    "        # 여기서는 명시 적으로 softmax를 계산할 필요가 없습니다.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-homeless",
   "metadata": {},
   "source": [
    "### 장난감 모델을 이용한 WaveNet의 동작 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: inference와 호환되는 WaveNet을 사용하려면 다음 줄을 주석 처리하십시오.\n",
    "# from ttslearn.wavenet import WaveNet\n",
    "\n",
    "# 여기서는 inference 함수의 구현을 생략합니다.\n",
    "\n",
    "wavenet = WaveNet(out_channels=256, layers=2, stacks=1, kernel_size=2, cin_channels=64)\n",
    "wavenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0에서 255 사이의 값을 갖는 적절한 입력 신호\n",
    "x = torch.randint(0, 255, (16, 16000))\n",
    "# 프레임 시프트를 80개 샘플로 하여 64차원 조건부 특징량 생성\n",
    "c = torch.rand(16, 64, 16000//80)\n",
    "\n",
    "print(\"입력 사이즈:\", tuple(x.shape))\n",
    "print(\"조건부 특징량의 크기:\", tuple(c.shape))\n",
    "\n",
    "x_hat = wavenet(x, c)\n",
    "\n",
    "# 업샘플링 동작 확인을 위해 조건부 특징량의 업샘플링만 수행\n",
    "c_up = wavenet.upsample_net(c)\n",
    "\n",
    "print(\"업샘플링된 조건부 특징량의 크기:\", tuple(c_up.shape))\n",
    "print(\"WaveNet 출력 크기:\", tuple(x_hat.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-librarian",
   "metadata": {},
   "source": [
    "### 음의 로그 우도(likelihood) 최소화 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = F.log_softmax(x_hat, dim=1)\n",
    "# 자기 회귀성을 유지하기 위해 출력을 시간 방향으로 하나 이동\n",
    "nll = nn.NLLLoss()(log_prob[:, :, :-1], x[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()(x_hat[:, :, :-1], x[:, 1:])\n",
    "print(\"nll:\", nll.item())\n",
    "print(\"ce_loss\", ce_loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}