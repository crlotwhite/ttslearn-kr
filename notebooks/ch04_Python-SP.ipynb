{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제4장 Python에 의한 음성 신호 처리\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/r9y9/ttslearn/blob/master/notebooks/ch04_Python-SP.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -VV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ttslearn 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import ttslearn\n",
    "except ImportError:\n",
    "    !pip install ttslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttslearn\n",
    "ttslearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ttslearn 동작 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dnntts import DNNTTS\n",
    "from IPython.display import Audio\n",
    "\n",
    "engine = DNNTTS()\n",
    "wav, sr = engine.tts(\"日本語音声合成のデモです。\")\n",
    "Audio(wav, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,2))\n",
    "librosa.display.waveshow(wav.astype(np.float32), sr, ax=ax)\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정\n",
    "from ttslearn.util import init_seed\n",
    "init_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프 그리기 설정 (描画周りの設定) // 번역 수정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.notebook import get_cmap, init_plot_style, savefig\n",
    "cmap = get_cmap()\n",
    "init_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 수치 계산을 위한 Python 라이브러리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy와 Torch를 이용한 배열 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((2,2), dtype=np.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.zeros(2,2, dtype=torch.float)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy.ndarray와 torch.Tensor의 인터페이스 차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy에서는 배열 크기를 tuple로 줍니다.\n",
    "x = np.zeros((1,2,3), dtype=np.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch에서는 배열 크기를 다른 인수로 제공합니다.\n",
    "y = torch.zeros(1, 2, 3, dtype=torch.float32)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape == y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy.ndarray와 torch.Tensor의 상호 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((2,2), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.zeros((2,2), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Tensor에서 numpy.ndarray로의 변환\n",
    "type(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy.ndarray에서 torch.Tensor로의 변환\n",
    "type(torch.from_numpy(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy.ndarray와 torch.Tensor의 메모리 공유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((2,2), dtype=np.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.from_numpy(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,0] = 1.0 # 메모리가 공유되어 x로의 변경은 y에도 반영됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 음성 파일 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scipy.io.wavfile을 이용한 음성 파일 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "import ttslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr, wav = wavfile.read(ttslearn.util.example_audio_file())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wav) / sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wav)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 음성의 가시화(시각화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 음성 파일 읽기\n",
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,2))\n",
    "librosa.display.waveshow(x.astype(np.float32), sr, ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 4-2\n",
    "savefig(\"fig/pyssp_waveshow\")\n",
    "\n",
    "# 오디오 플레이어의 표시\n",
    "Audio(x.astype(np.float32), rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 음성의 푸리에 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음성 파일 읽기\n",
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "# 진폭 스펙트로그램\n",
    "X = np.abs(np.fft.rfft(x))\n",
    "# 로그 진폭 스펙트로그램\n",
    "logX = 20*np.log10(X)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4), sharex=True)\n",
    "freq = np.arange(len(X)) / 2 / len(X) * sr\n",
    "ax[0].plot(freq, X)\n",
    "ax[0].set_title(\"Amplitude spectrum\")\n",
    "ax[0].set_xlim(0, sr // 2)\n",
    "ax[0].set_xlabel(\"Frequency [Hz]\")\n",
    "ax[0].set_ylabel(\"Amplitude\")\n",
    "\n",
    "ax[1].plot(freq, logX)\n",
    "ax[1].set_title(\"Log amplitude spectrum\")\n",
    "ax[1].set_xlabel(\"Frequency [Hz]\")\n",
    "ax[1].set_ylabel(\"Amplitude [dB]\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 4-3\n",
    "savefig(\"fig/pyssp_rfftplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 음성의 단시간 푸리에 변환(STFT, Short Time Fourier Transform)과 그 역변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 창함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1024\n",
    "n = np.arange(N)\n",
    "w = 0.5 - 0.5 * np.cos(2*np.pi * n / N)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(w)\n",
    "ax.set_xlim(0, N)\n",
    "ax.set_xlabel(\"Time [sample]\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단시간 푸리에 변환(STFT) 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hanning(N):\n",
    "    n = np.arange(N)\n",
    "    w = 0.5 - 0.5 * np.cos(2*np.pi * n / N)\n",
    "    return w\n",
    "\n",
    "def stft(x, N, S):\n",
    "    # 창함수(간단하기 때문에 창폭과 프레임길이 N은 같습니다)\n",
    "    w = hanning(N)\n",
    "    # 단시간 푸리에 변환 프레임 수\n",
    "    M = (len(x) - N) // S + 1\n",
    "    # 단시간 푸리에 변환 결과 저장을 위한 2차원 배열\n",
    "    X = np.zeros((M, N//2 + 1), dtype=complex)\n",
    "    # 음성을 어긋나게 잘라내고 푸리에 변환\n",
    "    for m in range(M):\n",
    "        x_m = w * x[m*S:m*S+N]\n",
    "        X[m, :] = np.fft.rfft(x_m)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단시간 푸리에 변환 결과 가시화(시각화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음성 파일 읽기\n",
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "\n",
    "# 5밀리초의 프레임 시프트를 연산합니다.\n",
    "frame_shift = int(sr * 0.005)\n",
    "n_fft = 2048\n",
    "# 스펙트로그램\n",
    "X = stft(x.astype(np.float32), n_fft, frame_shift)\n",
    "# 로그 진폭으로 변환\n",
    "logX = librosa.amplitude_to_db(np.abs(X), ref=np.max)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,4), sharex=True)\n",
    "img = librosa.display.specshow(logX.T, hop_length=frame_shift, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax)\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
    "# 음성 출력은 저역에 집중하기 때문에 8000Hz까지 표시한다\n",
    "ax.set_ylim(0, 8000)\n",
    "\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Frequency [Hz]\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 4-5\n",
    "savefig(\"fig/pyssp_stft_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### librosa.stft를 이용한 단시간 푸리에 변환\n",
    "\n",
    "librosa.stft 는 STFT 를 실행하기 전에 디폴트로 신호의 첫머리와 끝에 패딩 처리를 합니다. 앞에서 설명한 STFT 구현은 이 처리를 지원하지 않기 때문에 동등한 STFT 결과를 얻기 위해서는 center=False로 패딩 처리를 하지 않도록 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "# n_fft: 2048, frame_shift: 240\n",
    "X = librosa.stft(x.astype(np.float32), n_fft=n_fft, win_length=n_fft, hop_length=frame_shift, window=\"hann\", center=False).T\n",
    "# 로그 진폭으로 변환\n",
    "logX = librosa.amplitude_to_db(np.abs(X), ref=np.max)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,4), sharex=True)\n",
    "img = librosa.display.specshow(logX.T, hop_length=frame_shift, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax)\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
    "# 음성 출력은 저역에 집중하기 때문에 8000Hz까지 표시한다\n",
    "ax.set_ylim(0, 8000)\n",
    "\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Frequency [Hz]\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시간 해상도와 주파수 해상도의 트레이드 오프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_power_of_2(x):\n",
    "    return 1 if x == 0 else 2**(x - 1).bit_length()\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10,5), sharex=True, sharey=True)\n",
    "\n",
    "for idx, win_length_ms in enumerate([0.05, 0.02, 0.01]):\n",
    "    win_length = int(sr * win_length_ms)\n",
    "    frame_shift = win_length // 4\n",
    "    n_fft = next_power_of_2(win_length)\n",
    "    \n",
    "    X = librosa.stft(x.astype(np.float32), n_fft=n_fft, win_length=n_fft, hop_length=frame_shift).T\n",
    "    logX =  librosa.amplitude_to_db(np.abs(X), ref=np.max)\n",
    "    mesh = librosa.display.specshow(\n",
    "        logX.T, hop_length=frame_shift, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax[idx])\n",
    "    fig.colorbar(mesh, ax=ax[idx], format=\"%+2.f dB\")\n",
    "    ax[idx].set_title(f\"win_length: {win_length}\")\n",
    "    mesh.set_clim(-80, 0)\n",
    "    ax[idx].set_xlim(1.0, 1.5)\n",
    "    ax[idx].set_xticks([1.0, 1.25, 1.5])\n",
    "    # 나중에 라벨을 다시 붙일 테니 여기에서는 지워둔다\n",
    "    ax[idx].set_ylabel(\"\")\n",
    "\n",
    "ax[0].set_ylabel(\"Frequency [Hz]\")\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylim(0, 8000)\n",
    "    a.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 4-6\n",
    "savefig(\"fig/pyssp_stft_tradeoff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 역단시간 푸리에 변환(ISTFT, Inverse Short-Time Fourier Transform)을 통한 음성 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음성 파일 읽기\n",
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "# 5밀리초의 프레임 시프트를 연산합니다.\n",
    "frame_shift = int(sr * 0.005)\n",
    "n_fft = 2048\n",
    "\n",
    "# STFT\n",
    "X = librosa.stft(x.astype(np.float32), n_fft=n_fft, win_length=n_fft, hop_length=frame_shift, window=\"hann\")\n",
    "# ISTFT\n",
    "x_hat = librosa.istft(X, win_length=n_fft, hop_length=frame_shift, window=\"hann\")\n",
    "\n",
    "IPython.display.display(Audio(x.astype(np.float32), rate=sr))\n",
    "IPython.display.display(Audio(x_hat.astype(np.float32), rate=sr))\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8,4), sharey=True)\n",
    "ax[0].set_title(\"Original speech\")\n",
    "ax[1].set_title(\"Reconstructed speech by ISTFT\")\n",
    "librosa.display.waveshow(x.astype(np.float32), sr, ax=ax[0])\n",
    "librosa.display.waveshow(x_hat.astype(np.float32), sr, ax=ax[1])\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 멜스펙트로그램 (mel-spectrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 멜 필터 뱅크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 16000\n",
    "n_fft = 2048\n",
    "n_mels = 8\n",
    "\n",
    "# 음성 파일 읽기\n",
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "x = x.astype(np.float32)\n",
    "x = librosa.resample(x, sr, 16000)\n",
    "sr = 16000\n",
    "\n",
    "# 5밀리초의 프레임 시프트를 연산합니다.\n",
    "frame_shift = int(sr * 0.005)\n",
    "# STFT\n",
    "X = librosa.stft(x, n_fft=n_fft, win_length=n_fft, hop_length=frame_shift, window=\"hann\")\n",
    "# 1 프레임을 잘라내다.\n",
    "X_m = np.abs(X[:, 280])\n",
    "\n",
    "# 멜 필터 뱅크: n_mels 개의 필터로 구성됩니다.\n",
    "melfb = librosa.filters.mel(sr, n_fft, n_mels=n_mels, norm=None)\n",
    "freq = librosa.fft_frequencies(sr, n_fft)\n",
    "\n",
    "# 멜 필터 뱅크를 표시\n",
    "fig, ax = plt.subplots(n_mels+1, 2, figsize=(8,10), sharex=True)\n",
    "ax[0][0].plot(freq, np.ones_like(freq))\n",
    "ax[0][0].set_title(\"All pass filter\")\n",
    "ax[0][0].set_ylim(0,1.1)\n",
    "ax[0][1].plot(freq, X_m)\n",
    "ax[0][1].set_title(\"Input amplitude spectrum\")\n",
    "for idx, fb in enumerate(melfb):\n",
    "    ax[idx+1][0].plot(freq, fb)\n",
    "    ax[idx+1][0].set_title(f\"Filter {idx+1}\")\n",
    "    ax[idx+1][1].plot(freq, fb * X_m)\n",
    "    ax[idx+1][1].set_title(f\"Filtered amplitude {idx+1}\")\n",
    "\n",
    "for a,b in ax:\n",
    "    a.set_xlabel(\"Frequency [Hz]\")\n",
    "    b.set_xlabel(\"Frequency [Hz]\")\n",
    "    a.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 4-7\n",
    "savefig(\"fig/pyssp_melfb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 멜스펙트로그램 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음성 파일 읽기\n",
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "# 5밀리초의 프레임 시프트를 연산합니다.\n",
    "frame_shift = int(sr * 0.005)\n",
    "n_fft = 2048\n",
    "\n",
    "# 스펙트로그램\n",
    "X = librosa.stft(x.astype(np.float32), n_fft=n_fft, hop_length=frame_shift)\n",
    "\n",
    "# 80차원 멜스펙트로그램\n",
    "n_mels = 80\n",
    "melfb = librosa.filters.mel(sr, n_fft, n_mels=n_mels)\n",
    "melspec = librosa.amplitude_to_db(np.dot(melfb, np.abs(X)), ref=np.max)\n",
    "\n",
    "# 비교용 로그 진폭 스펙트로그램\n",
    "logX = librosa.amplitude_to_db(np.abs(X), ref=np.max)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
    "ax[0].set_title(\"Spectrogram\")\n",
    "ax[1].set_title(\"80-dim Mel-spectrogram\")\n",
    "mesh = librosa.display.specshow(logX, hop_length=frame_shift, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax[0])\n",
    "fig.colorbar(mesh, ax=ax[0], format=\"%+2.f dB\")\n",
    "mesh.set_clim(-80, 0)\n",
    "mesh = librosa.display.specshow(melspec, hop_length=frame_shift, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"mel\",ax=ax[1])\n",
    "fig.colorbar(mesh, ax=ax[1], format=\"%+2.f dB\")\n",
    "mesh.set_clim(-80, 0)\n",
    "\n",
    "for a in ax:\n",
    "    a.set_ylim(0, 8000)\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Frequency [Hz]\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 4-8\n",
    "savefig(\"fig/pyssp_melspectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Griffin-Lim 알고리즘 기반의 위상 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음성 파일 읽기\n",
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "# 5밀리초의 프레임 시프트를 연산합니다.\n",
    "frame_shift = int(sr * 0.005)\n",
    "n_fft = 2048\n",
    "\n",
    "# 진폭 스펙트로그램\n",
    "X = np.abs(librosa.stft(x.astype(np.float32), n_fft=n_fft, hop_length=frame_shift))\n",
    "\n",
    "y1 = librosa.griffinlim(X, hop_length=frame_shift, n_iter=1)\n",
    "y2 = librosa.griffinlim(X, hop_length=frame_shift, n_iter=100)\n",
    "\n",
    "# 오디오 플레이어의 표시\n",
    "IPython.display.display(Audio(y1, rate=sr))\n",
    "IPython.display.display(Audio(y2, rate=sr))\n",
    "IPython.display.display(Audio(x, rate=sr))\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(8,6), sharey=True)\n",
    "ax[0].set_title(\"Griffin-Lim # of iteration: 1\")\n",
    "ax[1].set_title(\"Griffin-Lim # of iteration: 100\")\n",
    "ax[2].set_title(\"Natural speech\")\n",
    "librosa.display.waveshow(y1, sr=sr, ax=ax[0])\n",
    "librosa.display.waveshow(y2, sr=sr, ax=ax[1])\n",
    "librosa.display.waveshow(x.astype(np.float32), sr=sr, ax=ax[2])\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그림 4-9\n",
    "savefig(\"fig/pyssp_griffin_lim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순간 주파수 가시화(시각화) (bonus)\n",
    "\n",
    "Griffin-Lim 알고리즘은 위상 복원 기법입니다. 합성 음성과 자연 음성의 순시 위상(위상의 시간 미분)을 비교함으로써 위상 복원이 기대대로 이루어지고 있는지를 시각적으로 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 1024\n",
    "hop_length = n_fft // 4\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10,5), sharex=True)\n",
    "\n",
    "C = librosa.stft(y1, n_fft=n_fft, hop_length=hop_length)\n",
    "ifreq = np.angle(C[:, 1:] * np.conjugate(C[:, :-1]))\n",
    "mesh = librosa.display.specshow(ifreq, cmap=cmap, ax=ax[0], x_axis=\"time\", y_axis=\"hz\", sr=sr, hop_length=hop_length)\n",
    "fig.colorbar(mesh, ax=ax[0])\n",
    "ax[0].set_title(\"GL # of iteration: 1\")\n",
    "\n",
    "C = librosa.stft(y2, n_fft=n_fft, hop_length=hop_length)\n",
    "ifreq = np.angle(C[:, 1:] * np.conjugate(C[:, :-1]))\n",
    "mesh = librosa.display.specshow(ifreq, cmap=cmap, ax=ax[1], x_axis=\"time\", y_axis=\"hz\", sr=sr, hop_length=hop_length)\n",
    "fig.colorbar(mesh, ax=ax[1])\n",
    "ax[1].set_title(\"GL # of iteration: 100\")\n",
    "\n",
    "C = librosa.stft(x.astype(np.float32), n_fft=n_fft, hop_length=hop_length)\n",
    "ifreq = np.angle(C[:, 1:] * np.conjugate(C[:, :-1]))\n",
    "mesh = librosa.display.specshow(ifreq, cmap=cmap, ax=ax[2], x_axis=\"time\", y_axis=\"hz\", sr=sr, hop_length=hop_length)\n",
    "fig.colorbar(mesh, ax=ax[2])\n",
    "ax[2].set_title(\"Natural speech\")\n",
    "\n",
    "for a in ax:\n",
    "    # 나중에 라벨을 다시 붙일 테니 여기에서는 지워둔다\n",
    "    a.set_ylabel(\"\")\n",
    "\n",
    "ax[0].set_ylabel(\"Frequency [Hz]\")\n",
    "for a in ax:\n",
    "    a.set_xlim(1.5, 3.0)\n",
    "    a.set_ylim(0, 4000)\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_xticks([1.5, 2.0, 2.5, 3.0])\n",
    "    \n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}